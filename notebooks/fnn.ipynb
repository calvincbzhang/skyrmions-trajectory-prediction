{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN for Skyrmion trajectories prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import trackpy as tp\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# import functions\n",
    "from my_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>frame</th>\n",
       "      <th>particle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.420047</td>\n",
       "      <td>61.809992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.518261</td>\n",
       "      <td>109.009463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.658864</td>\n",
       "      <td>41.007417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.994689</td>\n",
       "      <td>82.173861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61.572998</td>\n",
       "      <td>129.252586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>26.838018</td>\n",
       "      <td>9158.734705</td>\n",
       "      <td>799.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>92.153535</td>\n",
       "      <td>8909.539660</td>\n",
       "      <td>799.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>160.675052</td>\n",
       "      <td>8812.401110</td>\n",
       "      <td>799.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>156.663224</td>\n",
       "      <td>8600.194927</td>\n",
       "      <td>799.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>93.779025</td>\n",
       "      <td>8679.555818</td>\n",
       "      <td>799.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                y            x  frame  particle\n",
       "0       24.420047    61.809992    0.0       0.0\n",
       "1       31.518261   109.009463    0.0       1.0\n",
       "2       51.658864    41.007417    0.0       2.0\n",
       "3       60.994689    82.173861    0.0       3.0\n",
       "4       61.572998   129.252586    0.0       4.0\n",
       "...           ...          ...    ...       ...\n",
       "11995   26.838018  9158.734705  799.0      10.0\n",
       "11996   92.153535  8909.539660  799.0      11.0\n",
       "11997  160.675052  8812.401110  799.0      12.0\n",
       "11998  156.663224  8600.194927  799.0      13.0\n",
       "11999   93.779025  8679.555818  799.0      14.0\n",
       "\n",
       "[12000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = 'Rec_EDGE_300K_1L_50MA.out'\n",
    "data = pd.read_csv(directory + '/filled_trajectories.csv')\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "no_skyrmions = data[data['frame'] == 0].shape[0]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Put data in list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:06<00:00, 123.98it/s]\n"
     ]
    }
   ],
   "source": [
    "no_skyrmions = int(max(data[data['frame'] == 0]['particle']) + 1)\n",
    "\n",
    "frames = None\n",
    "\n",
    "# iterate through the frames\n",
    "for f in tqdm(data['frame'].unique()):\n",
    "    coordinates = None\n",
    "    for p in data[data['frame'] == f]['particle']:\n",
    "        particle = data[(data['frame'] == f) & (data['particle'] == p)]\n",
    "        coordinates = np.append(coordinates, [particle['x'].values[0], particle['y'].values[0]]) if coordinates is not None else [particle['x'].values[0], particle['y'].values[0]]\n",
    "    \n",
    "    frames = np.append(frames, coordinates) if frames is not None else [coordinates]\n",
    "                                                                        \n",
    "frames = frames.reshape(-1, 2 * no_skyrmions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One image input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make data samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples are (frame, next_frame)\n",
    "\n",
    "X = frames[:-1]\n",
    "y = frames[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data for training and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (639, 30)\n",
      "y_train shape: (639, 30)\n",
      "X_test shape: (160, 30)\n",
      "y_test shape: (160, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"y_train shape: \" + str(y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "\n",
    "optimizer = 'NAdam'\n",
    "loss = 'mae'\n",
    "metrics = ['accuracy']\n",
    "training_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "n_input = 2 * no_skyrmions\n",
    "n_hidden_1 = 64\n",
    "n_hidden_2 = 64\n",
    "n_hidden_3 = 64\n",
    "n_output = 2 * no_skyrmions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "\n",
    "# Neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_1, input_dim=n_input, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_hidden_2, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_hidden_3, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 639 samples, validate on 160 samples\n",
      "Epoch 1/1000\n",
      "639/639 [==============================] - 0s 378us/step - loss: 1580.2537 - accuracy: 0.0642 - val_loss: 676.4967 - val_accuracy: 0.3750\n",
      "Epoch 2/1000\n",
      "639/639 [==============================] - 0s 61us/step - loss: 313.2213 - accuracy: 0.1643 - val_loss: 228.5528 - val_accuracy: 0.0812\n",
      "Epoch 3/1000\n",
      "639/639 [==============================] - 0s 56us/step - loss: 189.2074 - accuracy: 0.1236 - val_loss: 182.4017 - val_accuracy: 0.1937\n",
      "Epoch 4/1000\n",
      "639/639 [==============================] - 0s 51us/step - loss: 165.6381 - accuracy: 0.2222 - val_loss: 73.2899 - val_accuracy: 0.3938\n",
      "Epoch 5/1000\n",
      "639/639 [==============================] - 0s 53us/step - loss: 136.3814 - accuracy: 0.1315 - val_loss: 131.0051 - val_accuracy: 0.0500\n",
      "Epoch 6/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 149.8216 - accuracy: 0.2911 - val_loss: 167.9790 - val_accuracy: 0.1437\n",
      "Epoch 7/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 136.6567 - accuracy: 0.2504 - val_loss: 115.3003 - val_accuracy: 0.3938\n",
      "Epoch 8/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 140.0639 - accuracy: 0.2848 - val_loss: 88.2420 - val_accuracy: 0.3500\n",
      "Epoch 9/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 133.1750 - accuracy: 0.2520 - val_loss: 103.7819 - val_accuracy: 0.3375\n",
      "Epoch 10/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 130.1350 - accuracy: 0.3099 - val_loss: 144.3637 - val_accuracy: 0.3875\n",
      "Epoch 11/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 122.0863 - accuracy: 0.2504 - val_loss: 126.1263 - val_accuracy: 0.3812\n",
      "Epoch 12/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 120.0657 - accuracy: 0.2160 - val_loss: 141.4929 - val_accuracy: 0.0125\n",
      "Epoch 13/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 120.8004 - accuracy: 0.2347 - val_loss: 49.6447 - val_accuracy: 0.2313\n",
      "Epoch 14/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 118.5633 - accuracy: 0.2332 - val_loss: 105.3336 - val_accuracy: 0.2313\n",
      "Epoch 15/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 118.7982 - accuracy: 0.3709 - val_loss: 59.3483 - val_accuracy: 0.3812\n",
      "Epoch 16/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 121.2416 - accuracy: 0.1862 - val_loss: 65.3468 - val_accuracy: 0.1063\n",
      "Epoch 17/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 112.7275 - accuracy: 0.3239 - val_loss: 42.0594 - val_accuracy: 0.2937\n",
      "Epoch 18/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 110.7576 - accuracy: 0.3459 - val_loss: 69.4037 - val_accuracy: 0.2313\n",
      "Epoch 19/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 119.7902 - accuracy: 0.2175 - val_loss: 89.1629 - val_accuracy: 0.3688\n",
      "Epoch 20/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 112.2070 - accuracy: 0.2942 - val_loss: 91.3765 - val_accuracy: 0.1000\n",
      "Epoch 21/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 115.9999 - accuracy: 0.2879 - val_loss: 174.1937 - val_accuracy: 0.3562\n",
      "Epoch 22/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 113.7894 - accuracy: 0.2817 - val_loss: 134.5349 - val_accuracy: 0.1688\n",
      "Epoch 23/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 116.4487 - accuracy: 0.2316 - val_loss: 143.6042 - val_accuracy: 0.4313\n",
      "Epoch 24/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 111.5377 - accuracy: 0.3412 - val_loss: 68.3705 - val_accuracy: 0.3313\n",
      "Epoch 25/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 106.6540 - accuracy: 0.4131 - val_loss: 149.9050 - val_accuracy: 0.3562\n",
      "Epoch 26/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 108.6784 - accuracy: 0.4022 - val_loss: 131.1036 - val_accuracy: 0.5625\n",
      "Epoch 27/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 98.3372 - accuracy: 0.3756 - val_loss: 62.2655 - val_accuracy: 0.3375\n",
      "Epoch 28/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 108.6938 - accuracy: 0.3302 - val_loss: 58.9369 - val_accuracy: 0.1688\n",
      "Epoch 29/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 100.1773 - accuracy: 0.3537 - val_loss: 86.5879 - val_accuracy: 0.5437\n",
      "Epoch 30/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 87.9612 - accuracy: 0.781 - 0s 36us/step - loss: 106.3528 - accuracy: 0.5696 - val_loss: 79.8189 - val_accuracy: 0.6250\n",
      "Epoch 31/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 92.8351 - accuracy: 0.5102 - val_loss: 141.7759 - val_accuracy: 0.4437\n",
      "Epoch 32/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 108.0403 - accuracy: 0.4538 - val_loss: 74.7695 - val_accuracy: 0.4812\n",
      "Epoch 33/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 100.5764 - accuracy: 0.5446 - val_loss: 61.4490 - val_accuracy: 0.3625\n",
      "Epoch 34/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 104.4210 - accuracy: 0.4726 - val_loss: 113.1207 - val_accuracy: 0.5562\n",
      "Epoch 35/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 105.7731 - accuracy: 0.5274 - val_loss: 145.1167 - val_accuracy: 0.3063\n",
      "Epoch 36/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 97.0354 - accuracy: 0.3897 - val_loss: 172.1690 - val_accuracy: 0.2562\n",
      "Epoch 37/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 101.5772 - accuracy: 0.5070 - val_loss: 137.8669 - val_accuracy: 0.5125\n",
      "Epoch 38/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 93.2245 - accuracy: 0.5884 - val_loss: 105.8795 - val_accuracy: 0.4437\n",
      "Epoch 39/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 102.0531 - accuracy: 0.5290 - val_loss: 116.4269 - val_accuracy: 0.5938\n",
      "Epoch 40/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 94.9024 - accuracy: 0.5023 - val_loss: 163.2858 - val_accuracy: 0.5437\n",
      "Epoch 41/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 90.2041 - accuracy: 0.5556 - val_loss: 77.3550 - val_accuracy: 0.2438\n",
      "Epoch 42/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 85.5745 - accuracy: 0.4914 - val_loss: 109.7515 - val_accuracy: 0.3375\n",
      "Epoch 43/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 98.0288 - accuracy: 0.4585 - val_loss: 82.6824 - val_accuracy: 0.2562\n",
      "Epoch 44/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 93.4338 - accuracy: 0.3897 - val_loss: 123.2396 - val_accuracy: 0.2375\n",
      "Epoch 45/1000\n",
      "639/639 [==============================] - 0s 52us/step - loss: 87.9058 - accuracy: 0.4820 - val_loss: 129.7939 - val_accuracy: 0.5500\n",
      "Epoch 46/1000\n",
      "639/639 [==============================] - 0s 47us/step - loss: 91.2013 - accuracy: 0.5117 - val_loss: 120.8568 - val_accuracy: 0.3000\n",
      "Epoch 47/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 99.4527 - accuracy: 0.5055 - val_loss: 40.4732 - val_accuracy: 0.5188\n",
      "Epoch 48/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 86.4813 - accuracy: 0.5415 - val_loss: 86.1451 - val_accuracy: 0.4125\n",
      "Epoch 49/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 90.9993 - accuracy: 0.5383 - val_loss: 79.2548 - val_accuracy: 0.4125\n",
      "Epoch 50/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 86.6705 - accuracy: 0.4914 - val_loss: 143.5385 - val_accuracy: 0.5437\n",
      "Epoch 51/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 97.4522 - accuracy: 0.5790 - val_loss: 67.1939 - val_accuracy: 0.4812\n",
      "Epoch 52/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 82.3859 - accuracy: 0.5618 - val_loss: 66.4082 - val_accuracy: 0.2562\n",
      "Epoch 53/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 85.3682 - accuracy: 0.4523 - val_loss: 90.1031 - val_accuracy: 0.3688\n",
      "Epoch 54/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 91.9880 - accuracy: 0.3850 - val_loss: 61.8073 - val_accuracy: 0.4375\n",
      "Epoch 55/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 89.3443 - accuracy: 0.5164 - val_loss: 83.3863 - val_accuracy: 0.4187\n",
      "Epoch 56/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 87.7545 - accuracy: 0.4789 - val_loss: 130.3053 - val_accuracy: 0.5938\n",
      "Epoch 57/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 90.9516 - accuracy: 0.5493 - val_loss: 57.1437 - val_accuracy: 0.6250\n",
      "Epoch 58/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 87.1161 - accuracy: 0.4726 - val_loss: 49.2502 - val_accuracy: 0.5250\n",
      "Epoch 59/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 81.7147 - accuracy: 0.4930 - val_loss: 163.3315 - val_accuracy: 0.3063\n",
      "Epoch 60/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 72.0189 - accuracy: 0.4945 - val_loss: 78.7533 - val_accuracy: 0.5437\n",
      "Epoch 61/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 74.1143 - accuracy: 0.687 - 0s 36us/step - loss: 87.1014 - accuracy: 0.5321 - val_loss: 160.2568 - val_accuracy: 0.5437\n",
      "Epoch 62/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 83.8417 - accuracy: 0.4867 - val_loss: 29.5215 - val_accuracy: 0.5250\n",
      "Epoch 63/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 81.5423 - accuracy: 0.4898 - val_loss: 137.0108 - val_accuracy: 0.2125\n",
      "Epoch 64/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 86.7527 - accuracy: 0.5509 - val_loss: 121.4056 - val_accuracy: 0.5375\n",
      "Epoch 65/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 86.9458 - accuracy: 0.4898 - val_loss: 116.1377 - val_accuracy: 0.2812\n",
      "Epoch 66/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 81.3717 - accuracy: 0.5180 - val_loss: 120.2290 - val_accuracy: 0.5063\n",
      "Epoch 67/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 82.9186 - accuracy: 0.5290 - val_loss: 78.8054 - val_accuracy: 0.6062\n",
      "Epoch 68/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 79.6946 - accuracy: 0.5728 - val_loss: 104.9104 - val_accuracy: 0.6062\n",
      "Epoch 69/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 86.0823 - accuracy: 0.5368 - val_loss: 24.5929 - val_accuracy: 0.6187\n",
      "Epoch 70/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 73.8096 - accuracy: 0.5196 - val_loss: 84.4517 - val_accuracy: 0.5063\n",
      "Epoch 71/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 75.1702 - accuracy: 0.5149 - val_loss: 74.1846 - val_accuracy: 0.5750\n",
      "Epoch 72/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 74.3276 - accuracy: 0.5696 - val_loss: 112.3410 - val_accuracy: 0.5688\n",
      "Epoch 73/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 84.0354 - accuracy: 0.5837 - val_loss: 114.2147 - val_accuracy: 0.5750\n",
      "Epoch 74/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 80.7893 - accuracy: 0.3772 - val_loss: 36.5599 - val_accuracy: 0.5188\n",
      "Epoch 75/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 79.5102 - accuracy: 0.5227 - val_loss: 120.5845 - val_accuracy: 0.3562\n",
      "Epoch 76/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 78.5176 - accuracy: 0.5681 - val_loss: 156.4820 - val_accuracy: 0.5312\n",
      "Epoch 77/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 78.5642 - accuracy: 0.5399 - val_loss: 94.4890 - val_accuracy: 0.3812\n",
      "Epoch 78/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 80.6263 - accuracy: 0.5352 - val_loss: 82.3480 - val_accuracy: 0.3688\n",
      "Epoch 79/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 82.3821 - accuracy: 0.5790 - val_loss: 84.0783 - val_accuracy: 0.5375\n",
      "Epoch 80/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 72.7187 - accuracy: 0.5227 - val_loss: 81.8815 - val_accuracy: 0.5125\n",
      "Epoch 81/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 76.7074 - accuracy: 0.5430 - val_loss: 73.1438 - val_accuracy: 0.5188\n",
      "Epoch 82/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 82.0714 - accuracy: 0.5493 - val_loss: 67.0655 - val_accuracy: 0.6000\n",
      "Epoch 83/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 77.2605 - accuracy: 0.4632 - val_loss: 86.9094 - val_accuracy: 0.5125\n",
      "Epoch 84/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 78.7797 - accuracy: 0.5149 - val_loss: 107.1006 - val_accuracy: 0.5500\n",
      "Epoch 85/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 79.0911 - accuracy: 0.4351 - val_loss: 42.4298 - val_accuracy: 0.3750\n",
      "Epoch 86/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 76.4366 - accuracy: 0.5383 - val_loss: 69.3528 - val_accuracy: 0.5188\n",
      "Epoch 87/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 75.7017 - accuracy: 0.5712 - val_loss: 111.9074 - val_accuracy: 0.4625\n",
      "Epoch 88/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 79.8519 - accuracy: 0.4898 - val_loss: 106.2230 - val_accuracy: 0.3938\n",
      "Epoch 89/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 81.1688 - accuracy: 0.5336 - val_loss: 77.0542 - val_accuracy: 0.5250\n",
      "Epoch 90/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 75.6281 - accuracy: 0.5290 - val_loss: 110.9150 - val_accuracy: 0.5813\n",
      "Epoch 91/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 76.0869 - accuracy: 0.5634 - val_loss: 113.4891 - val_accuracy: 0.6187\n",
      "Epoch 92/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 120.3560 - accuracy: 0.53 - 0s 39us/step - loss: 76.4068 - accuracy: 0.5603 - val_loss: 113.7483 - val_accuracy: 0.6500\n",
      "Epoch 93/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 69.2085 - accuracy: 0.5321 - val_loss: 80.1858 - val_accuracy: 0.5625\n",
      "Epoch 94/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 75.5626 - accuracy: 0.5368 - val_loss: 89.8702 - val_accuracy: 0.4875\n",
      "Epoch 95/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 74.2867 - accuracy: 0.4883 - val_loss: 81.5608 - val_accuracy: 0.4875\n",
      "Epoch 96/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 74.5121 - accuracy: 0.5634 - val_loss: 109.5295 - val_accuracy: 0.5625\n",
      "Epoch 97/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 69.2150 - accuracy: 0.5102 - val_loss: 88.6500 - val_accuracy: 0.5063\n",
      "Epoch 98/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 76.4119 - accuracy: 0.5133 - val_loss: 54.0926 - val_accuracy: 0.5375\n",
      "Epoch 99/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 69.3303 - accuracy: 0.5712 - val_loss: 89.7649 - val_accuracy: 0.5688\n",
      "Epoch 100/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 70.1992 - accuracy: 0.4820 - val_loss: 40.0016 - val_accuracy: 0.5437\n",
      "Epoch 101/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 63.7794 - accuracy: 0.5462 - val_loss: 51.2467 - val_accuracy: 0.4250\n",
      "Epoch 102/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 70.8763 - accuracy: 0.4789 - val_loss: 114.1706 - val_accuracy: 0.5625\n",
      "Epoch 103/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 69.3740 - accuracy: 0.5540 - val_loss: 80.6654 - val_accuracy: 0.4375\n",
      "Epoch 104/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 72.8107 - accuracy: 0.5415 - val_loss: 42.5753 - val_accuracy: 0.6500\n",
      "Epoch 105/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 68.4942 - accuracy: 0.6072 - val_loss: 110.6154 - val_accuracy: 0.5000\n",
      "Epoch 106/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 65.7949 - accuracy: 0.5681 - val_loss: 120.4512 - val_accuracy: 0.5562\n",
      "Epoch 107/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 74.0736 - accuracy: 0.5415 - val_loss: 91.8082 - val_accuracy: 0.6062\n",
      "Epoch 108/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 70.7771 - accuracy: 0.5884 - val_loss: 94.3797 - val_accuracy: 0.5437\n",
      "Epoch 109/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 70.4304 - accuracy: 0.6072 - val_loss: 66.0517 - val_accuracy: 0.5938\n",
      "Epoch 110/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 65.6823 - accuracy: 0.5368 - val_loss: 91.8566 - val_accuracy: 0.5188\n",
      "Epoch 111/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 64.7435 - accuracy: 0.4930 - val_loss: 88.3367 - val_accuracy: 0.5500\n",
      "Epoch 112/1000\n",
      "639/639 [==============================] - 0s 47us/step - loss: 68.8658 - accuracy: 0.5321 - val_loss: 90.5799 - val_accuracy: 0.5813\n",
      "Epoch 113/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 60.6213 - accuracy: 0.5853 - val_loss: 76.3666 - val_accuracy: 0.4625\n",
      "Epoch 114/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 66.1595 - accuracy: 0.5493 - val_loss: 37.6793 - val_accuracy: 0.5562\n",
      "Epoch 115/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 67.7293 - accuracy: 0.5305 - val_loss: 107.5946 - val_accuracy: 0.5688\n",
      "Epoch 116/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 71.4572 - accuracy: 0.5915 - val_loss: 54.0360 - val_accuracy: 0.5250\n",
      "Epoch 117/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 64.0380 - accuracy: 0.5133 - val_loss: 71.1820 - val_accuracy: 0.4812\n",
      "Epoch 118/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 62.5052 - accuracy: 0.5822 - val_loss: 86.1720 - val_accuracy: 0.5875\n",
      "Epoch 119/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 70.1356 - accuracy: 0.5149 - val_loss: 41.3060 - val_accuracy: 0.4250\n",
      "Epoch 120/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 64.1369 - accuracy: 0.5383 - val_loss: 65.6246 - val_accuracy: 0.5688\n",
      "Epoch 121/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 64.3272 - accuracy: 0.5556 - val_loss: 90.7044 - val_accuracy: 0.3812\n",
      "Epoch 122/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 64.6573 - accuracy: 0.5524 - val_loss: 34.6859 - val_accuracy: 0.6125\n",
      "Epoch 123/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 62.7129 - accuracy: 0.5869 - val_loss: 83.5833 - val_accuracy: 0.5250\n",
      "Epoch 124/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 66.3335 - accuracy: 0.5853 - val_loss: 87.6450 - val_accuracy: 0.5437\n",
      "Epoch 125/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 64.2980 - accuracy: 0.5571 - val_loss: 70.5342 - val_accuracy: 0.5437\n",
      "Epoch 126/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 66.4786 - accuracy: 0.5509 - val_loss: 90.4133 - val_accuracy: 0.5625\n",
      "Epoch 127/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 61.0852 - accuracy: 0.5931 - val_loss: 31.1422 - val_accuracy: 0.5813\n",
      "Epoch 128/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 62.5620 - accuracy: 0.5603 - val_loss: 26.5809 - val_accuracy: 0.5688\n",
      "Epoch 129/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 63.4085 - accuracy: 0.5994 - val_loss: 101.6771 - val_accuracy: 0.5625\n",
      "Epoch 130/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 57.4601 - accuracy: 0.5524 - val_loss: 55.1232 - val_accuracy: 0.5750\n",
      "Epoch 131/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 61.8780 - accuracy: 0.5524 - val_loss: 68.8973 - val_accuracy: 0.5312\n",
      "Epoch 132/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 62.3823 - accuracy: 0.5258 - val_loss: 99.7824 - val_accuracy: 0.5312\n",
      "Epoch 133/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 64.5758 - accuracy: 0.5258 - val_loss: 60.6623 - val_accuracy: 0.6250\n",
      "Epoch 134/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 64.6583 - accuracy: 0.5728 - val_loss: 81.4553 - val_accuracy: 0.4875\n",
      "Epoch 135/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 62.7009 - accuracy: 0.5446 - val_loss: 28.6685 - val_accuracy: 0.5625\n",
      "Epoch 136/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 59.8693 - accuracy: 0.5994 - val_loss: 106.8290 - val_accuracy: 0.5875\n",
      "Epoch 137/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 63.5443 - accuracy: 0.5853 - val_loss: 84.5855 - val_accuracy: 0.5625\n",
      "Epoch 138/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 62.0468 - accuracy: 0.5571 - val_loss: 101.0429 - val_accuracy: 0.4250\n",
      "Epoch 139/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 63.9112 - accuracy: 0.5603 - val_loss: 81.1415 - val_accuracy: 0.6250\n",
      "Epoch 140/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 64.3904 - accuracy: 0.5430 - val_loss: 45.5074 - val_accuracy: 0.5562\n",
      "Epoch 141/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 58.5154 - accuracy: 0.5571 - val_loss: 36.3443 - val_accuracy: 0.5625\n",
      "Epoch 142/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 58.8572 - accuracy: 0.5227 - val_loss: 66.0958 - val_accuracy: 0.6000\n",
      "Epoch 143/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 63.3111 - accuracy: 0.5743 - val_loss: 45.7724 - val_accuracy: 0.5938\n",
      "Epoch 144/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 58.8574 - accuracy: 0.5587 - val_loss: 25.6326 - val_accuracy: 0.5188\n",
      "Epoch 145/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 59.2989 - accuracy: 0.5571 - val_loss: 65.6406 - val_accuracy: 0.4625\n",
      "Epoch 146/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 61.9811 - accuracy: 0.5665 - val_loss: 68.1651 - val_accuracy: 0.4500\n",
      "Epoch 147/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 57.3704 - accuracy: 0.5665 - val_loss: 84.1090 - val_accuracy: 0.5875\n",
      "Epoch 148/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 79.8462 - accuracy: 0.656 - 0s 44us/step - loss: 59.9519 - accuracy: 0.5915 - val_loss: 24.5339 - val_accuracy: 0.5188\n",
      "Epoch 149/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 23.3307 - accuracy: 0.593 - 0s 42us/step - loss: 57.8713 - accuracy: 0.5383 - val_loss: 49.4757 - val_accuracy: 0.3875\n",
      "Epoch 150/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 58.4554 - accuracy: 0.5086 - val_loss: 52.8448 - val_accuracy: 0.5625\n",
      "Epoch 151/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 56.0965 - accuracy: 0.5837 - val_loss: 87.7875 - val_accuracy: 0.5125\n",
      "Epoch 152/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 57.8937 - accuracy: 0.5603 - val_loss: 30.2169 - val_accuracy: 0.3187\n",
      "Epoch 153/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 56.2542 - accuracy: 0.5540 - val_loss: 51.5496 - val_accuracy: 0.5625\n",
      "Epoch 154/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 58.3912 - accuracy: 0.5477 - val_loss: 47.7854 - val_accuracy: 0.6125\n",
      "Epoch 155/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 57.0760 - accuracy: 0.5712 - val_loss: 61.3288 - val_accuracy: 0.5625\n",
      "Epoch 156/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 55.0622 - accuracy: 0.5070 - val_loss: 65.1750 - val_accuracy: 0.5688\n",
      "Epoch 157/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 61.2476 - accuracy: 0.6150 - val_loss: 58.1215 - val_accuracy: 0.5562\n",
      "Epoch 158/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 58.8440 - accuracy: 0.5978 - val_loss: 38.1275 - val_accuracy: 0.6250\n",
      "Epoch 159/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 55.7466 - accuracy: 0.5540 - val_loss: 72.9404 - val_accuracy: 0.4375\n",
      "Epoch 160/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 57.7849 - accuracy: 0.5509 - val_loss: 90.0392 - val_accuracy: 0.4500\n",
      "Epoch 161/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 60.1562 - accuracy: 0.5837 - val_loss: 52.6797 - val_accuracy: 0.6125\n",
      "Epoch 162/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 60.5649 - accuracy: 0.6150 - val_loss: 23.0368 - val_accuracy: 0.5562\n",
      "Epoch 163/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 56.4432 - accuracy: 0.5869 - val_loss: 35.5462 - val_accuracy: 0.5625\n",
      "Epoch 164/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 55.2123 - accuracy: 0.5712 - val_loss: 63.5304 - val_accuracy: 0.5750\n",
      "Epoch 165/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 54.1572 - accuracy: 0.5743 - val_loss: 35.1269 - val_accuracy: 0.5562\n",
      "Epoch 166/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 53.6808 - accuracy: 0.6260 - val_loss: 30.0590 - val_accuracy: 0.6062\n",
      "Epoch 167/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 55.4922 - accuracy: 0.5649 - val_loss: 54.8324 - val_accuracy: 0.5938\n",
      "Epoch 168/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 54.9161 - accuracy: 0.5634 - val_loss: 40.1416 - val_accuracy: 0.5688\n",
      "Epoch 169/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 54.6875 - accuracy: 0.5634 - val_loss: 37.5647 - val_accuracy: 0.5750\n",
      "Epoch 170/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 55.7307 - accuracy: 0.5931 - val_loss: 39.9564 - val_accuracy: 0.5750\n",
      "Epoch 171/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 53.2377 - accuracy: 0.5900 - val_loss: 68.3418 - val_accuracy: 0.5875\n",
      "Epoch 172/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 55.5850 - accuracy: 0.5947 - val_loss: 62.7359 - val_accuracy: 0.5938\n",
      "Epoch 173/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 55.0431 - accuracy: 0.6182 - val_loss: 51.5178 - val_accuracy: 0.4500\n",
      "Epoch 174/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 53.3154 - accuracy: 0.6197 - val_loss: 46.7552 - val_accuracy: 0.6500\n",
      "Epoch 175/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 52.5924 - accuracy: 0.5430 - val_loss: 72.3692 - val_accuracy: 0.5625\n",
      "Epoch 176/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 51.4059 - accuracy: 0.6009 - val_loss: 33.7208 - val_accuracy: 0.5625\n",
      "Epoch 177/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 53.2753 - accuracy: 0.5540 - val_loss: 44.5939 - val_accuracy: 0.6187\n",
      "Epoch 178/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 54.6629 - accuracy: 0.5587 - val_loss: 19.3093 - val_accuracy: 0.5625\n",
      "Epoch 179/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 53.3909 - accuracy: 0.5790 - val_loss: 64.0564 - val_accuracy: 0.6000\n",
      "Epoch 180/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 52.4611 - accuracy: 0.5587 - val_loss: 57.0301 - val_accuracy: 0.4688\n",
      "Epoch 181/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 52.7785 - accuracy: 0.5556 - val_loss: 65.9041 - val_accuracy: 0.5750\n",
      "Epoch 182/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 53.7389 - accuracy: 0.5869 - val_loss: 46.4459 - val_accuracy: 0.5875\n",
      "Epoch 183/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 53.1654 - accuracy: 0.5477 - val_loss: 55.2769 - val_accuracy: 0.5250\n",
      "Epoch 184/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 47.7053 - accuracy: 0.5743 - val_loss: 73.6168 - val_accuracy: 0.5125\n",
      "Epoch 185/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 48.0306 - accuracy: 0.6135 - val_loss: 66.2875 - val_accuracy: 0.5437\n",
      "Epoch 186/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 52.1316 - accuracy: 0.5915 - val_loss: 51.5092 - val_accuracy: 0.5312\n",
      "Epoch 187/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 51.8934 - accuracy: 0.5853 - val_loss: 62.4669 - val_accuracy: 0.5750\n",
      "Epoch 188/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 52.8458 - accuracy: 0.5837 - val_loss: 31.5110 - val_accuracy: 0.6187\n",
      "Epoch 189/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 48.5370 - accuracy: 0.6072 - val_loss: 56.4867 - val_accuracy: 0.6313\n",
      "Epoch 190/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 53.1732 - accuracy: 0.5915 - val_loss: 56.2895 - val_accuracy: 0.5500\n",
      "Epoch 191/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 52.0178 - accuracy: 0.5634 - val_loss: 33.7256 - val_accuracy: 0.5188\n",
      "Epoch 192/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 49.1156 - accuracy: 0.5978 - val_loss: 66.7047 - val_accuracy: 0.5625\n",
      "Epoch 193/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 52.1431 - accuracy: 0.6119 - val_loss: 22.3695 - val_accuracy: 0.5688\n",
      "Epoch 194/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 49.6057 - accuracy: 0.5180 - val_loss: 77.8814 - val_accuracy: 0.4437\n",
      "Epoch 195/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 79.9530 - accuracy: 0.531 - 0s 42us/step - loss: 49.3234 - accuracy: 0.5712 - val_loss: 54.4507 - val_accuracy: 0.5813\n",
      "Epoch 196/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 49.7836 - accuracy: 0.6072 - val_loss: 79.0569 - val_accuracy: 0.5813\n",
      "Epoch 197/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 51.0545 - accuracy: 0.5869 - val_loss: 51.5962 - val_accuracy: 0.5188\n",
      "Epoch 198/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 45.5055 - accuracy: 0.5383 - val_loss: 20.1343 - val_accuracy: 0.5625\n",
      "Epoch 199/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 47.9190 - accuracy: 0.5196 - val_loss: 32.3683 - val_accuracy: 0.4625\n",
      "Epoch 200/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 50.5423 - accuracy: 0.5556 - val_loss: 42.6382 - val_accuracy: 0.5688\n",
      "Epoch 201/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 50.1431 - accuracy: 0.5493 - val_loss: 91.6545 - val_accuracy: 0.4437\n",
      "Epoch 202/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 51.3004 - accuracy: 0.6166 - val_loss: 33.4845 - val_accuracy: 0.4125\n",
      "Epoch 203/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 47.5628 - accuracy: 0.5900 - val_loss: 50.4400 - val_accuracy: 0.5625\n",
      "Epoch 204/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 50.0621 - accuracy: 0.4914 - val_loss: 73.0057 - val_accuracy: 0.5625\n",
      "Epoch 205/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 50.3798 - accuracy: 0.5728 - val_loss: 19.2193 - val_accuracy: 0.5500\n",
      "Epoch 206/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 44.1583 - accuracy: 0.5696 - val_loss: 45.4013 - val_accuracy: 0.5750\n",
      "Epoch 207/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 48.9165 - accuracy: 0.6009 - val_loss: 51.1797 - val_accuracy: 0.4938\n",
      "Epoch 208/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 49.2034 - accuracy: 0.5696 - val_loss: 24.9964 - val_accuracy: 0.5437\n",
      "Epoch 209/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 22.4376 - accuracy: 0.718 - 0s 38us/step - loss: 46.3568 - accuracy: 0.6088 - val_loss: 47.6251 - val_accuracy: 0.6062\n",
      "Epoch 210/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 47.2047 - accuracy: 0.6088 - val_loss: 52.5763 - val_accuracy: 0.5312\n",
      "Epoch 211/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 48.8208 - accuracy: 0.5368 - val_loss: 30.4779 - val_accuracy: 0.5312\n",
      "Epoch 212/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 44.9933 - accuracy: 0.5962 - val_loss: 24.6317 - val_accuracy: 0.6250\n",
      "Epoch 213/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 46.3599 - accuracy: 0.5587 - val_loss: 34.5182 - val_accuracy: 0.4500\n",
      "Epoch 214/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 45.2538 - accuracy: 0.5430 - val_loss: 64.1749 - val_accuracy: 0.5625\n",
      "Epoch 215/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 49.8075 - accuracy: 0.5712 - val_loss: 17.3300 - val_accuracy: 0.5625\n",
      "Epoch 216/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 43.0028 - accuracy: 0.5728 - val_loss: 37.8695 - val_accuracy: 0.6125\n",
      "Epoch 217/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 43.7177 - accuracy: 0.5712 - val_loss: 55.2815 - val_accuracy: 0.5500\n",
      "Epoch 218/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 44.9895 - accuracy: 0.5696 - val_loss: 23.0850 - val_accuracy: 0.5312\n",
      "Epoch 219/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 43.8138 - accuracy: 0.5681 - val_loss: 20.5296 - val_accuracy: 0.5750\n",
      "Epoch 220/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 42.9888 - accuracy: 0.6103 - val_loss: 28.8302 - val_accuracy: 0.3750\n",
      "Epoch 221/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 43.8105 - accuracy: 0.5743 - val_loss: 46.4150 - val_accuracy: 0.4313\n",
      "Epoch 222/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 42.6148 - accuracy: 0.5462 - val_loss: 77.7575 - val_accuracy: 0.5000\n",
      "Epoch 223/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 47.8758 - accuracy: 0.6244 - val_loss: 27.0431 - val_accuracy: 0.5688\n",
      "Epoch 224/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 42.9466 - accuracy: 0.5665 - val_loss: 13.9723 - val_accuracy: 0.5250\n",
      "Epoch 225/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 43.2759 - accuracy: 0.5587 - val_loss: 20.5809 - val_accuracy: 0.5625\n",
      "Epoch 226/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 41.2555 - accuracy: 0.6041 - val_loss: 49.7027 - val_accuracy: 0.5437\n",
      "Epoch 227/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 43.0925 - accuracy: 0.5978 - val_loss: 54.4426 - val_accuracy: 0.5437\n",
      "Epoch 228/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 46.1884 - accuracy: 0.5947 - val_loss: 43.9306 - val_accuracy: 0.5688\n",
      "Epoch 229/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 43.8653 - accuracy: 0.5931 - val_loss: 49.3287 - val_accuracy: 0.5750\n",
      "Epoch 230/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 43.6856 - accuracy: 0.6135 - val_loss: 65.7040 - val_accuracy: 0.5688\n",
      "Epoch 231/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 45.5053 - accuracy: 0.6135 - val_loss: 54.9720 - val_accuracy: 0.6000\n",
      "Epoch 232/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 44.8225 - accuracy: 0.5728 - val_loss: 35.1806 - val_accuracy: 0.5625\n",
      "Epoch 233/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 44.6156 - accuracy: 0.5540 - val_loss: 62.0451 - val_accuracy: 0.4125\n",
      "Epoch 234/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 45.9536 - accuracy: 0.5696 - val_loss: 37.5417 - val_accuracy: 0.6000\n",
      "Epoch 235/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 43.9454 - accuracy: 0.6119 - val_loss: 57.3865 - val_accuracy: 0.5625\n",
      "Epoch 236/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 44.5696 - accuracy: 0.5931 - val_loss: 54.9133 - val_accuracy: 0.5312\n",
      "Epoch 237/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 44.9064 - accuracy: 0.5540 - val_loss: 70.5973 - val_accuracy: 0.5250\n",
      "Epoch 238/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 43.0188 - accuracy: 0.5759 - val_loss: 25.9637 - val_accuracy: 0.4375\n",
      "Epoch 239/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 44.8750 - accuracy: 0.5947 - val_loss: 27.4353 - val_accuracy: 0.5750\n",
      "Epoch 240/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 43.7333 - accuracy: 0.5665 - val_loss: 23.7422 - val_accuracy: 0.5813\n",
      "Epoch 241/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 42.2532 - accuracy: 0.6228 - val_loss: 54.8599 - val_accuracy: 0.5250\n",
      "Epoch 242/1000\n",
      "639/639 [==============================] - 0s 50us/step - loss: 44.0447 - accuracy: 0.5900 - val_loss: 32.9390 - val_accuracy: 0.5500\n",
      "Epoch 243/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 42.7505 - accuracy: 0.6182 - val_loss: 44.9664 - val_accuracy: 0.6125\n",
      "Epoch 244/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 42.5661 - accuracy: 0.5884 - val_loss: 24.7818 - val_accuracy: 0.5312\n",
      "Epoch 245/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 38.6841 - accuracy: 0.5775 - val_loss: 58.8797 - val_accuracy: 0.5562\n",
      "Epoch 246/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 41.5292 - accuracy: 0.5915 - val_loss: 60.5061 - val_accuracy: 0.5875\n",
      "Epoch 247/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 41.9234 - accuracy: 0.5994 - val_loss: 61.3894 - val_accuracy: 0.5750\n",
      "Epoch 248/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 42.1248 - accuracy: 0.5869 - val_loss: 57.0460 - val_accuracy: 0.5813\n",
      "Epoch 249/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 44.0620 - accuracy: 0.5634 - val_loss: 48.3254 - val_accuracy: 0.5875\n",
      "Epoch 250/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 42.7809 - accuracy: 0.5853 - val_loss: 58.1889 - val_accuracy: 0.5750\n",
      "Epoch 251/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 44.1243 - accuracy: 0.5775 - val_loss: 33.7728 - val_accuracy: 0.5750\n",
      "Epoch 252/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 35.8957 - accuracy: 0.5571 - val_loss: 68.2651 - val_accuracy: 0.4750\n",
      "Epoch 253/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 41.8965 - accuracy: 0.5994 - val_loss: 30.2454 - val_accuracy: 0.5375\n",
      "Epoch 254/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 41.8380 - accuracy: 0.5806 - val_loss: 37.8111 - val_accuracy: 0.5500\n",
      "Epoch 255/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 42.5843 - accuracy: 0.5587 - val_loss: 17.6917 - val_accuracy: 0.6062\n",
      "Epoch 256/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 38.3726 - accuracy: 0.5837 - val_loss: 29.9009 - val_accuracy: 0.5625\n",
      "Epoch 257/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 38.9330 - accuracy: 0.6119 - val_loss: 78.0528 - val_accuracy: 0.5562\n",
      "Epoch 258/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 41.0524 - accuracy: 0.6119 - val_loss: 53.6038 - val_accuracy: 0.5250\n",
      "Epoch 259/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 43.4086 - accuracy: 0.5884 - val_loss: 15.1346 - val_accuracy: 0.6187\n",
      "Epoch 260/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 39.2083 - accuracy: 0.5931 - val_loss: 60.9180 - val_accuracy: 0.6125\n",
      "Epoch 261/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 41.5282 - accuracy: 0.5853 - val_loss: 43.7134 - val_accuracy: 0.6000\n",
      "Epoch 262/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 41.3445 - accuracy: 0.6244 - val_loss: 35.2851 - val_accuracy: 0.5562\n",
      "Epoch 263/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 40.5997 - accuracy: 0.5743 - val_loss: 58.4242 - val_accuracy: 0.6000\n",
      "Epoch 264/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 39.9521 - accuracy: 0.6009 - val_loss: 39.8844 - val_accuracy: 0.5500\n",
      "Epoch 265/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 40.2169 - accuracy: 0.6135 - val_loss: 46.9504 - val_accuracy: 0.6062\n",
      "Epoch 266/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 39.8768 - accuracy: 0.5947 - val_loss: 50.7563 - val_accuracy: 0.5500\n",
      "Epoch 267/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 40.7931 - accuracy: 0.5540 - val_loss: 45.6998 - val_accuracy: 0.5500\n",
      "Epoch 268/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 47.8932 - accuracy: 0.656 - 0s 45us/step - loss: 42.1869 - accuracy: 0.5806 - val_loss: 34.4616 - val_accuracy: 0.5625\n",
      "Epoch 269/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 38.6009 - accuracy: 0.6119 - val_loss: 43.6173 - val_accuracy: 0.5250\n",
      "Epoch 270/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 40.4773 - accuracy: 0.5243 - val_loss: 53.2639 - val_accuracy: 0.5375\n",
      "Epoch 271/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 39.8712 - accuracy: 0.6135 - val_loss: 42.3546 - val_accuracy: 0.5625\n",
      "Epoch 272/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 40.1281 - accuracy: 0.6338 - val_loss: 52.8045 - val_accuracy: 0.5750\n",
      "Epoch 273/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 40.4216 - accuracy: 0.6307 - val_loss: 55.8738 - val_accuracy: 0.5688\n",
      "Epoch 274/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 39.6280 - accuracy: 0.5994 - val_loss: 39.0176 - val_accuracy: 0.5500\n",
      "Epoch 275/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 38.9932 - accuracy: 0.5978 - val_loss: 53.2700 - val_accuracy: 0.5562\n",
      "Epoch 276/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 38.0958 - accuracy: 0.5869 - val_loss: 48.2392 - val_accuracy: 0.5437\n",
      "Epoch 277/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 36.5431 - accuracy: 0.5806 - val_loss: 32.3633 - val_accuracy: 0.5875\n",
      "Epoch 278/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 38.8138 - accuracy: 0.5853 - val_loss: 46.7988 - val_accuracy: 0.6187\n",
      "Epoch 279/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 37.7145 - accuracy: 0.6291 - val_loss: 15.2685 - val_accuracy: 0.6313\n",
      "Epoch 280/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 35.7996 - accuracy: 0.6338 - val_loss: 52.6591 - val_accuracy: 0.5625\n",
      "Epoch 281/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 39.6238 - accuracy: 0.6135 - val_loss: 15.5246 - val_accuracy: 0.5750\n",
      "Epoch 282/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 35.5061 - accuracy: 0.6197 - val_loss: 43.6355 - val_accuracy: 0.5750\n",
      "Epoch 283/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 37.2909 - accuracy: 0.6182 - val_loss: 40.6778 - val_accuracy: 0.5437\n",
      "Epoch 284/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 39.3204 - accuracy: 0.5790 - val_loss: 26.6924 - val_accuracy: 0.6562\n",
      "Epoch 285/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 38.6147 - accuracy: 0.5931 - val_loss: 72.0854 - val_accuracy: 0.6250\n",
      "Epoch 286/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 37.6109 - accuracy: 0.6275 - val_loss: 45.2716 - val_accuracy: 0.5813\n",
      "Epoch 287/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 35.7735 - accuracy: 0.6009 - val_loss: 32.8587 - val_accuracy: 0.5750\n",
      "Epoch 288/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 38.6583 - accuracy: 0.6103 - val_loss: 27.8007 - val_accuracy: 0.6250\n",
      "Epoch 289/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 35.0696 - accuracy: 0.5556 - val_loss: 53.4541 - val_accuracy: 0.6062\n",
      "Epoch 290/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 36.4568 - accuracy: 0.6182 - val_loss: 42.3767 - val_accuracy: 0.6313\n",
      "Epoch 291/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 36.8780 - accuracy: 0.5806 - val_loss: 36.9828 - val_accuracy: 0.5750\n",
      "Epoch 292/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 36.7685 - accuracy: 0.6197 - val_loss: 26.5008 - val_accuracy: 0.5875\n",
      "Epoch 293/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 37.3260 - accuracy: 0.6228 - val_loss: 43.3324 - val_accuracy: 0.5250\n",
      "Epoch 294/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 35.1459 - accuracy: 0.5869 - val_loss: 52.5352 - val_accuracy: 0.5000\n",
      "Epoch 295/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 36.7513 - accuracy: 0.6244 - val_loss: 57.9377 - val_accuracy: 0.4750\n",
      "Epoch 296/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 37.8133 - accuracy: 0.6182 - val_loss: 37.1472 - val_accuracy: 0.5625\n",
      "Epoch 297/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 35.3284 - accuracy: 0.5947 - val_loss: 24.7650 - val_accuracy: 0.5625\n",
      "Epoch 298/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 36.5254 - accuracy: 0.5822 - val_loss: 36.5532 - val_accuracy: 0.5250\n",
      "Epoch 299/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 36.8528 - accuracy: 0.5869 - val_loss: 32.1947 - val_accuracy: 0.4875\n",
      "Epoch 300/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 36.9144 - accuracy: 0.6228 - val_loss: 36.9307 - val_accuracy: 0.5875\n",
      "Epoch 301/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 35.8081 - accuracy: 0.6322 - val_loss: 54.7472 - val_accuracy: 0.5688\n",
      "Epoch 302/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 33.7873 - accuracy: 0.6291 - val_loss: 29.1582 - val_accuracy: 0.6062\n",
      "Epoch 303/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 34.2634 - accuracy: 0.6510 - val_loss: 27.6363 - val_accuracy: 0.5938\n",
      "Epoch 304/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 36.1100 - accuracy: 0.6275 - val_loss: 39.1866 - val_accuracy: 0.6125\n",
      "Epoch 305/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 36.0471 - accuracy: 0.5430 - val_loss: 33.2718 - val_accuracy: 0.5562\n",
      "Epoch 306/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 36.2384 - accuracy: 0.6009 - val_loss: 43.8786 - val_accuracy: 0.5688\n",
      "Epoch 307/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 37.0445 - accuracy: 0.6479 - val_loss: 37.0988 - val_accuracy: 0.5437\n",
      "Epoch 308/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 36.8175 - accuracy: 0.5947 - val_loss: 23.3899 - val_accuracy: 0.6187\n",
      "Epoch 309/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 34.5356 - accuracy: 0.6119 - val_loss: 34.8509 - val_accuracy: 0.5688\n",
      "Epoch 310/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 35.8327 - accuracy: 0.6322 - val_loss: 43.2616 - val_accuracy: 0.6062\n",
      "Epoch 311/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 35.9083 - accuracy: 0.6557 - val_loss: 27.6204 - val_accuracy: 0.5938\n",
      "Epoch 312/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 32.6328 - accuracy: 0.5806 - val_loss: 39.8009 - val_accuracy: 0.3750\n",
      "Epoch 313/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 34.3748 - accuracy: 0.5352 - val_loss: 49.6355 - val_accuracy: 0.6375\n",
      "Epoch 314/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 36.4793 - accuracy: 0.5994 - val_loss: 33.7508 - val_accuracy: 0.5813\n",
      "Epoch 315/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 35.5577 - accuracy: 0.5915 - val_loss: 42.9030 - val_accuracy: 0.5813\n",
      "Epoch 316/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 35.8331 - accuracy: 0.5915 - val_loss: 55.2228 - val_accuracy: 0.5938\n",
      "Epoch 317/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 34.3100 - accuracy: 0.6088 - val_loss: 16.9222 - val_accuracy: 0.6187\n",
      "Epoch 318/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 33.9530 - accuracy: 0.6322 - val_loss: 25.4375 - val_accuracy: 0.5188\n",
      "Epoch 319/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 33.2055 - accuracy: 0.6213 - val_loss: 35.1297 - val_accuracy: 0.5562\n",
      "Epoch 320/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 34.3458 - accuracy: 0.6369 - val_loss: 31.0413 - val_accuracy: 0.5938\n",
      "Epoch 321/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 35.7757 - accuracy: 0.6244 - val_loss: 34.2532 - val_accuracy: 0.4812\n",
      "Epoch 322/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 35.1143 - accuracy: 0.6244 - val_loss: 28.9533 - val_accuracy: 0.5875\n",
      "Epoch 323/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 33.8527 - accuracy: 0.5900 - val_loss: 20.1576 - val_accuracy: 0.5688\n",
      "Epoch 324/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 33.8738 - accuracy: 0.6244 - val_loss: 50.6152 - val_accuracy: 0.5125\n",
      "Epoch 325/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 34.3719 - accuracy: 0.6448 - val_loss: 55.2900 - val_accuracy: 0.5750\n",
      "Epoch 326/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 34.1884 - accuracy: 0.6526 - val_loss: 33.5027 - val_accuracy: 0.4875\n",
      "Epoch 327/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 34.8717 - accuracy: 0.5743 - val_loss: 60.1844 - val_accuracy: 0.5688\n",
      "Epoch 328/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 30.8087 - accuracy: 0.6307 - val_loss: 39.9599 - val_accuracy: 0.6313\n",
      "Epoch 329/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 34.3782 - accuracy: 0.5947 - val_loss: 34.8657 - val_accuracy: 0.5813\n",
      "Epoch 330/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 32.8629 - accuracy: 0.6088 - val_loss: 44.8885 - val_accuracy: 0.5813\n",
      "Epoch 331/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 33.5260 - accuracy: 0.6072 - val_loss: 36.4735 - val_accuracy: 0.5688\n",
      "Epoch 332/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 34.2385 - accuracy: 0.6119 - val_loss: 26.7241 - val_accuracy: 0.5813\n",
      "Epoch 333/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 32.7727 - accuracy: 0.6103 - val_loss: 26.5631 - val_accuracy: 0.4500\n",
      "Epoch 334/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 34.4380 - accuracy: 0.5759 - val_loss: 51.7329 - val_accuracy: 0.5625\n",
      "Epoch 335/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 33.1050 - accuracy: 0.6025 - val_loss: 43.2320 - val_accuracy: 0.5875\n",
      "Epoch 336/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 34.0878 - accuracy: 0.5822 - val_loss: 24.7879 - val_accuracy: 0.6125\n",
      "Epoch 337/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 32.6977 - accuracy: 0.6166 - val_loss: 36.9886 - val_accuracy: 0.5750\n",
      "Epoch 338/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 33.7289 - accuracy: 0.6275 - val_loss: 34.6704 - val_accuracy: 0.5750\n",
      "Epoch 339/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 31.8112 - accuracy: 0.6307 - val_loss: 52.1974 - val_accuracy: 0.6250\n",
      "Epoch 340/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 34.2792 - accuracy: 0.6009 - val_loss: 31.0572 - val_accuracy: 0.5625\n",
      "Epoch 341/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 31.8798 - accuracy: 0.6260 - val_loss: 29.1969 - val_accuracy: 0.6250\n",
      "Epoch 342/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 31.2915 - accuracy: 0.6541 - val_loss: 51.2458 - val_accuracy: 0.5875\n",
      "Epoch 343/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 34.0199 - accuracy: 0.6072 - val_loss: 37.9473 - val_accuracy: 0.5125\n",
      "Epoch 344/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 32.9032 - accuracy: 0.6260 - val_loss: 44.2358 - val_accuracy: 0.5750\n",
      "Epoch 345/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 31.5184 - accuracy: 0.6072 - val_loss: 32.5260 - val_accuracy: 0.6062\n",
      "Epoch 346/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 31.0493 - accuracy: 0.6103 - val_loss: 27.1034 - val_accuracy: 0.5562\n",
      "Epoch 347/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 30.5547 - accuracy: 0.5712 - val_loss: 35.9471 - val_accuracy: 0.5562\n",
      "Epoch 348/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 28.1851 - accuracy: 0.6228 - val_loss: 37.9866 - val_accuracy: 0.5875\n",
      "Epoch 349/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 31.9373 - accuracy: 0.6322 - val_loss: 36.4755 - val_accuracy: 0.5688\n",
      "Epoch 350/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 34.1466 - accuracy: 0.6322 - val_loss: 15.5104 - val_accuracy: 0.5500\n",
      "Epoch 351/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 31.0572 - accuracy: 0.6197 - val_loss: 42.2688 - val_accuracy: 0.5688\n",
      "Epoch 352/1000\n",
      "639/639 [==============================] - 0s 40us/step - loss: 31.4545 - accuracy: 0.6463 - val_loss: 45.1759 - val_accuracy: 0.5750\n",
      "Epoch 353/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 30.9395 - accuracy: 0.6291 - val_loss: 45.0480 - val_accuracy: 0.6187\n",
      "Epoch 354/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 33.0286 - accuracy: 0.6103 - val_loss: 28.7130 - val_accuracy: 0.5750\n",
      "Epoch 355/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 28.6467 - accuracy: 0.5931 - val_loss: 35.3902 - val_accuracy: 0.5938\n",
      "Epoch 356/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 32.8562 - accuracy: 0.6416 - val_loss: 43.0108 - val_accuracy: 0.5750\n",
      "Epoch 357/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 31.8044 - accuracy: 0.6103 - val_loss: 27.1265 - val_accuracy: 0.5813\n",
      "Epoch 358/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 29.7318 - accuracy: 0.5915 - val_loss: 40.6271 - val_accuracy: 0.5750\n",
      "Epoch 359/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 42.9045 - accuracy: 0.375 - 0s 34us/step - loss: 32.7371 - accuracy: 0.6072 - val_loss: 26.7461 - val_accuracy: 0.5938\n",
      "Epoch 360/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 31.9006 - accuracy: 0.6463 - val_loss: 47.1632 - val_accuracy: 0.5688\n",
      "Epoch 361/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 32.5573 - accuracy: 0.6135 - val_loss: 39.0352 - val_accuracy: 0.5813\n",
      "Epoch 362/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 30.4650 - accuracy: 0.6166 - val_loss: 27.9834 - val_accuracy: 0.5938\n",
      "Epoch 363/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 30.9949 - accuracy: 0.6228 - val_loss: 52.8458 - val_accuracy: 0.4875\n",
      "Epoch 364/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 31.2970 - accuracy: 0.6307 - val_loss: 30.7763 - val_accuracy: 0.5938\n",
      "Epoch 365/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 32.8191 - accuracy: 0.5978 - val_loss: 25.9750 - val_accuracy: 0.5813\n",
      "Epoch 366/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 32.0282 - accuracy: 0.6369 - val_loss: 50.9516 - val_accuracy: 0.5688\n",
      "Epoch 367/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 32.6414 - accuracy: 0.6573 - val_loss: 22.4225 - val_accuracy: 0.5750\n",
      "Epoch 368/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 30.3826 - accuracy: 0.6526 - val_loss: 19.6281 - val_accuracy: 0.5750\n",
      "Epoch 369/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 30.1073 - accuracy: 0.6291 - val_loss: 34.9269 - val_accuracy: 0.6313\n",
      "Epoch 370/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 31.4746 - accuracy: 0.5853 - val_loss: 24.5904 - val_accuracy: 0.5688\n",
      "Epoch 371/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 31.2068 - accuracy: 0.6573 - val_loss: 23.6546 - val_accuracy: 0.6375\n",
      "Epoch 372/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 30.8095 - accuracy: 0.6072 - val_loss: 28.7680 - val_accuracy: 0.6375\n",
      "Epoch 373/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 30.5446 - accuracy: 0.6135 - val_loss: 36.3359 - val_accuracy: 0.5875\n",
      "Epoch 374/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 29.4178 - accuracy: 0.6432 - val_loss: 31.1955 - val_accuracy: 0.6313\n",
      "Epoch 375/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 26.5838 - accuracy: 0.6416 - val_loss: 17.2010 - val_accuracy: 0.6125\n",
      "Epoch 376/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 29.9181 - accuracy: 0.6213 - val_loss: 26.0379 - val_accuracy: 0.5813\n",
      "Epoch 377/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 30.8990 - accuracy: 0.6682 - val_loss: 34.3019 - val_accuracy: 0.6187\n",
      "Epoch 378/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 31.2159 - accuracy: 0.6260 - val_loss: 47.1309 - val_accuracy: 0.5875\n",
      "Epoch 379/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 30.4687 - accuracy: 0.6432 - val_loss: 30.2127 - val_accuracy: 0.5875\n",
      "Epoch 380/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 27.2799 - accuracy: 0.6197 - val_loss: 30.9838 - val_accuracy: 0.5875\n",
      "Epoch 381/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 31.0466 - accuracy: 0.6401 - val_loss: 31.8646 - val_accuracy: 0.5688\n",
      "Epoch 382/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 29.7032 - accuracy: 0.6322 - val_loss: 34.6826 - val_accuracy: 0.5875\n",
      "Epoch 383/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 30.0708 - accuracy: 0.6072 - val_loss: 38.6964 - val_accuracy: 0.5750\n",
      "Epoch 384/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 31.2160 - accuracy: 0.6401 - val_loss: 22.3580 - val_accuracy: 0.5688\n",
      "Epoch 385/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 30.5700 - accuracy: 0.6056 - val_loss: 19.0197 - val_accuracy: 0.5875\n",
      "Epoch 386/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 29.9991 - accuracy: 0.6510 - val_loss: 51.1846 - val_accuracy: 0.5938\n",
      "Epoch 387/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 30.5466 - accuracy: 0.6260 - val_loss: 43.4448 - val_accuracy: 0.5813\n",
      "Epoch 388/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 30.5581 - accuracy: 0.6150 - val_loss: 42.4955 - val_accuracy: 0.6812\n",
      "Epoch 389/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 29.7035 - accuracy: 0.6338 - val_loss: 37.0493 - val_accuracy: 0.5750\n",
      "Epoch 390/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 28.7678 - accuracy: 0.6448 - val_loss: 35.6044 - val_accuracy: 0.5750\n",
      "Epoch 391/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 30.0403 - accuracy: 0.6401 - val_loss: 18.7460 - val_accuracy: 0.5625\n",
      "Epoch 392/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 25.4780 - accuracy: 0.6056 - val_loss: 33.2165 - val_accuracy: 0.5813\n",
      "Epoch 393/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 29.8693 - accuracy: 0.6244 - val_loss: 22.9162 - val_accuracy: 0.6125\n",
      "Epoch 394/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 29.1097 - accuracy: 0.6510 - val_loss: 34.5097 - val_accuracy: 0.5813\n",
      "Epoch 395/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 29.0399 - accuracy: 0.531 - 0s 41us/step - loss: 27.4583 - accuracy: 0.6228 - val_loss: 15.7255 - val_accuracy: 0.5875\n",
      "Epoch 396/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 29.2262 - accuracy: 0.5994 - val_loss: 35.2711 - val_accuracy: 0.6375\n",
      "Epoch 397/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 28.7866 - accuracy: 0.6322 - val_loss: 47.1870 - val_accuracy: 0.6375\n",
      "Epoch 398/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 29.1436 - accuracy: 0.6338 - val_loss: 35.0275 - val_accuracy: 0.6250\n",
      "Epoch 399/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 28.3051 - accuracy: 0.6541 - val_loss: 26.5342 - val_accuracy: 0.6000\n",
      "Epoch 400/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 27.3865 - accuracy: 0.5931 - val_loss: 15.3621 - val_accuracy: 0.5500\n",
      "Epoch 401/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 27.5534 - accuracy: 0.6291 - val_loss: 32.0627 - val_accuracy: 0.5813\n",
      "Epoch 402/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 29.6413 - accuracy: 0.6620 - val_loss: 40.9633 - val_accuracy: 0.5938\n",
      "Epoch 403/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 28.9781 - accuracy: 0.6088 - val_loss: 37.6191 - val_accuracy: 0.5750\n",
      "Epoch 404/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 28.8824 - accuracy: 0.6182 - val_loss: 12.0963 - val_accuracy: 0.6438\n",
      "Epoch 405/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 26.6215 - accuracy: 0.6275 - val_loss: 35.1473 - val_accuracy: 0.5500\n",
      "Epoch 406/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 28.0486 - accuracy: 0.6354 - val_loss: 36.6418 - val_accuracy: 0.6250\n",
      "Epoch 407/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 27.7627 - accuracy: 0.6103 - val_loss: 32.7265 - val_accuracy: 0.6000\n",
      "Epoch 408/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 28.7722 - accuracy: 0.6307 - val_loss: 37.8093 - val_accuracy: 0.5938\n",
      "Epoch 409/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 25.5919 - accuracy: 0.6088 - val_loss: 42.9018 - val_accuracy: 0.5938\n",
      "Epoch 410/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 27.2411 - accuracy: 0.6275 - val_loss: 47.0510 - val_accuracy: 0.6250\n",
      "Epoch 411/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 26.9270 - accuracy: 0.6573 - val_loss: 36.2498 - val_accuracy: 0.5875\n",
      "Epoch 412/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 28.3572 - accuracy: 0.6166 - val_loss: 19.1546 - val_accuracy: 0.6187\n",
      "Epoch 413/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 28.8010 - accuracy: 0.5947 - val_loss: 21.1216 - val_accuracy: 0.5625\n",
      "Epoch 414/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 28.4543 - accuracy: 0.5962 - val_loss: 22.4307 - val_accuracy: 0.5875\n",
      "Epoch 415/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 27.9943 - accuracy: 0.6448 - val_loss: 35.2411 - val_accuracy: 0.5688\n",
      "Epoch 416/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 28.6462 - accuracy: 0.6135 - val_loss: 35.8272 - val_accuracy: 0.6250\n",
      "Epoch 417/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 29.1622 - accuracy: 0.5869 - val_loss: 17.6558 - val_accuracy: 0.5875\n",
      "Epoch 418/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 28.3915 - accuracy: 0.6307 - val_loss: 23.8754 - val_accuracy: 0.5688\n",
      "Epoch 419/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 27.1662 - accuracy: 0.6322 - val_loss: 33.2212 - val_accuracy: 0.5437\n",
      "Epoch 420/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 26.7614 - accuracy: 0.6307 - val_loss: 11.7441 - val_accuracy: 0.4938\n",
      "Epoch 421/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 26.5983 - accuracy: 0.6479 - val_loss: 27.6642 - val_accuracy: 0.5375\n",
      "Epoch 422/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 27.3366 - accuracy: 0.6354 - val_loss: 33.3388 - val_accuracy: 0.5688\n",
      "Epoch 423/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 27.2170 - accuracy: 0.6166 - val_loss: 33.1668 - val_accuracy: 0.6250\n",
      "Epoch 424/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 26.0758 - accuracy: 0.6526 - val_loss: 20.3223 - val_accuracy: 0.6000\n",
      "Epoch 425/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 27.5799 - accuracy: 0.6150 - val_loss: 22.6618 - val_accuracy: 0.5688\n",
      "Epoch 426/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 28.4954 - accuracy: 0.6479 - val_loss: 29.5780 - val_accuracy: 0.5938\n",
      "Epoch 427/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 27.3155 - accuracy: 0.6275 - val_loss: 39.2118 - val_accuracy: 0.5938\n",
      "Epoch 428/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 28.0601 - accuracy: 0.6401 - val_loss: 17.2558 - val_accuracy: 0.5250\n",
      "Epoch 429/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 27.3175 - accuracy: 0.6260 - val_loss: 19.1360 - val_accuracy: 0.5562\n",
      "Epoch 430/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 26.5667 - accuracy: 0.6510 - val_loss: 28.9955 - val_accuracy: 0.6562\n",
      "Epoch 431/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 27.6718 - accuracy: 0.6401 - val_loss: 15.7876 - val_accuracy: 0.5875\n",
      "Epoch 432/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 25.9256 - accuracy: 0.6385 - val_loss: 23.4949 - val_accuracy: 0.5750\n",
      "Epoch 433/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 26.0813 - accuracy: 0.6526 - val_loss: 14.3121 - val_accuracy: 0.5875\n",
      "Epoch 434/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 26.4616 - accuracy: 0.6072 - val_loss: 23.2509 - val_accuracy: 0.5875\n",
      "Epoch 435/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 26.1776 - accuracy: 0.6416 - val_loss: 12.7715 - val_accuracy: 0.6062\n",
      "Epoch 436/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 24.5859 - accuracy: 0.6072 - val_loss: 19.1106 - val_accuracy: 0.5813\n",
      "Epoch 437/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 26.8991 - accuracy: 0.6573 - val_loss: 17.5722 - val_accuracy: 0.6625\n",
      "Epoch 438/1000\n",
      "639/639 [==============================] - 0s 48us/step - loss: 26.1754 - accuracy: 0.6385 - val_loss: 29.1311 - val_accuracy: 0.5875\n",
      "Epoch 439/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 27.1811 - accuracy: 0.6557 - val_loss: 43.2934 - val_accuracy: 0.5750\n",
      "Epoch 440/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 24.9464 - accuracy: 0.6275 - val_loss: 42.3830 - val_accuracy: 0.5938\n",
      "Epoch 441/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 27.6742 - accuracy: 0.6072 - val_loss: 11.9369 - val_accuracy: 0.6250\n",
      "Epoch 442/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 26.2600 - accuracy: 0.6573 - val_loss: 17.5450 - val_accuracy: 0.6313\n",
      "Epoch 443/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 22.9124 - accuracy: 0.6228 - val_loss: 28.2825 - val_accuracy: 0.5875\n",
      "Epoch 444/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 25.6217 - accuracy: 0.6307 - val_loss: 36.6287 - val_accuracy: 0.5813\n",
      "Epoch 445/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 26.5398 - accuracy: 0.6228 - val_loss: 29.8411 - val_accuracy: 0.6250\n",
      "Epoch 446/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 26.3632 - accuracy: 0.5978 - val_loss: 11.9599 - val_accuracy: 0.6250\n",
      "Epoch 447/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 24.3020 - accuracy: 0.6541 - val_loss: 24.0737 - val_accuracy: 0.5938\n",
      "Epoch 448/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 27.3331 - accuracy: 0.5978 - val_loss: 39.5413 - val_accuracy: 0.5688\n",
      "Epoch 449/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 27.2227 - accuracy: 0.6103 - val_loss: 26.7729 - val_accuracy: 0.6313\n",
      "Epoch 450/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 26.5422 - accuracy: 0.6244 - val_loss: 26.3913 - val_accuracy: 0.6187\n",
      "Epoch 451/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 27.3091 - accuracy: 0.6275 - val_loss: 31.4536 - val_accuracy: 0.6187\n",
      "Epoch 452/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 27.0406 - accuracy: 0.6119 - val_loss: 29.4929 - val_accuracy: 0.6313\n",
      "Epoch 453/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 25.3630 - accuracy: 0.6166 - val_loss: 12.2588 - val_accuracy: 0.5750\n",
      "Epoch 454/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 24.4844 - accuracy: 0.6291 - val_loss: 35.0737 - val_accuracy: 0.6125\n",
      "Epoch 455/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 25.6192 - accuracy: 0.6448 - val_loss: 22.2374 - val_accuracy: 0.5625\n",
      "Epoch 456/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 25.9470 - accuracy: 0.6244 - val_loss: 18.2664 - val_accuracy: 0.5813\n",
      "Epoch 457/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 26.0706 - accuracy: 0.6307 - val_loss: 16.3657 - val_accuracy: 0.7000\n",
      "Epoch 458/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 24.8595 - accuracy: 0.6761 - val_loss: 37.6683 - val_accuracy: 0.5813\n",
      "Epoch 459/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 26.4529 - accuracy: 0.6635 - val_loss: 29.6020 - val_accuracy: 0.5813\n",
      "Epoch 460/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 25.7072 - accuracy: 0.6401 - val_loss: 29.3605 - val_accuracy: 0.5813\n",
      "Epoch 461/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 26.1066 - accuracy: 0.6244 - val_loss: 13.4331 - val_accuracy: 0.5938\n",
      "Epoch 462/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 25.4921 - accuracy: 0.6103 - val_loss: 13.2435 - val_accuracy: 0.5875\n",
      "Epoch 463/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 25.2263 - accuracy: 0.6448 - val_loss: 25.7132 - val_accuracy: 0.6438\n",
      "Epoch 464/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 26.6306 - accuracy: 0.6510 - val_loss: 25.0117 - val_accuracy: 0.6250\n",
      "Epoch 465/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 25.7437 - accuracy: 0.6354 - val_loss: 31.0674 - val_accuracy: 0.6687\n",
      "Epoch 466/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 26.8400 - accuracy: 0.6275 - val_loss: 18.5185 - val_accuracy: 0.5875\n",
      "Epoch 467/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 24.4626 - accuracy: 0.6401 - val_loss: 17.2146 - val_accuracy: 0.5750\n",
      "Epoch 468/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 24.4018 - accuracy: 0.6338 - val_loss: 20.9537 - val_accuracy: 0.5625\n",
      "Epoch 469/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 24.7529 - accuracy: 0.6557 - val_loss: 41.7009 - val_accuracy: 0.6250\n",
      "Epoch 470/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 25.0928 - accuracy: 0.6385 - val_loss: 29.4341 - val_accuracy: 0.5813\n",
      "Epoch 471/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 26.0423 - accuracy: 0.6495 - val_loss: 12.3226 - val_accuracy: 0.6250\n",
      "Epoch 472/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 24.0357 - accuracy: 0.6698 - val_loss: 34.5995 - val_accuracy: 0.5625\n",
      "Epoch 473/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 24.2759 - accuracy: 0.6275 - val_loss: 32.2162 - val_accuracy: 0.5938\n",
      "Epoch 474/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 24.8627 - accuracy: 0.6557 - val_loss: 28.6322 - val_accuracy: 0.6000\n",
      "Epoch 475/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 25.7655 - accuracy: 0.6651 - val_loss: 37.0362 - val_accuracy: 0.6250\n",
      "Epoch 476/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 25.7491 - accuracy: 0.6275 - val_loss: 27.8296 - val_accuracy: 0.6250\n",
      "Epoch 477/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 24.8256 - accuracy: 0.6354 - val_loss: 33.2366 - val_accuracy: 0.5875\n",
      "Epoch 478/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 25.0526 - accuracy: 0.6088 - val_loss: 27.4965 - val_accuracy: 0.6812\n",
      "Epoch 479/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 24.8396 - accuracy: 0.6682 - val_loss: 36.3250 - val_accuracy: 0.5625\n",
      "Epoch 480/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 25.5791 - accuracy: 0.6150 - val_loss: 37.8782 - val_accuracy: 0.6250\n",
      "Epoch 481/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 25.1331 - accuracy: 0.6635 - val_loss: 20.7938 - val_accuracy: 0.6000\n",
      "Epoch 482/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 23.0736 - accuracy: 0.6588 - val_loss: 29.1107 - val_accuracy: 0.6125\n",
      "Epoch 483/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 25.3551 - accuracy: 0.6541 - val_loss: 21.5192 - val_accuracy: 0.5625\n",
      "Epoch 484/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 24.5884 - accuracy: 0.6291 - val_loss: 11.9676 - val_accuracy: 0.6187\n",
      "Epoch 485/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 24.6398 - accuracy: 0.6416 - val_loss: 19.1128 - val_accuracy: 0.7063\n",
      "Epoch 486/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 18.2366 - accuracy: 0.843 - 0s 42us/step - loss: 25.1157 - accuracy: 0.6745 - val_loss: 13.7487 - val_accuracy: 0.6062\n",
      "Epoch 487/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 24.4126 - accuracy: 0.6557 - val_loss: 23.4939 - val_accuracy: 0.6938\n",
      "Epoch 488/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 25.4710 - accuracy: 0.6604 - val_loss: 12.1237 - val_accuracy: 0.6250\n",
      "Epoch 489/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 23.4468 - accuracy: 0.6401 - val_loss: 23.1998 - val_accuracy: 0.5750\n",
      "Epoch 490/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 23.6603 - accuracy: 0.6322 - val_loss: 29.4413 - val_accuracy: 0.5938\n",
      "Epoch 491/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 24.6897 - accuracy: 0.6228 - val_loss: 26.6387 - val_accuracy: 0.5500\n",
      "Epoch 492/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 24.0578 - accuracy: 0.6651 - val_loss: 19.7395 - val_accuracy: 0.5750\n",
      "Epoch 493/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 21.5428 - accuracy: 0.6448 - val_loss: 21.8695 - val_accuracy: 0.5688\n",
      "Epoch 494/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 24.4118 - accuracy: 0.6103 - val_loss: 28.9778 - val_accuracy: 0.6625\n",
      "Epoch 495/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 23.7656 - accuracy: 0.6526 - val_loss: 25.1085 - val_accuracy: 0.6187\n",
      "Epoch 496/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 25.0892 - accuracy: 0.6698 - val_loss: 21.2947 - val_accuracy: 0.5750\n",
      "Epoch 497/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 22.8698 - accuracy: 0.6448 - val_loss: 40.2555 - val_accuracy: 0.5813\n",
      "Epoch 498/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 22.5166 - accuracy: 0.6369 - val_loss: 28.3208 - val_accuracy: 0.5813\n",
      "Epoch 499/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 22.6192 - accuracy: 0.6573 - val_loss: 22.7964 - val_accuracy: 0.6625\n",
      "Epoch 500/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 23.5004 - accuracy: 0.6839 - val_loss: 40.0599 - val_accuracy: 0.6062\n",
      "Epoch 501/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 21.8878 - accuracy: 0.6980 - val_loss: 22.5456 - val_accuracy: 0.6187\n",
      "Epoch 502/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 24.3340 - accuracy: 0.6854 - val_loss: 11.0467 - val_accuracy: 0.6062\n",
      "Epoch 503/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 21.7890 - accuracy: 0.6761 - val_loss: 32.9094 - val_accuracy: 0.5625\n",
      "Epoch 504/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 23.8873 - accuracy: 0.6635 - val_loss: 15.9841 - val_accuracy: 0.6000\n",
      "Epoch 505/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 22.7392 - accuracy: 0.6495 - val_loss: 26.4636 - val_accuracy: 0.6313\n",
      "Epoch 506/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 25.0326 - accuracy: 0.6119 - val_loss: 22.0511 - val_accuracy: 0.6313\n",
      "Epoch 507/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 23.0303 - accuracy: 0.6776 - val_loss: 19.8019 - val_accuracy: 0.5562\n",
      "Epoch 508/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 23.3704 - accuracy: 0.6260 - val_loss: 22.7556 - val_accuracy: 0.5813\n",
      "Epoch 509/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 24.1524 - accuracy: 0.6307 - val_loss: 21.2464 - val_accuracy: 0.6500\n",
      "Epoch 510/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 24.1152 - accuracy: 0.6338 - val_loss: 21.2074 - val_accuracy: 0.5813\n",
      "Epoch 511/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 22.7949 - accuracy: 0.6761 - val_loss: 27.1288 - val_accuracy: 0.6438\n",
      "Epoch 512/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 27.1799 - accuracy: 0.718 - 0s 41us/step - loss: 23.5900 - accuracy: 0.6667 - val_loss: 16.4298 - val_accuracy: 0.6313\n",
      "Epoch 513/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 14.2444 - accuracy: 0.687 - 0s 42us/step - loss: 23.2630 - accuracy: 0.6808 - val_loss: 26.9280 - val_accuracy: 0.5875\n",
      "Epoch 514/1000\n",
      "639/639 [==============================] - 0s 48us/step - loss: 24.3112 - accuracy: 0.6557 - val_loss: 25.2904 - val_accuracy: 0.6250\n",
      "Epoch 515/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 23.6713 - accuracy: 0.6322 - val_loss: 37.5482 - val_accuracy: 0.6125\n",
      "Epoch 516/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 21.8955 - accuracy: 0.6651 - val_loss: 29.7405 - val_accuracy: 0.5750\n",
      "Epoch 517/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 22.6376 - accuracy: 0.6322 - val_loss: 31.7458 - val_accuracy: 0.6187\n",
      "Epoch 518/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 21.7122 - accuracy: 0.6541 - val_loss: 32.2892 - val_accuracy: 0.6187\n",
      "Epoch 519/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 23.4471 - accuracy: 0.7027 - val_loss: 17.2559 - val_accuracy: 0.6000\n",
      "Epoch 520/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 23.8999 - accuracy: 0.6072 - val_loss: 20.6165 - val_accuracy: 0.6250\n",
      "Epoch 521/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 23.6308 - accuracy: 0.6714 - val_loss: 28.4285 - val_accuracy: 0.5813\n",
      "Epoch 522/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 24.4860 - accuracy: 0.6620 - val_loss: 17.7248 - val_accuracy: 0.5813\n",
      "Epoch 523/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 24.0876 - accuracy: 0.6479 - val_loss: 15.7368 - val_accuracy: 0.5437\n",
      "Epoch 524/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 22.9121 - accuracy: 0.6432 - val_loss: 17.8808 - val_accuracy: 0.5875\n",
      "Epoch 525/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 21.9882 - accuracy: 0.6228 - val_loss: 31.0967 - val_accuracy: 0.5813\n",
      "Epoch 526/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 23.1958 - accuracy: 0.6682 - val_loss: 16.5931 - val_accuracy: 0.5437\n",
      "Epoch 527/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 22.3225 - accuracy: 0.6416 - val_loss: 13.4438 - val_accuracy: 0.6250\n",
      "Epoch 528/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 23.1886 - accuracy: 0.6416 - val_loss: 21.5928 - val_accuracy: 0.5625\n",
      "Epoch 529/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 23.4423 - accuracy: 0.6588 - val_loss: 28.8394 - val_accuracy: 0.5625\n",
      "Epoch 530/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 23.8109 - accuracy: 0.6291 - val_loss: 16.4773 - val_accuracy: 0.6187\n",
      "Epoch 531/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 21.3372 - accuracy: 0.6338 - val_loss: 31.2674 - val_accuracy: 0.6375\n",
      "Epoch 532/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 22.7611 - accuracy: 0.6228 - val_loss: 33.4468 - val_accuracy: 0.5688\n",
      "Epoch 533/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 22.9898 - accuracy: 0.6854 - val_loss: 21.6851 - val_accuracy: 0.6875\n",
      "Epoch 534/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 21.7247 - accuracy: 0.6698 - val_loss: 32.1434 - val_accuracy: 0.5750\n",
      "Epoch 535/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 23.9933 - accuracy: 0.5994 - val_loss: 23.6703 - val_accuracy: 0.5875\n",
      "Epoch 536/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 23.7261 - accuracy: 0.6510 - val_loss: 18.9325 - val_accuracy: 0.6250\n",
      "Epoch 537/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 22.5878 - accuracy: 0.6307 - val_loss: 26.8588 - val_accuracy: 0.6125\n",
      "Epoch 538/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 24.3221 - accuracy: 0.6260 - val_loss: 13.9999 - val_accuracy: 0.5688\n",
      "Epoch 539/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 22.0077 - accuracy: 0.6135 - val_loss: 44.0707 - val_accuracy: 0.5875\n",
      "Epoch 540/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 20.5165 - accuracy: 0.6541 - val_loss: 37.6331 - val_accuracy: 0.5500\n",
      "Epoch 541/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 21.2862 - accuracy: 0.6604 - val_loss: 30.2876 - val_accuracy: 0.6000\n",
      "Epoch 542/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 23.1826 - accuracy: 0.6354 - val_loss: 25.8853 - val_accuracy: 0.5875\n",
      "Epoch 543/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 21.7509 - accuracy: 0.6416 - val_loss: 34.2803 - val_accuracy: 0.5875\n",
      "Epoch 544/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 21.6343 - accuracy: 0.6448 - val_loss: 29.0214 - val_accuracy: 0.6313\n",
      "Epoch 545/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 22.8978 - accuracy: 0.6322 - val_loss: 20.5079 - val_accuracy: 0.6375\n",
      "Epoch 546/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 21.9966 - accuracy: 0.6401 - val_loss: 22.4276 - val_accuracy: 0.5813\n",
      "Epoch 547/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 23.1593 - accuracy: 0.6620 - val_loss: 17.8681 - val_accuracy: 0.5938\n",
      "Epoch 548/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 21.3919 - accuracy: 0.6839 - val_loss: 35.3566 - val_accuracy: 0.5875\n",
      "Epoch 549/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 22.0946 - accuracy: 0.6495 - val_loss: 27.4875 - val_accuracy: 0.5750\n",
      "Epoch 550/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 22.7458 - accuracy: 0.6448 - val_loss: 22.6395 - val_accuracy: 0.5813\n",
      "Epoch 551/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 22.5797 - accuracy: 0.6510 - val_loss: 22.7432 - val_accuracy: 0.6625\n",
      "Epoch 552/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 22.6966 - accuracy: 0.6510 - val_loss: 26.9124 - val_accuracy: 0.6000\n",
      "Epoch 553/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 21.8559 - accuracy: 0.6072 - val_loss: 29.9452 - val_accuracy: 0.5750\n",
      "Epoch 554/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 26.8581 - accuracy: 0.656 - 0s 33us/step - loss: 22.3710 - accuracy: 0.6510 - val_loss: 13.2400 - val_accuracy: 0.5688\n",
      "Epoch 555/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 20.6878 - accuracy: 0.6213 - val_loss: 15.8565 - val_accuracy: 0.5875\n",
      "Epoch 556/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 22.5244 - accuracy: 0.6573 - val_loss: 23.2587 - val_accuracy: 0.5875\n",
      "Epoch 557/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 23.2594 - accuracy: 0.6588 - val_loss: 24.4485 - val_accuracy: 0.5375\n",
      "Epoch 558/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 21.8613 - accuracy: 0.6667 - val_loss: 24.3752 - val_accuracy: 0.6187\n",
      "Epoch 559/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 21.5605 - accuracy: 0.6839 - val_loss: 20.5588 - val_accuracy: 0.6313\n",
      "Epoch 560/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 22.6900 - accuracy: 0.6698 - val_loss: 28.6790 - val_accuracy: 0.6062\n",
      "Epoch 561/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.6131 - accuracy: 0.6369 - val_loss: 25.1152 - val_accuracy: 0.5938\n",
      "Epoch 562/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 22.2473 - accuracy: 0.6667 - val_loss: 16.3177 - val_accuracy: 0.6062\n",
      "Epoch 563/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 22.6501 - accuracy: 0.6682 - val_loss: 28.3770 - val_accuracy: 0.7250\n",
      "Epoch 564/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 22.2286 - accuracy: 0.6948 - val_loss: 15.8150 - val_accuracy: 0.6250\n",
      "Epoch 565/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.0433 - accuracy: 0.6244 - val_loss: 27.5141 - val_accuracy: 0.5750\n",
      "Epoch 566/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 22.6086 - accuracy: 0.6291 - val_loss: 13.5811 - val_accuracy: 0.5938\n",
      "Epoch 567/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 21.7164 - accuracy: 0.6244 - val_loss: 22.0657 - val_accuracy: 0.6000\n",
      "Epoch 568/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 22.1104 - accuracy: 0.6588 - val_loss: 15.4003 - val_accuracy: 0.6000\n",
      "Epoch 569/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.1705 - accuracy: 0.6495 - val_loss: 23.8896 - val_accuracy: 0.5500\n",
      "Epoch 570/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 22.3454 - accuracy: 0.6510 - val_loss: 10.5731 - val_accuracy: 0.6313\n",
      "Epoch 571/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 20.3050 - accuracy: 0.6354 - val_loss: 23.4504 - val_accuracy: 0.6000\n",
      "Epoch 572/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 22.8224 - accuracy: 0.750 - 0s 36us/step - loss: 21.8203 - accuracy: 0.6635 - val_loss: 22.0421 - val_accuracy: 0.5938\n",
      "Epoch 573/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 21.9978 - accuracy: 0.6260 - val_loss: 16.7523 - val_accuracy: 0.5750\n",
      "Epoch 574/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 21.2069 - accuracy: 0.6401 - val_loss: 23.5377 - val_accuracy: 0.6187\n",
      "Epoch 575/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.8490 - accuracy: 0.6322 - val_loss: 15.1828 - val_accuracy: 0.6125\n",
      "Epoch 576/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 20.0943 - accuracy: 0.6479 - val_loss: 18.9758 - val_accuracy: 0.6438\n",
      "Epoch 577/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 19.8798 - accuracy: 0.6588 - val_loss: 26.2013 - val_accuracy: 0.5375\n",
      "Epoch 578/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 22.2700 - accuracy: 0.6604 - val_loss: 27.5359 - val_accuracy: 0.5688\n",
      "Epoch 579/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.8933 - accuracy: 0.6416 - val_loss: 24.9432 - val_accuracy: 0.6625\n",
      "Epoch 580/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 21.4725 - accuracy: 0.6479 - val_loss: 15.5907 - val_accuracy: 0.5625\n",
      "Epoch 581/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.4479 - accuracy: 0.6260 - val_loss: 22.1700 - val_accuracy: 0.5938\n",
      "Epoch 582/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.7783 - accuracy: 0.6635 - val_loss: 19.5280 - val_accuracy: 0.5750\n",
      "Epoch 583/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 22.1136 - accuracy: 0.6338 - val_loss: 25.4356 - val_accuracy: 0.6375\n",
      "Epoch 584/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.9205 - accuracy: 0.6745 - val_loss: 21.3963 - val_accuracy: 0.6062\n",
      "Epoch 585/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 19.8745 - accuracy: 0.6573 - val_loss: 21.8450 - val_accuracy: 0.6000\n",
      "Epoch 586/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 21.9989 - accuracy: 0.6479 - val_loss: 17.1777 - val_accuracy: 0.5188\n",
      "Epoch 587/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.5571 - accuracy: 0.6432 - val_loss: 13.0198 - val_accuracy: 0.5813\n",
      "Epoch 588/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 21.7924 - accuracy: 0.6291 - val_loss: 21.9849 - val_accuracy: 0.6000\n",
      "Epoch 589/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 21.9605 - accuracy: 0.6307 - val_loss: 13.6902 - val_accuracy: 0.6187\n",
      "Epoch 590/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 20.6474 - accuracy: 0.6432 - val_loss: 19.7936 - val_accuracy: 0.6250\n",
      "Epoch 591/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.5493 - accuracy: 0.6948 - val_loss: 18.4578 - val_accuracy: 0.6750\n",
      "Epoch 592/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.8269 - accuracy: 0.6385 - val_loss: 27.5049 - val_accuracy: 0.5938\n",
      "Epoch 593/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.4798 - accuracy: 0.6338 - val_loss: 10.7209 - val_accuracy: 0.5750\n",
      "Epoch 594/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 21.0193 - accuracy: 0.6307 - val_loss: 18.4213 - val_accuracy: 0.6062\n",
      "Epoch 595/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 19.8686 - accuracy: 0.6322 - val_loss: 23.2639 - val_accuracy: 0.6250\n",
      "Epoch 596/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.9870 - accuracy: 0.6573 - val_loss: 18.6327 - val_accuracy: 0.6313\n",
      "Epoch 597/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 20.1396 - accuracy: 0.6667 - val_loss: 15.5666 - val_accuracy: 0.6438\n",
      "Epoch 598/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 19.6324 - accuracy: 0.6354 - val_loss: 14.1440 - val_accuracy: 0.6375\n",
      "Epoch 599/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 20.6042 - accuracy: 0.6651 - val_loss: 21.4414 - val_accuracy: 0.6062\n",
      "Epoch 600/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.2651 - accuracy: 0.6588 - val_loss: 20.9889 - val_accuracy: 0.5188\n",
      "Epoch 601/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.4028 - accuracy: 0.6573 - val_loss: 20.0863 - val_accuracy: 0.5938\n",
      "Epoch 602/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 20.7339 - accuracy: 0.6682 - val_loss: 14.5123 - val_accuracy: 0.5750\n",
      "Epoch 603/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.6475 - accuracy: 0.6635 - val_loss: 13.9177 - val_accuracy: 0.5813\n",
      "Epoch 604/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 21.2023 - accuracy: 0.6432 - val_loss: 17.2470 - val_accuracy: 0.5938\n",
      "Epoch 605/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 20.3179 - accuracy: 0.6495 - val_loss: 25.8512 - val_accuracy: 0.5813\n",
      "Epoch 606/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.8731 - accuracy: 0.6479 - val_loss: 19.2413 - val_accuracy: 0.5813\n",
      "Epoch 607/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 21.3604 - accuracy: 0.6557 - val_loss: 21.6313 - val_accuracy: 0.6313\n",
      "Epoch 608/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 21.3903 - accuracy: 0.6620 - val_loss: 26.0054 - val_accuracy: 0.5938\n",
      "Epoch 609/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 26.8039 - accuracy: 0.656 - 0s 33us/step - loss: 19.6120 - accuracy: 0.6448 - val_loss: 11.0104 - val_accuracy: 0.5688\n",
      "Epoch 610/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 19.4649 - accuracy: 0.6573 - val_loss: 11.6733 - val_accuracy: 0.6938\n",
      "Epoch 611/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 21.0315 - accuracy: 0.6510 - val_loss: 29.8916 - val_accuracy: 0.5813\n",
      "Epoch 612/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 20.7307 - accuracy: 0.6682 - val_loss: 26.8672 - val_accuracy: 0.6750\n",
      "Epoch 613/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 21.0456 - accuracy: 0.6244 - val_loss: 26.6282 - val_accuracy: 0.6812\n",
      "Epoch 614/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.5492 - accuracy: 0.6244 - val_loss: 22.6644 - val_accuracy: 0.5875\n",
      "Epoch 615/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 20.1625 - accuracy: 0.6354 - val_loss: 32.1959 - val_accuracy: 0.5562\n",
      "Epoch 616/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 21.0571 - accuracy: 0.6682 - val_loss: 24.8624 - val_accuracy: 0.5625\n",
      "Epoch 617/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 20.9543 - accuracy: 0.6307 - val_loss: 14.8265 - val_accuracy: 0.6187\n",
      "Epoch 618/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 20.2186 - accuracy: 0.6354 - val_loss: 21.5871 - val_accuracy: 0.6062\n",
      "Epoch 619/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 20.4934 - accuracy: 0.6588 - val_loss: 52.9437 - val_accuracy: 0.5813\n",
      "Epoch 620/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 22.3634 - accuracy: 0.6510 - val_loss: 23.6016 - val_accuracy: 0.5875\n",
      "Epoch 621/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 21.3431 - accuracy: 0.6401 - val_loss: 17.8507 - val_accuracy: 0.6250\n",
      "Epoch 622/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 20.3018 - accuracy: 0.6463 - val_loss: 26.1356 - val_accuracy: 0.6313\n",
      "Epoch 623/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 24.5164 - accuracy: 0.750 - 0s 38us/step - loss: 21.2155 - accuracy: 0.6432 - val_loss: 15.8446 - val_accuracy: 0.6125\n",
      "Epoch 624/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 18.9408 - accuracy: 0.6745 - val_loss: 19.2286 - val_accuracy: 0.5625\n",
      "Epoch 625/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 19.6546 - accuracy: 0.6933 - val_loss: 30.1448 - val_accuracy: 0.6125\n",
      "Epoch 626/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.2484 - accuracy: 0.6557 - val_loss: 12.1185 - val_accuracy: 0.5875\n",
      "Epoch 627/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 18.1659 - accuracy: 0.6729 - val_loss: 21.1425 - val_accuracy: 0.5688\n",
      "Epoch 628/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.9400 - accuracy: 0.6463 - val_loss: 20.7467 - val_accuracy: 0.5562\n",
      "Epoch 629/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.8250 - accuracy: 0.6635 - val_loss: 25.0037 - val_accuracy: 0.5688\n",
      "Epoch 630/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 20.4636 - accuracy: 0.6401 - val_loss: 16.8647 - val_accuracy: 0.5688\n",
      "Epoch 631/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 20.8510 - accuracy: 0.6479 - val_loss: 16.3157 - val_accuracy: 0.5750\n",
      "Epoch 632/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 20.5179 - accuracy: 0.6729 - val_loss: 20.2790 - val_accuracy: 0.5813\n",
      "Epoch 633/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 19.5080 - accuracy: 0.6761 - val_loss: 22.7338 - val_accuracy: 0.5875\n",
      "Epoch 634/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 19.4646 - accuracy: 0.6635 - val_loss: 30.8774 - val_accuracy: 0.5938\n",
      "Epoch 635/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.2650 - accuracy: 0.6620 - val_loss: 23.5665 - val_accuracy: 0.6000\n",
      "Epoch 636/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 18.4416 - accuracy: 0.6604 - val_loss: 25.3547 - val_accuracy: 0.5875\n",
      "Epoch 637/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 19.1974 - accuracy: 0.6275 - val_loss: 15.5440 - val_accuracy: 0.6125\n",
      "Epoch 638/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.7891 - accuracy: 0.6479 - val_loss: 22.7558 - val_accuracy: 0.5938\n",
      "Epoch 639/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 20.4696 - accuracy: 0.6354 - val_loss: 9.8453 - val_accuracy: 0.6438\n",
      "Epoch 640/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 19.8989 - accuracy: 0.6541 - val_loss: 22.5919 - val_accuracy: 0.5938\n",
      "Epoch 641/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.0019 - accuracy: 0.6651 - val_loss: 29.7875 - val_accuracy: 0.6062\n",
      "Epoch 642/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.7565 - accuracy: 0.6510 - val_loss: 19.4891 - val_accuracy: 0.5938\n",
      "Epoch 643/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 19.7868 - accuracy: 0.6635 - val_loss: 15.3426 - val_accuracy: 0.6500\n",
      "Epoch 644/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 18.6186 - accuracy: 0.6479 - val_loss: 17.8403 - val_accuracy: 0.5938\n",
      "Epoch 645/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.5926 - accuracy: 0.6416 - val_loss: 20.6318 - val_accuracy: 0.5813\n",
      "Epoch 646/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.4066 - accuracy: 0.6792 - val_loss: 22.8548 - val_accuracy: 0.6062\n",
      "Epoch 647/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.0149 - accuracy: 0.6729 - val_loss: 24.2676 - val_accuracy: 0.5813\n",
      "Epoch 648/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.0725 - accuracy: 0.6510 - val_loss: 11.9467 - val_accuracy: 0.5562\n",
      "Epoch 649/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 19.2565 - accuracy: 0.6745 - val_loss: 26.9412 - val_accuracy: 0.5813\n",
      "Epoch 650/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 20.3507 - accuracy: 0.6635 - val_loss: 12.8466 - val_accuracy: 0.6187\n",
      "Epoch 651/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.3690 - accuracy: 0.6385 - val_loss: 27.0974 - val_accuracy: 0.5813\n",
      "Epoch 652/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 20.1062 - accuracy: 0.6479 - val_loss: 26.9404 - val_accuracy: 0.6438\n",
      "Epoch 653/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.9052 - accuracy: 0.6823 - val_loss: 13.6430 - val_accuracy: 0.6562\n",
      "Epoch 654/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 19.7160 - accuracy: 0.6651 - val_loss: 21.2942 - val_accuracy: 0.6375\n",
      "Epoch 655/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 20.3430 - accuracy: 0.6745 - val_loss: 17.7482 - val_accuracy: 0.5938\n",
      "Epoch 656/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 19.9986 - accuracy: 0.6620 - val_loss: 14.0825 - val_accuracy: 0.5625\n",
      "Epoch 657/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 19.6520 - accuracy: 0.6901 - val_loss: 26.7941 - val_accuracy: 0.5562\n",
      "Epoch 658/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.5727 - accuracy: 0.6886 - val_loss: 15.3487 - val_accuracy: 0.5938\n",
      "Epoch 659/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.9408 - accuracy: 0.6854 - val_loss: 12.6987 - val_accuracy: 0.5750\n",
      "Epoch 660/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 19.5028 - accuracy: 0.6651 - val_loss: 24.3840 - val_accuracy: 0.5688\n",
      "Epoch 661/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.2499 - accuracy: 0.6698 - val_loss: 17.2474 - val_accuracy: 0.6750\n",
      "Epoch 662/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 20.2816 - accuracy: 0.6823 - val_loss: 13.1160 - val_accuracy: 0.6438\n",
      "Epoch 663/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 17.7878 - accuracy: 0.6432 - val_loss: 26.6320 - val_accuracy: 0.5813\n",
      "Epoch 664/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 20.0956 - accuracy: 0.6745 - val_loss: 12.0053 - val_accuracy: 0.6250\n",
      "Epoch 665/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.1427 - accuracy: 0.6275 - val_loss: 32.7792 - val_accuracy: 0.6187\n",
      "Epoch 666/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.5167 - accuracy: 0.6964 - val_loss: 20.0104 - val_accuracy: 0.6125\n",
      "Epoch 667/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.5487 - accuracy: 0.6808 - val_loss: 24.9498 - val_accuracy: 0.7000\n",
      "Epoch 668/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.8322 - accuracy: 0.6792 - val_loss: 24.3455 - val_accuracy: 0.6375\n",
      "Epoch 669/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.3175 - accuracy: 0.6729 - val_loss: 30.1927 - val_accuracy: 0.5312\n",
      "Epoch 670/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 19.1666 - accuracy: 0.6620 - val_loss: 27.9418 - val_accuracy: 0.5938\n",
      "Epoch 671/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.1717 - accuracy: 0.6745 - val_loss: 13.7793 - val_accuracy: 0.6250\n",
      "Epoch 672/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.1344 - accuracy: 0.6995 - val_loss: 25.5475 - val_accuracy: 0.6375\n",
      "Epoch 673/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.3013 - accuracy: 0.7042 - val_loss: 29.9684 - val_accuracy: 0.6062\n",
      "Epoch 674/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 19.9128 - accuracy: 0.6573 - val_loss: 18.7426 - val_accuracy: 0.5938\n",
      "Epoch 675/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.6895 - accuracy: 0.6745 - val_loss: 17.3351 - val_accuracy: 0.6375\n",
      "Epoch 676/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 19.1532 - accuracy: 0.7230 - val_loss: 19.2068 - val_accuracy: 0.5750\n",
      "Epoch 677/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 18.9393 - accuracy: 0.6854 - val_loss: 18.3053 - val_accuracy: 0.6250\n",
      "Epoch 678/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.4728 - accuracy: 0.6541 - val_loss: 32.8508 - val_accuracy: 0.6000\n",
      "Epoch 679/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 20.2151 - accuracy: 0.6604 - val_loss: 17.7418 - val_accuracy: 0.6375\n",
      "Epoch 680/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 18.8185 - accuracy: 0.7246 - val_loss: 20.4154 - val_accuracy: 0.6750\n",
      "Epoch 681/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.6010 - accuracy: 0.6745 - val_loss: 15.8368 - val_accuracy: 0.6313\n",
      "Epoch 682/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.9942 - accuracy: 0.6714 - val_loss: 19.4761 - val_accuracy: 0.5813\n",
      "Epoch 683/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.0281 - accuracy: 0.6667 - val_loss: 12.0860 - val_accuracy: 0.6375\n",
      "Epoch 684/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 19.6584 - accuracy: 0.6307 - val_loss: 11.3256 - val_accuracy: 0.5750\n",
      "Epoch 685/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.6851 - accuracy: 0.6761 - val_loss: 16.2998 - val_accuracy: 0.6375\n",
      "Epoch 686/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.5452 - accuracy: 0.6510 - val_loss: 21.7495 - val_accuracy: 0.6250\n",
      "Epoch 687/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 20.2820 - accuracy: 0.625 - 0s 31us/step - loss: 18.6343 - accuracy: 0.6541 - val_loss: 22.4948 - val_accuracy: 0.6125\n",
      "Epoch 688/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.2116 - accuracy: 0.6886 - val_loss: 23.5825 - val_accuracy: 0.5625\n",
      "Epoch 689/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 18.6065 - accuracy: 0.6651 - val_loss: 36.9970 - val_accuracy: 0.6250\n",
      "Epoch 690/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 19.2108 - accuracy: 0.6917 - val_loss: 20.5056 - val_accuracy: 0.6062\n",
      "Epoch 691/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 18.3076 - accuracy: 0.6995 - val_loss: 17.7795 - val_accuracy: 0.5750\n",
      "Epoch 692/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 19.9665 - accuracy: 0.6557 - val_loss: 13.0504 - val_accuracy: 0.7188\n",
      "Epoch 693/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.1358 - accuracy: 0.7152 - val_loss: 18.3477 - val_accuracy: 0.6625\n",
      "Epoch 694/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 19.0313 - accuracy: 0.7027 - val_loss: 12.3821 - val_accuracy: 0.6250\n",
      "Epoch 695/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 18.4959 - accuracy: 0.6463 - val_loss: 18.6111 - val_accuracy: 0.5625\n",
      "Epoch 696/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.2324 - accuracy: 0.6917 - val_loss: 34.7477 - val_accuracy: 0.5813\n",
      "Epoch 697/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.0200 - accuracy: 0.7230 - val_loss: 18.0754 - val_accuracy: 0.5938\n",
      "Epoch 698/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 19.4926 - accuracy: 0.6901 - val_loss: 15.0496 - val_accuracy: 0.6500\n",
      "Epoch 699/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 18.9123 - accuracy: 0.7136 - val_loss: 18.1897 - val_accuracy: 0.6500\n",
      "Epoch 700/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.9489 - accuracy: 0.6573 - val_loss: 10.2468 - val_accuracy: 0.6062\n",
      "Epoch 701/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.5986 - accuracy: 0.6980 - val_loss: 16.0717 - val_accuracy: 0.6875\n",
      "Epoch 702/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.7254 - accuracy: 0.6886 - val_loss: 20.3516 - val_accuracy: 0.6313\n",
      "Epoch 703/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.8248 - accuracy: 0.6479 - val_loss: 12.0495 - val_accuracy: 0.7188\n",
      "Epoch 704/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 18.5308 - accuracy: 0.6745 - val_loss: 18.3440 - val_accuracy: 0.6500\n",
      "Epoch 705/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 18.6365 - accuracy: 0.6745 - val_loss: 15.8909 - val_accuracy: 0.5312\n",
      "Epoch 706/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 18.6010 - accuracy: 0.6729 - val_loss: 25.0651 - val_accuracy: 0.5938\n",
      "Epoch 707/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 19.3781 - accuracy: 0.6354 - val_loss: 16.2828 - val_accuracy: 0.5938\n",
      "Epoch 708/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 18.8172 - accuracy: 0.6886 - val_loss: 11.7412 - val_accuracy: 0.6000\n",
      "Epoch 709/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 19.0633 - accuracy: 0.6416 - val_loss: 17.3067 - val_accuracy: 0.5188\n",
      "Epoch 710/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 16.4186 - accuracy: 0.6698 - val_loss: 9.8534 - val_accuracy: 0.6313\n",
      "Epoch 711/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 17.4821 - accuracy: 0.7089 - val_loss: 14.2476 - val_accuracy: 0.5750\n",
      "Epoch 712/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 18.6608 - accuracy: 0.6870 - val_loss: 25.8301 - val_accuracy: 0.5875\n",
      "Epoch 713/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 18.4881 - accuracy: 0.6995 - val_loss: 12.5050 - val_accuracy: 0.7000\n",
      "Epoch 714/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 17.9374 - accuracy: 0.6479 - val_loss: 11.9242 - val_accuracy: 0.6187\n",
      "Epoch 715/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 19.0204 - accuracy: 0.6620 - val_loss: 14.6480 - val_accuracy: 0.5875\n",
      "Epoch 716/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 17.9571 - accuracy: 0.7183 - val_loss: 18.9126 - val_accuracy: 0.7000\n",
      "Epoch 717/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 18.6363 - accuracy: 0.6995 - val_loss: 19.4663 - val_accuracy: 0.7125\n",
      "Epoch 718/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 18.9572 - accuracy: 0.7136 - val_loss: 17.0382 - val_accuracy: 0.5813\n",
      "Epoch 719/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.8942 - accuracy: 0.6635 - val_loss: 25.4608 - val_accuracy: 0.6875\n",
      "Epoch 720/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 18.4186 - accuracy: 0.6964 - val_loss: 21.4690 - val_accuracy: 0.6375\n",
      "Epoch 721/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 18.5924 - accuracy: 0.6573 - val_loss: 20.9006 - val_accuracy: 0.5375\n",
      "Epoch 722/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 18.7914 - accuracy: 0.6307 - val_loss: 28.2514 - val_accuracy: 0.5813\n",
      "Epoch 723/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 18.2907 - accuracy: 0.6448 - val_loss: 29.5070 - val_accuracy: 0.7375\n",
      "Epoch 724/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 17.4323 - accuracy: 0.6964 - val_loss: 16.0831 - val_accuracy: 0.6125\n",
      "Epoch 725/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 18.3648 - accuracy: 0.6698 - val_loss: 19.9727 - val_accuracy: 0.5938\n",
      "Epoch 726/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 18.5232 - accuracy: 0.6901 - val_loss: 11.9568 - val_accuracy: 0.6562\n",
      "Epoch 727/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 17.4064 - accuracy: 0.6776 - val_loss: 14.6576 - val_accuracy: 0.6687\n",
      "Epoch 728/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 18.1967 - accuracy: 0.6620 - val_loss: 15.3682 - val_accuracy: 0.6812\n",
      "Epoch 729/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 18.3501 - accuracy: 0.7418 - val_loss: 9.8391 - val_accuracy: 0.6875\n",
      "Epoch 730/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 18.3000 - accuracy: 0.6682 - val_loss: 9.4090 - val_accuracy: 0.6938\n",
      "Epoch 731/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 18.1958 - accuracy: 0.7027 - val_loss: 19.4532 - val_accuracy: 0.7063\n",
      "Epoch 732/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 18.2788 - accuracy: 0.6823 - val_loss: 18.4589 - val_accuracy: 0.6562\n",
      "Epoch 733/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.7671 - accuracy: 0.6948 - val_loss: 9.0356 - val_accuracy: 0.6187\n",
      "Epoch 734/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.3723 - accuracy: 0.6917 - val_loss: 13.9856 - val_accuracy: 0.6562\n",
      "Epoch 735/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 18.5998 - accuracy: 0.6416 - val_loss: 21.1093 - val_accuracy: 0.7437\n",
      "Epoch 736/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 18.0019 - accuracy: 0.6823 - val_loss: 13.5125 - val_accuracy: 0.5625\n",
      "Epoch 737/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 17.6336 - accuracy: 0.6761 - val_loss: 18.9584 - val_accuracy: 0.6062\n",
      "Epoch 738/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.6787 - accuracy: 0.6776 - val_loss: 23.9850 - val_accuracy: 0.6250\n",
      "Epoch 739/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.2846 - accuracy: 0.6917 - val_loss: 17.5257 - val_accuracy: 0.6500\n",
      "Epoch 740/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.4048 - accuracy: 0.7136 - val_loss: 24.2522 - val_accuracy: 0.6375\n",
      "Epoch 741/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 18.6209 - accuracy: 0.6620 - val_loss: 21.7221 - val_accuracy: 0.5625\n",
      "Epoch 742/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.9759 - accuracy: 0.6729 - val_loss: 21.5683 - val_accuracy: 0.6375\n",
      "Epoch 743/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.3770 - accuracy: 0.6995 - val_loss: 29.6874 - val_accuracy: 0.5938\n",
      "Epoch 744/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 18.5666 - accuracy: 0.6823 - val_loss: 22.4663 - val_accuracy: 0.6000\n",
      "Epoch 745/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.2804 - accuracy: 0.7293 - val_loss: 10.8731 - val_accuracy: 0.6000\n",
      "Epoch 746/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.3508 - accuracy: 0.6901 - val_loss: 13.2430 - val_accuracy: 0.6438\n",
      "Epoch 747/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.8839 - accuracy: 0.6948 - val_loss: 22.9060 - val_accuracy: 0.5938\n",
      "Epoch 748/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.2112 - accuracy: 0.7152 - val_loss: 18.2181 - val_accuracy: 0.7875\n",
      "Epoch 749/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.7813 - accuracy: 0.6917 - val_loss: 20.2346 - val_accuracy: 0.6812\n",
      "Epoch 750/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.5370 - accuracy: 0.6870 - val_loss: 12.4443 - val_accuracy: 0.6000\n",
      "Epoch 751/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.3770 - accuracy: 0.7324 - val_loss: 16.7390 - val_accuracy: 0.6375\n",
      "Epoch 752/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 18.3139 - accuracy: 0.6698 - val_loss: 16.7480 - val_accuracy: 0.5813\n",
      "Epoch 753/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.0140 - accuracy: 0.6776 - val_loss: 19.2683 - val_accuracy: 0.6438\n",
      "Epoch 754/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.8096 - accuracy: 0.6792 - val_loss: 21.5783 - val_accuracy: 0.6500\n",
      "Epoch 755/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 17.5094 - accuracy: 0.6886 - val_loss: 29.8876 - val_accuracy: 0.6187\n",
      "Epoch 756/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 16.6245 - accuracy: 0.6792 - val_loss: 21.3228 - val_accuracy: 0.5750\n",
      "Epoch 757/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 17.4201 - accuracy: 0.6401 - val_loss: 27.1410 - val_accuracy: 0.6438\n",
      "Epoch 758/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.9663 - accuracy: 0.7199 - val_loss: 23.7504 - val_accuracy: 0.5875\n",
      "Epoch 759/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.4105 - accuracy: 0.6870 - val_loss: 24.3212 - val_accuracy: 0.6250\n",
      "Epoch 760/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.0422 - accuracy: 0.7214 - val_loss: 26.8265 - val_accuracy: 0.6187\n",
      "Epoch 761/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 17.8927 - accuracy: 0.7293 - val_loss: 28.5649 - val_accuracy: 0.6500\n",
      "Epoch 762/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.3509 - accuracy: 0.7308 - val_loss: 8.3975 - val_accuracy: 0.7188\n",
      "Epoch 763/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.7354 - accuracy: 0.7074 - val_loss: 19.9644 - val_accuracy: 0.6500\n",
      "Epoch 764/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 17.9185 - accuracy: 0.7606 - val_loss: 18.4819 - val_accuracy: 0.7812\n",
      "Epoch 765/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.0482 - accuracy: 0.7308 - val_loss: 15.9917 - val_accuracy: 0.5938\n",
      "Epoch 766/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 18.0499 - accuracy: 0.6495 - val_loss: 20.5267 - val_accuracy: 0.5938\n",
      "Epoch 767/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.7548 - accuracy: 0.7027 - val_loss: 27.0347 - val_accuracy: 0.6000\n",
      "Epoch 768/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 17.7123 - accuracy: 0.6933 - val_loss: 22.4855 - val_accuracy: 0.6125\n",
      "Epoch 769/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.4168 - accuracy: 0.7183 - val_loss: 23.9133 - val_accuracy: 0.6375\n",
      "Epoch 770/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 17.5964 - accuracy: 0.6479 - val_loss: 14.1765 - val_accuracy: 0.6313\n",
      "Epoch 771/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 17.9101 - accuracy: 0.7199 - val_loss: 9.2398 - val_accuracy: 0.5875\n",
      "Epoch 772/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.0926 - accuracy: 0.7105 - val_loss: 21.3279 - val_accuracy: 0.6000\n",
      "Epoch 773/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.8722 - accuracy: 0.6933 - val_loss: 11.1007 - val_accuracy: 0.7437\n",
      "Epoch 774/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 16.7330 - accuracy: 0.7371 - val_loss: 14.6185 - val_accuracy: 0.6562\n",
      "Epoch 775/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 17.1779 - accuracy: 0.7183 - val_loss: 14.8785 - val_accuracy: 0.6812\n",
      "Epoch 776/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.8307 - accuracy: 0.7042 - val_loss: 9.0000 - val_accuracy: 0.6750\n",
      "Epoch 777/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.5342 - accuracy: 0.6933 - val_loss: 17.2686 - val_accuracy: 0.6187\n",
      "Epoch 778/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 17.2267 - accuracy: 0.6698 - val_loss: 14.5938 - val_accuracy: 0.6625\n",
      "Epoch 779/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.6324 - accuracy: 0.7277 - val_loss: 8.5999 - val_accuracy: 0.5813\n",
      "Epoch 780/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.0951 - accuracy: 0.6886 - val_loss: 18.0849 - val_accuracy: 0.7312\n",
      "Epoch 781/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 18.0096 - accuracy: 0.7261 - val_loss: 19.5121 - val_accuracy: 0.7312\n",
      "Epoch 782/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 15.6632 - accuracy: 0.7324 - val_loss: 14.5048 - val_accuracy: 0.6438\n",
      "Epoch 783/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.7203 - accuracy: 0.6823 - val_loss: 17.7588 - val_accuracy: 0.6500\n",
      "Epoch 784/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.4631 - accuracy: 0.7167 - val_loss: 26.5573 - val_accuracy: 0.6500\n",
      "Epoch 785/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.7112 - accuracy: 0.7011 - val_loss: 16.6692 - val_accuracy: 0.6500\n",
      "Epoch 786/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.1421 - accuracy: 0.6698 - val_loss: 23.5335 - val_accuracy: 0.7063\n",
      "Epoch 787/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 16.7426 - accuracy: 0.7308 - val_loss: 20.4265 - val_accuracy: 0.6875\n",
      "Epoch 788/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 17.0148 - accuracy: 0.7277 - val_loss: 19.8234 - val_accuracy: 0.7312\n",
      "Epoch 789/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 17.2288 - accuracy: 0.7387 - val_loss: 26.9423 - val_accuracy: 0.6500\n",
      "Epoch 790/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.8392 - accuracy: 0.7074 - val_loss: 22.9476 - val_accuracy: 0.5875\n",
      "Epoch 791/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.5309 - accuracy: 0.6980 - val_loss: 27.8387 - val_accuracy: 0.6750\n",
      "Epoch 792/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.2969 - accuracy: 0.7136 - val_loss: 8.9770 - val_accuracy: 0.5875\n",
      "Epoch 793/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.7537 - accuracy: 0.6948 - val_loss: 13.6556 - val_accuracy: 0.6000\n",
      "Epoch 794/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.0983 - accuracy: 0.6635 - val_loss: 17.2488 - val_accuracy: 0.6438\n",
      "Epoch 795/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.3981 - accuracy: 0.6823 - val_loss: 13.6232 - val_accuracy: 0.5750\n",
      "Epoch 796/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 16.9885 - accuracy: 0.6886 - val_loss: 9.8310 - val_accuracy: 0.6125\n",
      "Epoch 797/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.2748 - accuracy: 0.6839 - val_loss: 25.7602 - val_accuracy: 0.5875\n",
      "Epoch 798/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.2612 - accuracy: 0.7621 - val_loss: 13.2767 - val_accuracy: 0.6812\n",
      "Epoch 799/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 14.9753 - accuracy: 0.7136 - val_loss: 15.7552 - val_accuracy: 0.6313\n",
      "Epoch 800/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.3332 - accuracy: 0.7543 - val_loss: 17.9895 - val_accuracy: 0.6875\n",
      "Epoch 801/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 16.5425 - accuracy: 0.7027 - val_loss: 11.9289 - val_accuracy: 0.5938\n",
      "Epoch 802/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 16.8512 - accuracy: 0.7606 - val_loss: 29.3933 - val_accuracy: 0.6500\n",
      "Epoch 803/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.2035 - accuracy: 0.7277 - val_loss: 23.2776 - val_accuracy: 0.6062\n",
      "Epoch 804/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.7665 - accuracy: 0.7293 - val_loss: 21.0546 - val_accuracy: 0.6000\n",
      "Epoch 805/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 17.0039 - accuracy: 0.7105 - val_loss: 10.5076 - val_accuracy: 0.7125\n",
      "Epoch 806/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 16.4004 - accuracy: 0.7089 - val_loss: 19.9886 - val_accuracy: 0.7125\n",
      "Epoch 807/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.5103 - accuracy: 0.7058 - val_loss: 14.2369 - val_accuracy: 0.6438\n",
      "Epoch 808/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 17.0419 - accuracy: 0.7449 - val_loss: 18.7785 - val_accuracy: 0.7312\n",
      "Epoch 809/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.7716 - accuracy: 0.7027 - val_loss: 24.0330 - val_accuracy: 0.5875\n",
      "Epoch 810/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.4805 - accuracy: 0.6698 - val_loss: 26.1161 - val_accuracy: 0.6062\n",
      "Epoch 811/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.6952 - accuracy: 0.6761 - val_loss: 23.5666 - val_accuracy: 0.6438\n",
      "Epoch 812/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.0435 - accuracy: 0.7199 - val_loss: 21.9735 - val_accuracy: 0.6375\n",
      "Epoch 813/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 16.0261 - accuracy: 0.7058 - val_loss: 17.9960 - val_accuracy: 0.6562\n",
      "Epoch 814/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 17.4088 - accuracy: 0.7121 - val_loss: 13.4011 - val_accuracy: 0.7188\n",
      "Epoch 815/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.0686 - accuracy: 0.7152 - val_loss: 17.8246 - val_accuracy: 0.6375\n",
      "Epoch 816/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.7470 - accuracy: 0.7074 - val_loss: 15.2023 - val_accuracy: 0.6375\n",
      "Epoch 817/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.0688 - accuracy: 0.7152 - val_loss: 14.2516 - val_accuracy: 0.6687\n",
      "Epoch 818/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.6746 - accuracy: 0.7246 - val_loss: 16.8639 - val_accuracy: 0.5938\n",
      "Epoch 819/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.1262 - accuracy: 0.7011 - val_loss: 25.5981 - val_accuracy: 0.6812\n",
      "Epoch 820/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.3122 - accuracy: 0.6886 - val_loss: 24.1778 - val_accuracy: 0.5938\n",
      "Epoch 821/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.4016 - accuracy: 0.7402 - val_loss: 26.4916 - val_accuracy: 0.5875\n",
      "Epoch 822/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.8529 - accuracy: 0.7433 - val_loss: 8.6871 - val_accuracy: 0.6687\n",
      "Epoch 823/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 15.7315 - accuracy: 0.7496 - val_loss: 13.1845 - val_accuracy: 0.6625\n",
      "Epoch 824/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 15.5551 - accuracy: 0.7402 - val_loss: 12.8185 - val_accuracy: 0.6687\n",
      "Epoch 825/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.8098 - accuracy: 0.7793 - val_loss: 11.4914 - val_accuracy: 0.7188\n",
      "Epoch 826/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.1944 - accuracy: 0.7559 - val_loss: 21.9103 - val_accuracy: 0.5750\n",
      "Epoch 827/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.4226 - accuracy: 0.7433 - val_loss: 9.6138 - val_accuracy: 0.6125\n",
      "Epoch 828/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.0408 - accuracy: 0.6886 - val_loss: 19.6670 - val_accuracy: 0.6125\n",
      "Epoch 829/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 17.0365 - accuracy: 0.7778 - val_loss: 15.9513 - val_accuracy: 0.7625\n",
      "Epoch 830/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.2711 - accuracy: 0.7105 - val_loss: 32.1871 - val_accuracy: 0.5813\n",
      "Epoch 831/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 15.2946 - accuracy: 0.6761 - val_loss: 15.8635 - val_accuracy: 0.6750\n",
      "Epoch 832/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 16.2593 - accuracy: 0.7293 - val_loss: 23.6939 - val_accuracy: 0.6562\n",
      "Epoch 833/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.0166 - accuracy: 0.7121 - val_loss: 8.0237 - val_accuracy: 0.5875\n",
      "Epoch 834/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 15.5975 - accuracy: 0.7074 - val_loss: 15.6233 - val_accuracy: 0.6250\n",
      "Epoch 835/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.9713 - accuracy: 0.7418 - val_loss: 26.8574 - val_accuracy: 0.6938\n",
      "Epoch 836/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.3402 - accuracy: 0.7340 - val_loss: 24.0297 - val_accuracy: 0.6250\n",
      "Epoch 837/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 16.4521 - accuracy: 0.7621 - val_loss: 10.5235 - val_accuracy: 0.5938\n",
      "Epoch 838/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 15.1873 - accuracy: 0.7074 - val_loss: 14.9131 - val_accuracy: 0.6062\n",
      "Epoch 839/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 16.5559 - accuracy: 0.7387 - val_loss: 12.9336 - val_accuracy: 0.6125\n",
      "Epoch 840/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 16.2625 - accuracy: 0.6964 - val_loss: 15.0139 - val_accuracy: 0.7250\n",
      "Epoch 841/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 16.2959 - accuracy: 0.7653 - val_loss: 17.3035 - val_accuracy: 0.6375\n",
      "Epoch 842/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 16.6381 - accuracy: 0.7700 - val_loss: 20.0702 - val_accuracy: 0.7063\n",
      "Epoch 843/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 19.7979 - accuracy: 0.718 - 0s 39us/step - loss: 17.4085 - accuracy: 0.7496 - val_loss: 11.3029 - val_accuracy: 0.6812\n",
      "Epoch 844/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 16.4102 - accuracy: 0.7371 - val_loss: 8.4498 - val_accuracy: 0.6687\n",
      "Epoch 845/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.5278 - accuracy: 0.7371 - val_loss: 19.0446 - val_accuracy: 0.6812\n",
      "Epoch 846/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 16.7433 - accuracy: 0.7387 - val_loss: 12.7316 - val_accuracy: 0.6375\n",
      "Epoch 847/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 16.0985 - accuracy: 0.7027 - val_loss: 20.8490 - val_accuracy: 0.7312\n",
      "Epoch 848/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 16.6199 - accuracy: 0.7027 - val_loss: 16.9622 - val_accuracy: 0.6250\n",
      "Epoch 849/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.8210 - accuracy: 0.7543 - val_loss: 10.7832 - val_accuracy: 0.6938\n",
      "Epoch 850/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.7223 - accuracy: 0.7089 - val_loss: 16.1387 - val_accuracy: 0.6562\n",
      "Epoch 851/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.7771 - accuracy: 0.6854 - val_loss: 21.3451 - val_accuracy: 0.6438\n",
      "Epoch 852/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.6953 - accuracy: 0.7340 - val_loss: 11.3623 - val_accuracy: 0.5750\n",
      "Epoch 853/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 16.3670 - accuracy: 0.7340 - val_loss: 17.6122 - val_accuracy: 0.6313\n",
      "Epoch 854/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 16.5992 - accuracy: 0.7089 - val_loss: 22.5891 - val_accuracy: 0.7563\n",
      "Epoch 855/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.9520 - accuracy: 0.7512 - val_loss: 24.2256 - val_accuracy: 0.7063\n",
      "Epoch 856/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 16.6208 - accuracy: 0.7011 - val_loss: 10.8395 - val_accuracy: 0.7312\n",
      "Epoch 857/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.1111 - accuracy: 0.7199 - val_loss: 18.0331 - val_accuracy: 0.6687\n",
      "Epoch 858/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.9913 - accuracy: 0.6635 - val_loss: 20.2879 - val_accuracy: 0.6125\n",
      "Epoch 859/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 16.0120 - accuracy: 0.7214 - val_loss: 21.7733 - val_accuracy: 0.6625\n",
      "Epoch 860/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 16.4132 - accuracy: 0.6995 - val_loss: 22.2362 - val_accuracy: 0.6000\n",
      "Epoch 861/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 14.9452 - accuracy: 0.7074 - val_loss: 19.9250 - val_accuracy: 0.6625\n",
      "Epoch 862/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.1372 - accuracy: 0.6573 - val_loss: 20.4949 - val_accuracy: 0.6812\n",
      "Epoch 863/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.2006 - accuracy: 0.7308 - val_loss: 17.0849 - val_accuracy: 0.6812\n",
      "Epoch 864/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 15.9687 - accuracy: 0.7512 - val_loss: 20.7503 - val_accuracy: 0.7500\n",
      "Epoch 865/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.8494 - accuracy: 0.7058 - val_loss: 14.2374 - val_accuracy: 0.6125\n",
      "Epoch 866/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 16.5104 - accuracy: 0.7183 - val_loss: 13.8957 - val_accuracy: 0.7500\n",
      "Epoch 867/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 15.9963 - accuracy: 0.6901 - val_loss: 20.3368 - val_accuracy: 0.6250\n",
      "Epoch 868/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 16.1563 - accuracy: 0.7512 - val_loss: 22.6587 - val_accuracy: 0.6313\n",
      "Epoch 869/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.6268 - accuracy: 0.6933 - val_loss: 13.3752 - val_accuracy: 0.7375\n",
      "Epoch 870/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 16.0642 - accuracy: 0.7355 - val_loss: 12.4826 - val_accuracy: 0.5938\n",
      "Epoch 871/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 15.6430 - accuracy: 0.7214 - val_loss: 20.7239 - val_accuracy: 0.6562\n",
      "Epoch 872/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.5049 - accuracy: 0.6823 - val_loss: 15.2515 - val_accuracy: 0.7188\n",
      "Epoch 873/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.7616 - accuracy: 0.7746 - val_loss: 24.3282 - val_accuracy: 0.6875\n",
      "Epoch 874/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.0878 - accuracy: 0.7606 - val_loss: 17.4786 - val_accuracy: 0.6875\n",
      "Epoch 875/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.4922 - accuracy: 0.7684 - val_loss: 21.9352 - val_accuracy: 0.7688\n",
      "Epoch 876/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.2924 - accuracy: 0.7261 - val_loss: 12.9366 - val_accuracy: 0.5813\n",
      "Epoch 877/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 15.2826 - accuracy: 0.7293 - val_loss: 15.0225 - val_accuracy: 0.5875\n",
      "Epoch 878/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.3693 - accuracy: 0.6917 - val_loss: 18.6793 - val_accuracy: 0.6375\n",
      "Epoch 879/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.6281 - accuracy: 0.7480 - val_loss: 19.8178 - val_accuracy: 0.6938\n",
      "Epoch 880/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.5602 - accuracy: 0.7371 - val_loss: 18.3852 - val_accuracy: 0.6187\n",
      "Epoch 881/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.7207 - accuracy: 0.7418 - val_loss: 14.2709 - val_accuracy: 0.7063\n",
      "Epoch 882/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 15.2299 - accuracy: 0.7809 - val_loss: 10.2760 - val_accuracy: 0.6250\n",
      "Epoch 883/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.5580 - accuracy: 0.7324 - val_loss: 13.8082 - val_accuracy: 0.6938\n",
      "Epoch 884/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.8224 - accuracy: 0.7355 - val_loss: 22.3848 - val_accuracy: 0.7063\n",
      "Epoch 885/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 14.8616 - accuracy: 0.7324 - val_loss: 18.5596 - val_accuracy: 0.5813\n",
      "Epoch 886/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.0151 - accuracy: 0.6995 - val_loss: 17.2406 - val_accuracy: 0.7000\n",
      "Epoch 887/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.2929 - accuracy: 0.7324 - val_loss: 12.0708 - val_accuracy: 0.7000\n",
      "Epoch 888/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.5362 - accuracy: 0.6870 - val_loss: 17.5752 - val_accuracy: 0.6750\n",
      "Epoch 889/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.6402 - accuracy: 0.7606 - val_loss: 17.4532 - val_accuracy: 0.7188\n",
      "Epoch 890/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 15.9817 - accuracy: 0.6901 - val_loss: 12.5205 - val_accuracy: 0.6313\n",
      "Epoch 891/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.6400 - accuracy: 0.7731 - val_loss: 9.4296 - val_accuracy: 0.6938\n",
      "Epoch 892/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 14.6186 - accuracy: 0.7668 - val_loss: 21.2563 - val_accuracy: 0.7250\n",
      "Epoch 893/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 16.3726 - accuracy: 0.7574 - val_loss: 10.9139 - val_accuracy: 0.6375\n",
      "Epoch 894/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.1877 - accuracy: 0.6964 - val_loss: 12.5316 - val_accuracy: 0.6125\n",
      "Epoch 895/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 16.6118 - accuracy: 0.7715 - val_loss: 16.2906 - val_accuracy: 0.7437\n",
      "Epoch 896/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.4196 - accuracy: 0.7402 - val_loss: 26.2752 - val_accuracy: 0.7188\n",
      "Epoch 897/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 16.0412 - accuracy: 0.7668 - val_loss: 14.4126 - val_accuracy: 0.6625\n",
      "Epoch 898/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 15.9677 - accuracy: 0.7277 - val_loss: 13.4721 - val_accuracy: 0.5813\n",
      "Epoch 899/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 15.8369 - accuracy: 0.6714 - val_loss: 17.2137 - val_accuracy: 0.6000\n",
      "Epoch 900/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.7632 - accuracy: 0.7559 - val_loss: 11.6753 - val_accuracy: 0.6375\n",
      "Epoch 901/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.5745 - accuracy: 0.6839 - val_loss: 18.8422 - val_accuracy: 0.6687\n",
      "Epoch 902/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.9409 - accuracy: 0.7105 - val_loss: 19.8687 - val_accuracy: 0.5938\n",
      "Epoch 903/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.2437 - accuracy: 0.7621 - val_loss: 20.0125 - val_accuracy: 0.7188\n",
      "Epoch 904/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 19.6600 - accuracy: 0.875 - 0s 39us/step - loss: 16.5473 - accuracy: 0.7199 - val_loss: 17.0451 - val_accuracy: 0.7563\n",
      "Epoch 905/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.0169 - accuracy: 0.7825 - val_loss: 24.6633 - val_accuracy: 0.6812\n",
      "Epoch 906/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.8148 - accuracy: 0.7340 - val_loss: 13.9820 - val_accuracy: 0.5938\n",
      "Epoch 907/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 15.6156 - accuracy: 0.6995 - val_loss: 9.1216 - val_accuracy: 0.5813\n",
      "Epoch 908/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 15.2426 - accuracy: 0.7512 - val_loss: 18.7749 - val_accuracy: 0.7063\n",
      "Epoch 909/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.0580 - accuracy: 0.7590 - val_loss: 23.3855 - val_accuracy: 0.5938\n",
      "Epoch 910/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 15.7519 - accuracy: 0.7418 - val_loss: 15.6381 - val_accuracy: 0.6125\n",
      "Epoch 911/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 14.9893 - accuracy: 0.7418 - val_loss: 9.6377 - val_accuracy: 0.6375\n",
      "Epoch 912/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 13.6078 - accuracy: 0.7512 - val_loss: 8.7653 - val_accuracy: 0.6438\n",
      "Epoch 913/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.0191 - accuracy: 0.7856 - val_loss: 20.0096 - val_accuracy: 0.7063\n",
      "Epoch 914/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.7285 - accuracy: 0.7668 - val_loss: 11.3718 - val_accuracy: 0.6500\n",
      "Epoch 915/1000\n",
      "639/639 [==============================] - 0s 45us/step - loss: 16.4568 - accuracy: 0.7387 - val_loss: 14.1564 - val_accuracy: 0.6938\n",
      "Epoch 916/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.0284 - accuracy: 0.7465 - val_loss: 18.2320 - val_accuracy: 0.7125\n",
      "Epoch 917/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 15.2533 - accuracy: 0.7637 - val_loss: 20.3258 - val_accuracy: 0.7250\n",
      "Epoch 918/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 16.1455 - accuracy: 0.7762 - val_loss: 9.0476 - val_accuracy: 0.7437\n",
      "Epoch 919/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 14.9941 - accuracy: 0.7590 - val_loss: 19.6426 - val_accuracy: 0.6625\n",
      "Epoch 920/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.0488 - accuracy: 0.7371 - val_loss: 15.2144 - val_accuracy: 0.6438\n",
      "Epoch 921/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 16.0283 - accuracy: 0.6901 - val_loss: 11.4771 - val_accuracy: 0.6812\n",
      "Epoch 922/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 16.0953 - accuracy: 0.7355 - val_loss: 17.9664 - val_accuracy: 0.6625\n",
      "Epoch 923/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.2027 - accuracy: 0.7214 - val_loss: 23.1971 - val_accuracy: 0.7875\n",
      "Epoch 924/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 15.3870 - accuracy: 0.7371 - val_loss: 19.3455 - val_accuracy: 0.6375\n",
      "Epoch 925/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 15.6856 - accuracy: 0.7684 - val_loss: 9.1023 - val_accuracy: 0.6687\n",
      "Epoch 926/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.3733 - accuracy: 0.7637 - val_loss: 8.1018 - val_accuracy: 0.7312\n",
      "Epoch 927/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.1359 - accuracy: 0.7559 - val_loss: 19.6583 - val_accuracy: 0.6500\n",
      "Epoch 928/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 17.9328 - accuracy: 0.593 - 0s 34us/step - loss: 16.0658 - accuracy: 0.7559 - val_loss: 10.0440 - val_accuracy: 0.7063\n",
      "Epoch 929/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.2581 - accuracy: 0.7574 - val_loss: 12.8551 - val_accuracy: 0.7375\n",
      "Epoch 930/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.5460 - accuracy: 0.7121 - val_loss: 7.8029 - val_accuracy: 0.7750\n",
      "Epoch 931/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 14.8077 - accuracy: 0.7512 - val_loss: 8.0199 - val_accuracy: 0.7375\n",
      "Epoch 932/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 13.7672 - accuracy: 0.7340 - val_loss: 23.5781 - val_accuracy: 0.6625\n",
      "Epoch 933/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.2287 - accuracy: 0.7512 - val_loss: 15.0605 - val_accuracy: 0.7125\n",
      "Epoch 934/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 15.3263 - accuracy: 0.7418 - val_loss: 13.2520 - val_accuracy: 0.7688\n",
      "Epoch 935/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 15.1579 - accuracy: 0.7465 - val_loss: 17.5346 - val_accuracy: 0.7250\n",
      "Epoch 936/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 14.2256 - accuracy: 0.7418 - val_loss: 15.0517 - val_accuracy: 0.6938\n",
      "Epoch 937/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 14.8851 - accuracy: 0.7684 - val_loss: 15.7844 - val_accuracy: 0.7125\n",
      "Epoch 938/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 14.4050 - accuracy: 0.7449 - val_loss: 16.1615 - val_accuracy: 0.7812\n",
      "Epoch 939/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 15.4977 - accuracy: 0.7293 - val_loss: 22.5829 - val_accuracy: 0.7375\n",
      "Epoch 940/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.9891 - accuracy: 0.6917 - val_loss: 15.4434 - val_accuracy: 0.7437\n",
      "Epoch 941/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 15.5635 - accuracy: 0.7496 - val_loss: 17.8834 - val_accuracy: 0.6562\n",
      "Epoch 942/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 15.2795 - accuracy: 0.7559 - val_loss: 16.1764 - val_accuracy: 0.5875\n",
      "Epoch 943/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.0055 - accuracy: 0.7809 - val_loss: 20.7870 - val_accuracy: 0.6750\n",
      "Epoch 944/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 14.8151 - accuracy: 0.7606 - val_loss: 20.8477 - val_accuracy: 0.7312\n",
      "Epoch 945/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 15.4007 - accuracy: 0.7433 - val_loss: 17.6101 - val_accuracy: 0.6375\n",
      "Epoch 946/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 16.0187 - accuracy: 0.7574 - val_loss: 9.6246 - val_accuracy: 0.6375\n",
      "Epoch 947/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 14.4490 - accuracy: 0.7621 - val_loss: 17.1199 - val_accuracy: 0.7063\n",
      "Epoch 948/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 14.9307 - accuracy: 0.7856 - val_loss: 14.4575 - val_accuracy: 0.7000\n",
      "Epoch 949/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 13.5771 - accuracy: 0.7840 - val_loss: 13.2652 - val_accuracy: 0.7063\n",
      "Epoch 950/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.5911 - accuracy: 0.7121 - val_loss: 17.4517 - val_accuracy: 0.7250\n",
      "Epoch 951/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.4526 - accuracy: 0.7778 - val_loss: 10.5908 - val_accuracy: 0.7125\n",
      "Epoch 952/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.0122 - accuracy: 0.7668 - val_loss: 20.6762 - val_accuracy: 0.7000\n",
      "Epoch 953/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.6014 - accuracy: 0.7778 - val_loss: 15.6099 - val_accuracy: 0.6000\n",
      "Epoch 954/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 15.7257 - accuracy: 0.687 - 0s 41us/step - loss: 15.4453 - accuracy: 0.7715 - val_loss: 17.6502 - val_accuracy: 0.7500\n",
      "Epoch 955/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 14.6678 - accuracy: 0.7042 - val_loss: 17.3036 - val_accuracy: 0.6562\n",
      "Epoch 956/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.3492 - accuracy: 0.7809 - val_loss: 10.0379 - val_accuracy: 0.7875\n",
      "Epoch 957/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 14.4604 - accuracy: 0.7512 - val_loss: 18.6157 - val_accuracy: 0.6438\n",
      "Epoch 958/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.0337 - accuracy: 0.7355 - val_loss: 20.5204 - val_accuracy: 0.6687\n",
      "Epoch 959/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.3455 - accuracy: 0.7621 - val_loss: 15.2480 - val_accuracy: 0.7375\n",
      "Epoch 960/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 13.9911 - accuracy: 0.7606 - val_loss: 25.7743 - val_accuracy: 0.7188\n",
      "Epoch 961/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 14.4209 - accuracy: 0.7762 - val_loss: 13.9225 - val_accuracy: 0.7312\n",
      "Epoch 962/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 14.7981 - accuracy: 0.7887 - val_loss: 15.6075 - val_accuracy: 0.6250\n",
      "Epoch 963/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 15.3959 - accuracy: 0.7011 - val_loss: 10.7775 - val_accuracy: 0.7312\n",
      "Epoch 964/1000\n",
      "639/639 [==============================] - 0s 31us/step - loss: 15.2560 - accuracy: 0.7308 - val_loss: 16.7581 - val_accuracy: 0.7875\n",
      "Epoch 965/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 15.4124 - accuracy: 0.7668 - val_loss: 11.4888 - val_accuracy: 0.7188\n",
      "Epoch 966/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.3518 - accuracy: 0.7355 - val_loss: 15.6167 - val_accuracy: 0.7625\n",
      "Epoch 967/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 14.7435 - accuracy: 0.7919 - val_loss: 20.5260 - val_accuracy: 0.7812\n",
      "Epoch 968/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 14.8971 - accuracy: 0.7512 - val_loss: 8.0613 - val_accuracy: 0.6562\n",
      "Epoch 969/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 14.3508 - accuracy: 0.7512 - val_loss: 8.8968 - val_accuracy: 0.7375\n",
      "Epoch 970/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 14.6562 - accuracy: 0.7324 - val_loss: 18.9352 - val_accuracy: 0.7500\n",
      "Epoch 971/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.4255 - accuracy: 0.8028 - val_loss: 16.6743 - val_accuracy: 0.7563\n",
      "Epoch 972/1000\n",
      "639/639 [==============================] - 0s 38us/step - loss: 15.3996 - accuracy: 0.7934 - val_loss: 17.7982 - val_accuracy: 0.7875\n",
      "Epoch 973/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.3826 - accuracy: 0.7903 - val_loss: 9.5216 - val_accuracy: 0.6500\n",
      "Epoch 974/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 14.5768 - accuracy: 0.7825 - val_loss: 15.4712 - val_accuracy: 0.7625\n",
      "Epoch 975/1000\n",
      "639/639 [==============================] - 0s 33us/step - loss: 15.2926 - accuracy: 0.7402 - val_loss: 8.5067 - val_accuracy: 0.7500\n",
      "Epoch 976/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 14.7447 - accuracy: 0.7606 - val_loss: 18.0509 - val_accuracy: 0.6687\n",
      "Epoch 977/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.0628 - accuracy: 0.7637 - val_loss: 22.8278 - val_accuracy: 0.7812\n",
      "Epoch 978/1000\n",
      "639/639 [==============================] - 0s 36us/step - loss: 15.1971 - accuracy: 0.7559 - val_loss: 21.8300 - val_accuracy: 0.6687\n",
      "Epoch 979/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 13.9125 - accuracy: 0.7261 - val_loss: 10.5581 - val_accuracy: 0.6938\n",
      "Epoch 980/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 14.6793 - accuracy: 0.7856 - val_loss: 17.4921 - val_accuracy: 0.6375\n",
      "Epoch 981/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 15.5499 - accuracy: 0.6839 - val_loss: 11.5222 - val_accuracy: 0.6125\n",
      "Epoch 982/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 14.7459 - accuracy: 0.7340 - val_loss: 12.9470 - val_accuracy: 0.7125\n",
      "Epoch 983/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 13.0474 - accuracy: 0.7387 - val_loss: 10.3140 - val_accuracy: 0.7063\n",
      "Epoch 984/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 14.5082 - accuracy: 0.7762 - val_loss: 14.4722 - val_accuracy: 0.7437\n",
      "Epoch 985/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.4692 - accuracy: 0.7293 - val_loss: 11.8729 - val_accuracy: 0.7125\n",
      "Epoch 986/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.4682 - accuracy: 0.7903 - val_loss: 15.8768 - val_accuracy: 0.7688\n",
      "Epoch 987/1000\n",
      "639/639 [==============================] - 0s 44us/step - loss: 14.6845 - accuracy: 0.8013 - val_loss: 24.2849 - val_accuracy: 0.7063\n",
      "Epoch 988/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 14.7838 - accuracy: 0.7574 - val_loss: 14.3872 - val_accuracy: 0.7312\n",
      "Epoch 989/1000\n",
      "639/639 [==============================] - ETA: 0s - loss: 15.1395 - accuracy: 0.750 - 0s 42us/step - loss: 14.5804 - accuracy: 0.7559 - val_loss: 9.1006 - val_accuracy: 0.5750\n",
      "Epoch 990/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 12.6888 - accuracy: 0.7715 - val_loss: 14.3630 - val_accuracy: 0.7312\n",
      "Epoch 991/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 15.3590 - accuracy: 0.7840 - val_loss: 8.5150 - val_accuracy: 0.7688\n",
      "Epoch 992/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 14.2871 - accuracy: 0.7543 - val_loss: 10.2820 - val_accuracy: 0.6313\n",
      "Epoch 993/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 14.9939 - accuracy: 0.7183 - val_loss: 16.5763 - val_accuracy: 0.7688\n",
      "Epoch 994/1000\n",
      "639/639 [==============================] - 0s 37us/step - loss: 15.4936 - accuracy: 0.7684 - val_loss: 14.9577 - val_accuracy: 0.7563\n",
      "Epoch 995/1000\n",
      "639/639 [==============================] - 0s 41us/step - loss: 14.8909 - accuracy: 0.7668 - val_loss: 15.5910 - val_accuracy: 0.7063\n",
      "Epoch 996/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 14.4211 - accuracy: 0.7480 - val_loss: 15.5203 - val_accuracy: 0.6438\n",
      "Epoch 997/1000\n",
      "639/639 [==============================] - 0s 34us/step - loss: 13.7713 - accuracy: 0.7136 - val_loss: 19.0626 - val_accuracy: 0.7688\n",
      "Epoch 998/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 15.4929 - accuracy: 0.7105 - val_loss: 12.9421 - val_accuracy: 0.6313\n",
      "Epoch 999/1000\n",
      "639/639 [==============================] - 0s 42us/step - loss: 14.8363 - accuracy: 0.6933 - val_loss: 10.8028 - val_accuracy: 0.6250\n",
      "Epoch 1000/1000\n",
      "639/639 [==============================] - 0s 39us/step - loss: 14.7096 - accuracy: 0.7512 - val_loss: 18.4246 - val_accuracy: 0.7625\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_test,y_test), epochs=training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3959.79927654   88.38690687 3833.36343175   73.86084999 3720.26202803\n",
      "   80.89942707 3875.73527218   89.76332243 3984.65855778   61.87102125\n",
      " 3719.09628146  182.6610984  3820.20357804  120.41394201 3784.76640989\n",
      "  129.26342628 4008.75510365   97.71687554 3794.63708076  176.50269565\n",
      " 4028.530833     41.54125543 3908.45807054   92.69808456 3837.47288001\n",
      "  158.77981372 3639.46159761  148.1763349  3754.47286213  171.42434555]\n",
      "[3930.4724     94.14286  3813.228      71.28311  3704.6692     81.81841\n",
      " 3854.9978     97.633316 3959.8105     52.63658  3693.191     177.30429\n",
      " 3792.008     127.32965  3760.1562    139.93283  3995.86       85.98746\n",
      " 3771.652     176.88643  4002.2498     49.231865 3880.1807     88.90663\n",
      " 3808.5774    151.40303  3619.0918    162.06358  3713.0576    170.9978  ]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "print(y_test[0])\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict of test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accurcy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two imges input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples are ((frame, frame), next_frame)\n",
    "\n",
    "X = None\n",
    "\n",
    "for i in range(len(frames) - 2):\n",
    "    X = np.append(X, np.append(frames[i], frames[i+1])) if X is not None else np.append(frames[i], frames[i+1])\n",
    "X = X.reshape(-1, 4 * no_skyrmions)\n",
    "y = frames[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (638, 60)\n",
      "y_train shape: (638, 30)\n",
      "X_test shape: (160, 60)\n",
      "y_test shape: (160, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"y_train shape: \" + str(y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "\n",
    "optimizer = 'NAdam'\n",
    "loss = 'mae'\n",
    "metrics = ['accuracy']\n",
    "training_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "n_input = 4 * no_skyrmions\n",
    "n_hidden_1 = 64\n",
    "n_hidden_2 = 64\n",
    "n_hidden_3 = 64\n",
    "n_output = 2 * no_skyrmions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "\n",
    "# Neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_1, input_dim=n_input, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_hidden_2, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_hidden_3, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 638 samples, validate on 160 samples\n",
      "Epoch 1/1000\n",
      "638/638 [==============================] - 0s 278us/step - loss: 1482.4425 - accuracy: 0.0737 - val_loss: 584.5781 - val_accuracy: 0.3812\n",
      "Epoch 2/1000\n",
      "638/638 [==============================] - 0s 53us/step - loss: 387.8674 - accuracy: 0.1536 - val_loss: 189.8469 - val_accuracy: 0.2438\n",
      "Epoch 3/1000\n",
      "638/638 [==============================] - 0s 48us/step - loss: 201.8183 - accuracy: 0.2476 - val_loss: 204.3425 - val_accuracy: 0.2625\n",
      "Epoch 4/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 174.9707 - accuracy: 0.1677 - val_loss: 132.2345 - val_accuracy: 0.0312\n",
      "Epoch 5/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 144.2128 - accuracy: 0.1865 - val_loss: 147.6192 - val_accuracy: 0.2625\n",
      "Epoch 6/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 165.0457 - accuracy: 0.1881 - val_loss: 178.0202 - val_accuracy: 0.0812\n",
      "Epoch 7/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 157.0532 - accuracy: 0.2398 - val_loss: 221.7124 - val_accuracy: 0.3688\n",
      "Epoch 8/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 153.4230 - accuracy: 0.3166 - val_loss: 174.6239 - val_accuracy: 0.2688\n",
      "Epoch 9/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 142.2388 - accuracy: 0.3433 - val_loss: 55.3747 - val_accuracy: 0.5688\n",
      "Epoch 10/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 134.1745 - accuracy: 0.4060 - val_loss: 202.6259 - val_accuracy: 0.5188\n",
      "Epoch 11/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 123.5636 - accuracy: 0.3777 - val_loss: 240.1485 - val_accuracy: 0.3688\n",
      "Epoch 12/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 130.4309 - accuracy: 0.3009 - val_loss: 61.8167 - val_accuracy: 0.3875\n",
      "Epoch 13/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 128.9683 - accuracy: 0.4169 - val_loss: 134.9255 - val_accuracy: 0.5375\n",
      "Epoch 14/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 126.7847 - accuracy: 0.4451 - val_loss: 171.5871 - val_accuracy: 0.2625\n",
      "Epoch 15/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 130.0838 - accuracy: 0.3621 - val_loss: 111.2058 - val_accuracy: 0.2250\n",
      "Epoch 16/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 130.7157 - accuracy: 0.4875 - val_loss: 105.9134 - val_accuracy: 0.5625\n",
      "Epoch 17/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 122.6398 - accuracy: 0.5172 - val_loss: 101.6618 - val_accuracy: 0.3938\n",
      "Epoch 18/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 119.7994 - accuracy: 0.3840 - val_loss: 192.8339 - val_accuracy: 0.2937\n",
      "Epoch 19/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 120.6146 - accuracy: 0.3777 - val_loss: 94.5136 - val_accuracy: 0.3063\n",
      "Epoch 20/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 117.7186 - accuracy: 0.4091 - val_loss: 107.1713 - val_accuracy: 0.5437\n",
      "Epoch 21/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 119.5358 - accuracy: 0.4953 - val_loss: 161.5768 - val_accuracy: 0.5250\n",
      "Epoch 22/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 105.3214 - accuracy: 0.4624 - val_loss: 146.7382 - val_accuracy: 0.0250\n",
      "Epoch 23/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 113.0763 - accuracy: 0.5047 - val_loss: 87.1603 - val_accuracy: 0.0063\n",
      "Epoch 24/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 118.8613 - accuracy: 0.4467 - val_loss: 178.0242 - val_accuracy: 0.2937\n",
      "Epoch 25/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 115.4388 - accuracy: 0.4843 - val_loss: 117.2199 - val_accuracy: 0.3500\n",
      "Epoch 26/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 114.7439 - accuracy: 0.5564 - val_loss: 91.2568 - val_accuracy: 0.5688\n",
      "Epoch 27/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 113.7116 - accuracy: 0.4357 - val_loss: 122.3488 - val_accuracy: 0.0812\n",
      "Epoch 28/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 102.7510 - accuracy: 0.4154 - val_loss: 114.7263 - val_accuracy: 0.5813\n",
      "Epoch 29/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 111.3634 - accuracy: 0.5157 - val_loss: 147.5099 - val_accuracy: 0.5375\n",
      "Epoch 30/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 101.5167 - accuracy: 0.4357 - val_loss: 106.1212 - val_accuracy: 0.2625\n",
      "Epoch 31/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 116.4664 - accuracy: 0.4592 - val_loss: 142.1348 - val_accuracy: 0.5500\n",
      "Epoch 32/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 107.4310 - accuracy: 0.4765 - val_loss: 84.8067 - val_accuracy: 0.5375\n",
      "Epoch 33/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 103.2347 - accuracy: 0.4890 - val_loss: 119.4948 - val_accuracy: 0.1688\n",
      "Epoch 34/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 105.4186 - accuracy: 0.3981 - val_loss: 72.5117 - val_accuracy: 0.4062\n",
      "Epoch 35/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 102.1885 - accuracy: 0.4765 - val_loss: 170.3392 - val_accuracy: 0.2562\n",
      "Epoch 36/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 97.8721 - accuracy: 0.3777 - val_loss: 144.0021 - val_accuracy: 0.5000\n",
      "Epoch 37/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 107.9667 - accuracy: 0.5078 - val_loss: 69.3669 - val_accuracy: 0.5562\n",
      "Epoch 38/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 102.7450 - accuracy: 0.4436 - val_loss: 136.7415 - val_accuracy: 0.4500\n",
      "Epoch 39/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 105.0561 - accuracy: 0.5110 - val_loss: 119.1987 - val_accuracy: 0.5125\n",
      "Epoch 40/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 77.3577 - accuracy: 0.687 - 0s 34us/step - loss: 95.2336 - accuracy: 0.4561 - val_loss: 150.6517 - val_accuracy: 0.5063\n",
      "Epoch 41/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 102.6902 - accuracy: 0.3903 - val_loss: 147.4565 - val_accuracy: 0.5125\n",
      "Epoch 42/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 105.5861 - accuracy: 0.5361 - val_loss: 102.6399 - val_accuracy: 0.5500\n",
      "Epoch 43/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 91.7792 - accuracy: 0.3809 - val_loss: 200.2340 - val_accuracy: 0.2625\n",
      "Epoch 44/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 99.1412 - accuracy: 0.4483 - val_loss: 110.0712 - val_accuracy: 0.3875\n",
      "Epoch 45/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 102.8181 - accuracy: 0.5188 - val_loss: 153.7247 - val_accuracy: 0.5312\n",
      "Epoch 46/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 101.7036 - accuracy: 0.5016 - val_loss: 140.1522 - val_accuracy: 0.3625\n",
      "Epoch 47/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 96.9409 - accuracy: 0.5313 - val_loss: 106.3694 - val_accuracy: 0.3313\n",
      "Epoch 48/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 100.3843 - accuracy: 0.4734 - val_loss: 109.8884 - val_accuracy: 0.5562\n",
      "Epoch 49/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 100.1922 - accuracy: 0.5611 - val_loss: 102.7614 - val_accuracy: 0.5437\n",
      "Epoch 50/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 95.6830 - accuracy: 0.5580 - val_loss: 72.0295 - val_accuracy: 0.5562\n",
      "Epoch 51/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 92.8130 - accuracy: 0.5846 - val_loss: 150.5358 - val_accuracy: 0.3562\n",
      "Epoch 52/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 96.4867 - accuracy: 0.5110 - val_loss: 53.4005 - val_accuracy: 0.5125\n",
      "Epoch 53/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 89.8097 - accuracy: 0.5266 - val_loss: 127.3964 - val_accuracy: 0.1937\n",
      "Epoch 54/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 83.1424 - accuracy: 0.3793 - val_loss: 73.3392 - val_accuracy: 0.2688\n",
      "Epoch 55/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 94.8106 - accuracy: 0.5063 - val_loss: 115.3531 - val_accuracy: 0.5375\n",
      "Epoch 56/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 92.9944 - accuracy: 0.5737 - val_loss: 107.9523 - val_accuracy: 0.5437\n",
      "Epoch 57/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 83.4394 - accuracy: 0.5533 - val_loss: 83.4583 - val_accuracy: 0.5437\n",
      "Epoch 58/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 83.0546 - accuracy: 0.5455 - val_loss: 105.5072 - val_accuracy: 0.5500\n",
      "Epoch 59/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 95.0322 - accuracy: 0.6097 - val_loss: 124.2385 - val_accuracy: 0.5375\n",
      "Epoch 60/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 94.8376 - accuracy: 0.6003 - val_loss: 91.1681 - val_accuracy: 0.5562\n",
      "Epoch 61/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 87.2250 - accuracy: 0.5596 - val_loss: 143.8133 - val_accuracy: 0.5125\n",
      "Epoch 62/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 94.4590 - accuracy: 0.5533 - val_loss: 91.8456 - val_accuracy: 0.5500\n",
      "Epoch 63/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 97.5045 - accuracy: 0.6019 - val_loss: 104.0263 - val_accuracy: 0.5500\n",
      "Epoch 64/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 90.7395 - accuracy: 0.4122 - val_loss: 143.2385 - val_accuracy: 0.2625\n",
      "Epoch 65/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 80.2698 - accuracy: 0.5047 - val_loss: 165.1457 - val_accuracy: 0.5625\n",
      "Epoch 66/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 82.5837 - accuracy: 0.5564 - val_loss: 81.2647 - val_accuracy: 0.4500\n",
      "Epoch 67/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 81.7137 - accuracy: 0.5486 - val_loss: 98.6202 - val_accuracy: 0.5188\n",
      "Epoch 68/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 86.4070 - accuracy: 0.6034 - val_loss: 94.4638 - val_accuracy: 0.5125\n",
      "Epoch 69/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 86.8143 - accuracy: 0.5517 - val_loss: 30.9556 - val_accuracy: 0.4875\n",
      "Epoch 70/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 80.5470 - accuracy: 0.4514 - val_loss: 104.6779 - val_accuracy: 0.4812\n",
      "Epoch 71/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 83.9591 - accuracy: 0.5000 - val_loss: 123.9177 - val_accuracy: 0.5125\n",
      "Epoch 72/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 84.8212 - accuracy: 0.6113 - val_loss: 64.7662 - val_accuracy: 0.3313\n",
      "Epoch 73/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 86.1570 - accuracy: 0.5423 - val_loss: 131.6665 - val_accuracy: 0.5625\n",
      "Epoch 74/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 84.8946 - accuracy: 0.5235 - val_loss: 138.3162 - val_accuracy: 0.4938\n",
      "Epoch 75/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 85.9571 - accuracy: 0.5564 - val_loss: 55.3545 - val_accuracy: 0.5625\n",
      "Epoch 76/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 84.4659 - accuracy: 0.6254 - val_loss: 119.5043 - val_accuracy: 0.5562\n",
      "Epoch 77/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 79.2935 - accuracy: 0.5235 - val_loss: 87.6262 - val_accuracy: 0.5875\n",
      "Epoch 78/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 84.6345 - accuracy: 0.6034 - val_loss: 94.5211 - val_accuracy: 0.5562\n",
      "Epoch 79/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 82.3518 - accuracy: 0.6348 - val_loss: 132.4390 - val_accuracy: 0.5813\n",
      "Epoch 80/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 84.2394 - accuracy: 0.5972 - val_loss: 78.0862 - val_accuracy: 0.5500\n",
      "Epoch 81/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 79.9427 - accuracy: 0.6082 - val_loss: 31.4222 - val_accuracy: 0.2688\n",
      "Epoch 82/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 78.9507 - accuracy: 0.5251 - val_loss: 51.0542 - val_accuracy: 0.5500\n",
      "Epoch 83/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 78.8743 - accuracy: 0.5752 - val_loss: 35.7061 - val_accuracy: 0.5562\n",
      "Epoch 84/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 75.6156 - accuracy: 0.4075 - val_loss: 71.0665 - val_accuracy: 0.5125\n",
      "Epoch 85/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 82.1086 - accuracy: 0.5423 - val_loss: 123.1466 - val_accuracy: 0.5875\n",
      "Epoch 86/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 79.2076 - accuracy: 0.6034 - val_loss: 80.1922 - val_accuracy: 0.6562\n",
      "Epoch 87/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 82.5237 - accuracy: 0.6285 - val_loss: 72.0120 - val_accuracy: 0.5688\n",
      "Epoch 88/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 79.7584 - accuracy: 0.6003 - val_loss: 130.0419 - val_accuracy: 0.6000\n",
      "Epoch 89/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 76.0939 - accuracy: 0.5502 - val_loss: 73.5626 - val_accuracy: 0.5625\n",
      "Epoch 90/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 80.1157 - accuracy: 0.5690 - val_loss: 108.0279 - val_accuracy: 0.5562\n",
      "Epoch 91/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 75.5587 - accuracy: 0.6144 - val_loss: 57.4309 - val_accuracy: 0.5063\n",
      "Epoch 92/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 75.0451 - accuracy: 0.5172 - val_loss: 116.5133 - val_accuracy: 0.6062\n",
      "Epoch 93/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 75.6685 - accuracy: 0.5940 - val_loss: 40.2495 - val_accuracy: 0.6125\n",
      "Epoch 94/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 73.4302 - accuracy: 0.5690 - val_loss: 64.8616 - val_accuracy: 0.5625\n",
      "Epoch 95/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 54.9518 - accuracy: 0.687 - 0s 42us/step - loss: 76.6279 - accuracy: 0.5893 - val_loss: 29.7836 - val_accuracy: 0.5375\n",
      "Epoch 96/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 72.5915 - accuracy: 0.5878 - val_loss: 99.7563 - val_accuracy: 0.5500\n",
      "Epoch 97/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 71.5937 - accuracy: 0.5533 - val_loss: 112.9593 - val_accuracy: 0.2625\n",
      "Epoch 98/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 77.4414 - accuracy: 0.5987 - val_loss: 114.2719 - val_accuracy: 0.6187\n",
      "Epoch 99/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 78.5511 - accuracy: 0.6019 - val_loss: 66.3311 - val_accuracy: 0.3938\n",
      "Epoch 100/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 76.5432 - accuracy: 0.5878 - val_loss: 43.0715 - val_accuracy: 0.5000\n",
      "Epoch 101/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 57.3270 - accuracy: 0.5000 - val_loss: 61.3652 - val_accuracy: 0.2688\n",
      "Epoch 102/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 76.5883 - accuracy: 0.5627 - val_loss: 131.3058 - val_accuracy: 0.6062\n",
      "Epoch 103/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 75.3598 - accuracy: 0.5925 - val_loss: 76.5689 - val_accuracy: 0.5500\n",
      "Epoch 104/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 69.6151 - accuracy: 0.5799 - val_loss: 56.5203 - val_accuracy: 0.3438\n",
      "Epoch 105/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 71.4788 - accuracy: 0.5188 - val_loss: 25.2136 - val_accuracy: 0.5125\n",
      "Epoch 106/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 67.5971 - accuracy: 0.5925 - val_loss: 117.0052 - val_accuracy: 0.5437\n",
      "Epoch 107/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 75.7124 - accuracy: 0.6254 - val_loss: 93.3878 - val_accuracy: 0.5250\n",
      "Epoch 108/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 69.7825 - accuracy: 0.5925 - val_loss: 131.2955 - val_accuracy: 0.6750\n",
      "Epoch 109/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 73.1904 - accuracy: 0.6019 - val_loss: 79.4589 - val_accuracy: 0.5750\n",
      "Epoch 110/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 74.2655 - accuracy: 0.5502 - val_loss: 42.7687 - val_accuracy: 0.4750\n",
      "Epoch 111/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 73.1296 - accuracy: 0.5125 - val_loss: 51.4838 - val_accuracy: 0.6000\n",
      "Epoch 112/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 69.9631 - accuracy: 0.4984 - val_loss: 77.2702 - val_accuracy: 0.6062\n",
      "Epoch 113/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 75.0053 - accuracy: 0.5517 - val_loss: 49.6740 - val_accuracy: 0.4250\n",
      "Epoch 114/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 71.2443 - accuracy: 0.5690 - val_loss: 71.3774 - val_accuracy: 0.5437\n",
      "Epoch 115/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 71.3615 - accuracy: 0.6238 - val_loss: 26.4840 - val_accuracy: 0.5813\n",
      "Epoch 116/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 67.6821 - accuracy: 0.6160 - val_loss: 19.4649 - val_accuracy: 0.6375\n",
      "Epoch 117/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 62.8119 - accuracy: 0.6129 - val_loss: 48.2315 - val_accuracy: 0.5813\n",
      "Epoch 118/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 66.3587 - accuracy: 0.6270 - val_loss: 102.0175 - val_accuracy: 0.5437\n",
      "Epoch 119/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 73.5942 - accuracy: 0.5784 - val_loss: 77.4839 - val_accuracy: 0.3500\n",
      "Epoch 120/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 72.6394 - accuracy: 0.5658 - val_loss: 34.4972 - val_accuracy: 0.5875\n",
      "Epoch 121/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 71.0134 - accuracy: 0.6176 - val_loss: 95.3213 - val_accuracy: 0.6062\n",
      "Epoch 122/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 67.7919 - accuracy: 0.6050 - val_loss: 69.8562 - val_accuracy: 0.5000\n",
      "Epoch 123/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 70.0695 - accuracy: 0.6238 - val_loss: 47.8055 - val_accuracy: 0.5688\n",
      "Epoch 124/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 68.6528 - accuracy: 0.5737 - val_loss: 56.4407 - val_accuracy: 0.5938\n",
      "Epoch 125/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 60.6630 - accuracy: 0.5815 - val_loss: 42.4063 - val_accuracy: 0.6938\n",
      "Epoch 126/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 68.0103 - accuracy: 0.6160 - val_loss: 90.5350 - val_accuracy: 0.6125\n",
      "Epoch 127/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 68.7835 - accuracy: 0.6223 - val_loss: 69.7696 - val_accuracy: 0.5500\n",
      "Epoch 128/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 61.6277 - accuracy: 0.5831 - val_loss: 88.4913 - val_accuracy: 0.5312\n",
      "Epoch 129/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 67.0706 - accuracy: 0.4937 - val_loss: 27.0647 - val_accuracy: 0.2812\n",
      "Epoch 130/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 64.5537 - accuracy: 0.5862 - val_loss: 63.2713 - val_accuracy: 0.5125\n",
      "Epoch 131/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 64.4881 - accuracy: 0.6176 - val_loss: 54.3952 - val_accuracy: 0.5813\n",
      "Epoch 132/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 54.9901 - accuracy: 0.656 - 0s 39us/step - loss: 64.5306 - accuracy: 0.6129 - val_loss: 31.5198 - val_accuracy: 0.5813\n",
      "Epoch 133/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 62.1579 - accuracy: 0.6191 - val_loss: 63.5985 - val_accuracy: 0.5938\n",
      "Epoch 134/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 65.6841 - accuracy: 0.5408 - val_loss: 103.5456 - val_accuracy: 0.5250\n",
      "Epoch 135/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 57.7476 - accuracy: 0.5815 - val_loss: 68.2287 - val_accuracy: 0.3500\n",
      "Epoch 136/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 62.6289 - accuracy: 0.6019 - val_loss: 94.4811 - val_accuracy: 0.5250\n",
      "Epoch 137/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 66.4635 - accuracy: 0.5690 - val_loss: 87.9674 - val_accuracy: 0.5938\n",
      "Epoch 138/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 64.0685 - accuracy: 0.6238 - val_loss: 90.1125 - val_accuracy: 0.5500\n",
      "Epoch 139/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 65.2525 - accuracy: 0.5376 - val_loss: 89.8958 - val_accuracy: 0.5188\n",
      "Epoch 140/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 60.7165 - accuracy: 0.5063 - val_loss: 146.6191 - val_accuracy: 0.4500\n",
      "Epoch 141/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 67.4346 - accuracy: 0.5721 - val_loss: 86.2636 - val_accuracy: 0.2750\n",
      "Epoch 142/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 65.4254 - accuracy: 0.3981 - val_loss: 42.2678 - val_accuracy: 0.5125\n",
      "Epoch 143/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 64.7960 - accuracy: 0.5925 - val_loss: 40.9865 - val_accuracy: 0.6125\n",
      "Epoch 144/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 60.7125 - accuracy: 0.5893 - val_loss: 78.3286 - val_accuracy: 0.5125\n",
      "Epoch 145/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 56.4508 - accuracy: 0.5815 - val_loss: 95.5669 - val_accuracy: 0.5312\n",
      "Epoch 146/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 60.9787 - accuracy: 0.5987 - val_loss: 33.9077 - val_accuracy: 0.6000\n",
      "Epoch 147/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 61.5717 - accuracy: 0.6207 - val_loss: 60.9987 - val_accuracy: 0.5813\n",
      "Epoch 148/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 62.1957 - accuracy: 0.5361 - val_loss: 79.7348 - val_accuracy: 0.5688\n",
      "Epoch 149/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 58.5180 - accuracy: 0.5972 - val_loss: 87.8235 - val_accuracy: 0.5562\n",
      "Epoch 150/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 59.8893 - accuracy: 0.6050 - val_loss: 55.9552 - val_accuracy: 0.5188\n",
      "Epoch 151/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 61.7879 - accuracy: 0.6050 - val_loss: 72.3439 - val_accuracy: 0.5125\n",
      "Epoch 152/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 63.0904 - accuracy: 0.6082 - val_loss: 44.9350 - val_accuracy: 0.6062\n",
      "Epoch 153/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 38.6544 - accuracy: 0.593 - 0s 38us/step - loss: 60.3170 - accuracy: 0.5266 - val_loss: 55.8768 - val_accuracy: 0.3125\n",
      "Epoch 154/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 57.4519 - accuracy: 0.4702 - val_loss: 77.6257 - val_accuracy: 0.5188\n",
      "Epoch 155/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 59.4074 - accuracy: 0.5831 - val_loss: 40.3292 - val_accuracy: 0.0625\n",
      "Epoch 156/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 52.2984 - accuracy: 0.5752 - val_loss: 55.7574 - val_accuracy: 0.5250\n",
      "Epoch 157/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 59.9600 - accuracy: 0.5752 - val_loss: 33.7535 - val_accuracy: 0.5688\n",
      "Epoch 158/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 60.4307 - accuracy: 0.6364 - val_loss: 55.5550 - val_accuracy: 0.5625\n",
      "Epoch 159/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 57.6001 - accuracy: 0.6129 - val_loss: 98.8640 - val_accuracy: 0.4938\n",
      "Epoch 160/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 61.2350 - accuracy: 0.5298 - val_loss: 42.4279 - val_accuracy: 0.4938\n",
      "Epoch 161/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 55.8041 - accuracy: 0.5940 - val_loss: 120.6607 - val_accuracy: 0.5375\n",
      "Epoch 162/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 57.1073 - accuracy: 0.6207 - val_loss: 44.0087 - val_accuracy: 0.5813\n",
      "Epoch 163/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 56.8507 - accuracy: 0.6348 - val_loss: 42.1388 - val_accuracy: 0.5938\n",
      "Epoch 164/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 55.7280 - accuracy: 0.5893 - val_loss: 62.9696 - val_accuracy: 0.5938\n",
      "Epoch 165/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 58.7287 - accuracy: 0.5940 - val_loss: 59.2934 - val_accuracy: 0.5875\n",
      "Epoch 166/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 56.8260 - accuracy: 0.6160 - val_loss: 39.7590 - val_accuracy: 0.5875\n",
      "Epoch 167/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 58.5884 - accuracy: 0.5392 - val_loss: 65.5554 - val_accuracy: 0.5813\n",
      "Epoch 168/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 58.7343 - accuracy: 0.5690 - val_loss: 76.6656 - val_accuracy: 0.5875\n",
      "Epoch 169/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 55.6250 - accuracy: 0.6270 - val_loss: 85.7665 - val_accuracy: 0.5437\n",
      "Epoch 170/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 53.6356 - accuracy: 0.4545 - val_loss: 27.7430 - val_accuracy: 0.3500\n",
      "Epoch 171/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 55.5168 - accuracy: 0.5737 - val_loss: 67.4352 - val_accuracy: 0.5875\n",
      "Epoch 172/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 60.2288 - accuracy: 0.6160 - val_loss: 21.1703 - val_accuracy: 0.5375\n",
      "Epoch 173/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 53.4968 - accuracy: 0.6129 - val_loss: 26.1278 - val_accuracy: 0.5437\n",
      "Epoch 174/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 55.6443 - accuracy: 0.4498 - val_loss: 55.8302 - val_accuracy: 0.6000\n",
      "Epoch 175/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 57.9371 - accuracy: 0.5455 - val_loss: 77.3576 - val_accuracy: 0.5938\n",
      "Epoch 176/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 57.8426 - accuracy: 0.5658 - val_loss: 19.9500 - val_accuracy: 0.5250\n",
      "Epoch 177/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 50.5473 - accuracy: 0.5893 - val_loss: 49.5601 - val_accuracy: 0.5875\n",
      "Epoch 178/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 54.4573 - accuracy: 0.5831 - val_loss: 52.0328 - val_accuracy: 0.6000\n",
      "Epoch 179/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 53.6313 - accuracy: 0.6129 - val_loss: 50.3866 - val_accuracy: 0.5437\n",
      "Epoch 180/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 55.1175 - accuracy: 0.5972 - val_loss: 23.2979 - val_accuracy: 0.5875\n",
      "Epoch 181/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 52.3390 - accuracy: 0.6317 - val_loss: 82.1258 - val_accuracy: 0.5813\n",
      "Epoch 182/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 51.3174 - accuracy: 0.5690 - val_loss: 26.5902 - val_accuracy: 0.6062\n",
      "Epoch 183/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 47.5284 - accuracy: 0.5737 - val_loss: 86.8009 - val_accuracy: 0.6562\n",
      "Epoch 184/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 53.1152 - accuracy: 0.5846 - val_loss: 26.2175 - val_accuracy: 0.5437\n",
      "Epoch 185/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 52.0173 - accuracy: 0.5784 - val_loss: 84.0409 - val_accuracy: 0.5562\n",
      "Epoch 186/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 55.7562 - accuracy: 0.5862 - val_loss: 74.6069 - val_accuracy: 0.5375\n",
      "Epoch 187/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 53.1102 - accuracy: 0.5799 - val_loss: 40.6838 - val_accuracy: 0.5625\n",
      "Epoch 188/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 54.5248 - accuracy: 0.5846 - val_loss: 26.1034 - val_accuracy: 0.6000\n",
      "Epoch 189/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 54.4174 - accuracy: 0.5752 - val_loss: 40.8087 - val_accuracy: 0.5813\n",
      "Epoch 190/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 52.9987 - accuracy: 0.5846 - val_loss: 33.3800 - val_accuracy: 0.6000\n",
      "Epoch 191/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 50.8547 - accuracy: 0.5831 - val_loss: 48.7130 - val_accuracy: 0.5688\n",
      "Epoch 192/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 54.1068 - accuracy: 0.5737 - val_loss: 30.3845 - val_accuracy: 0.5500\n",
      "Epoch 193/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 52.6456 - accuracy: 0.6191 - val_loss: 71.0371 - val_accuracy: 0.6062\n",
      "Epoch 194/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 54.7744 - accuracy: 0.5768 - val_loss: 37.4128 - val_accuracy: 0.5562\n",
      "Epoch 195/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 53.6844 - accuracy: 0.6160 - val_loss: 24.0821 - val_accuracy: 0.5750\n",
      "Epoch 196/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 50.0321 - accuracy: 0.5627 - val_loss: 37.1634 - val_accuracy: 0.5500\n",
      "Epoch 197/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 50.2057 - accuracy: 0.5533 - val_loss: 34.9242 - val_accuracy: 0.5063\n",
      "Epoch 198/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 49.3850 - accuracy: 0.5486 - val_loss: 54.9912 - val_accuracy: 0.5500\n",
      "Epoch 199/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 47.9737 - accuracy: 0.5361 - val_loss: 22.5142 - val_accuracy: 0.6062\n",
      "Epoch 200/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 50.6982 - accuracy: 0.5596 - val_loss: 55.0009 - val_accuracy: 0.5750\n",
      "Epoch 201/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 51.6406 - accuracy: 0.5470 - val_loss: 41.9591 - val_accuracy: 0.3938\n",
      "Epoch 202/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 49.6898 - accuracy: 0.5643 - val_loss: 73.5400 - val_accuracy: 0.6062\n",
      "Epoch 203/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 51.2821 - accuracy: 0.5596 - val_loss: 26.4197 - val_accuracy: 0.5813\n",
      "Epoch 204/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 49.2580 - accuracy: 0.6066 - val_loss: 55.6016 - val_accuracy: 0.5562\n",
      "Epoch 205/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 45.5250 - accuracy: 0.6191 - val_loss: 55.5403 - val_accuracy: 0.5750\n",
      "Epoch 206/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 49.9489 - accuracy: 0.6207 - val_loss: 58.1682 - val_accuracy: 0.6000\n",
      "Epoch 207/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 51.5258 - accuracy: 0.5815 - val_loss: 47.5983 - val_accuracy: 0.6125\n",
      "Epoch 208/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 53.0069 - accuracy: 0.5846 - val_loss: 33.6301 - val_accuracy: 0.5562\n",
      "Epoch 209/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 48.7264 - accuracy: 0.5658 - val_loss: 55.3151 - val_accuracy: 0.5437\n",
      "Epoch 210/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 49.3135 - accuracy: 0.5721 - val_loss: 53.9953 - val_accuracy: 0.5375\n",
      "Epoch 211/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 50.4677 - accuracy: 0.5972 - val_loss: 75.0313 - val_accuracy: 0.5500\n",
      "Epoch 212/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 48.7985 - accuracy: 0.5768 - val_loss: 49.2214 - val_accuracy: 0.5000\n",
      "Epoch 213/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 48.5043 - accuracy: 0.5643 - val_loss: 54.3349 - val_accuracy: 0.5625\n",
      "Epoch 214/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 49.0884 - accuracy: 0.5799 - val_loss: 59.7053 - val_accuracy: 0.5750\n",
      "Epoch 215/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 50.3675 - accuracy: 0.5972 - val_loss: 52.5878 - val_accuracy: 0.5625\n",
      "Epoch 216/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 48.5740 - accuracy: 0.5423 - val_loss: 68.1453 - val_accuracy: 0.5375\n",
      "Epoch 217/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 49.9920 - accuracy: 0.5408 - val_loss: 34.8526 - val_accuracy: 0.5750\n",
      "Epoch 218/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 49.9553 - accuracy: 0.6411 - val_loss: 38.8204 - val_accuracy: 0.5688\n",
      "Epoch 219/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 47.0667 - accuracy: 0.5658 - val_loss: 55.0955 - val_accuracy: 0.4812\n",
      "Epoch 220/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 48.8320 - accuracy: 0.5204 - val_loss: 61.7133 - val_accuracy: 0.4812\n",
      "Epoch 221/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 48.7754 - accuracy: 0.5643 - val_loss: 46.9428 - val_accuracy: 0.5688\n",
      "Epoch 222/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 45.8034 - accuracy: 0.5533 - val_loss: 66.2135 - val_accuracy: 0.5500\n",
      "Epoch 223/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 46.8431 - accuracy: 0.5768 - val_loss: 48.3049 - val_accuracy: 0.4250\n",
      "Epoch 224/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 44.7575 - accuracy: 0.5470 - val_loss: 66.6023 - val_accuracy: 0.5875\n",
      "Epoch 225/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 48.8033 - accuracy: 0.5313 - val_loss: 53.4020 - val_accuracy: 0.5063\n",
      "Epoch 226/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 47.0956 - accuracy: 0.5909 - val_loss: 21.5318 - val_accuracy: 0.5063\n",
      "Epoch 227/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 44.7651 - accuracy: 0.5721 - val_loss: 50.8884 - val_accuracy: 0.5813\n",
      "Epoch 228/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 44.4651 - accuracy: 0.5721 - val_loss: 38.4015 - val_accuracy: 0.5188\n",
      "Epoch 229/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 47.1168 - accuracy: 0.4828 - val_loss: 43.8939 - val_accuracy: 0.5375\n",
      "Epoch 230/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 46.0283 - accuracy: 0.5768 - val_loss: 53.6467 - val_accuracy: 0.4812\n",
      "Epoch 231/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 48.0220 - accuracy: 0.5564 - val_loss: 43.7082 - val_accuracy: 0.5688\n",
      "Epoch 232/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 47.0461 - accuracy: 0.5925 - val_loss: 62.0652 - val_accuracy: 0.5562\n",
      "Epoch 233/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 46.5089 - accuracy: 0.5940 - val_loss: 43.5953 - val_accuracy: 0.5500\n",
      "Epoch 234/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 46.5663 - accuracy: 0.5423 - val_loss: 50.3693 - val_accuracy: 0.5750\n",
      "Epoch 235/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 47.7194 - accuracy: 0.6191 - val_loss: 52.3613 - val_accuracy: 0.5562\n",
      "Epoch 236/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 42.5838 - accuracy: 0.6113 - val_loss: 63.8174 - val_accuracy: 0.4625\n",
      "Epoch 237/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 45.8376 - accuracy: 0.6050 - val_loss: 25.6238 - val_accuracy: 0.5500\n",
      "Epoch 238/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 42.7232 - accuracy: 0.5282 - val_loss: 60.5781 - val_accuracy: 0.4875\n",
      "Epoch 239/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 45.6454 - accuracy: 0.5690 - val_loss: 65.2104 - val_accuracy: 0.5750\n",
      "Epoch 240/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 42.2141 - accuracy: 0.6003 - val_loss: 70.6674 - val_accuracy: 0.5562\n",
      "Epoch 241/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 68.5851 - accuracy: 0.562 - 0s 42us/step - loss: 46.3719 - accuracy: 0.5611 - val_loss: 60.2199 - val_accuracy: 0.5437\n",
      "Epoch 242/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 44.7715 - accuracy: 0.5596 - val_loss: 46.4681 - val_accuracy: 0.5188\n",
      "Epoch 243/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 45.3321 - accuracy: 0.5172 - val_loss: 28.0191 - val_accuracy: 0.5813\n",
      "Epoch 244/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 45.9353 - accuracy: 0.6082 - val_loss: 50.0687 - val_accuracy: 0.5688\n",
      "Epoch 245/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 43.7549 - accuracy: 0.6473 - val_loss: 75.3889 - val_accuracy: 0.5875\n",
      "Epoch 246/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 43.8986 - accuracy: 0.5940 - val_loss: 42.6605 - val_accuracy: 0.4313\n",
      "Epoch 247/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 42.7846 - accuracy: 0.5517 - val_loss: 37.3829 - val_accuracy: 0.4437\n",
      "Epoch 248/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 42.6570 - accuracy: 0.5376 - val_loss: 52.7143 - val_accuracy: 0.5625\n",
      "Epoch 249/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 43.0737 - accuracy: 0.5940 - val_loss: 63.4541 - val_accuracy: 0.5625\n",
      "Epoch 250/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 43.4379 - accuracy: 0.5909 - val_loss: 60.0187 - val_accuracy: 0.6500\n",
      "Epoch 251/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 44.3891 - accuracy: 0.6144 - val_loss: 33.0970 - val_accuracy: 0.5688\n",
      "Epoch 252/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 44.4583 - accuracy: 0.5455 - val_loss: 49.4672 - val_accuracy: 0.4750\n",
      "Epoch 253/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 41.1599 - accuracy: 0.5846 - val_loss: 55.3422 - val_accuracy: 0.5312\n",
      "Epoch 254/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 44.4537 - accuracy: 0.6050 - val_loss: 51.2156 - val_accuracy: 0.6500\n",
      "Epoch 255/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 49.4184 - accuracy: 0.750 - 0s 41us/step - loss: 43.4472 - accuracy: 0.6489 - val_loss: 18.5613 - val_accuracy: 0.5188\n",
      "Epoch 256/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 42.2738 - accuracy: 0.5846 - val_loss: 62.2805 - val_accuracy: 0.5688\n",
      "Epoch 257/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 43.0991 - accuracy: 0.5455 - val_loss: 46.6460 - val_accuracy: 0.5250\n",
      "Epoch 258/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 42.6820 - accuracy: 0.6207 - val_loss: 19.9719 - val_accuracy: 0.6062\n",
      "Epoch 259/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 39.5969 - accuracy: 0.5956 - val_loss: 34.2075 - val_accuracy: 0.6062\n",
      "Epoch 260/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 41.6651 - accuracy: 0.5502 - val_loss: 33.9721 - val_accuracy: 0.6062\n",
      "Epoch 261/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 40.5315 - accuracy: 0.5486 - val_loss: 29.0403 - val_accuracy: 0.5875\n",
      "Epoch 262/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 41.7973 - accuracy: 0.6270 - val_loss: 14.9448 - val_accuracy: 0.5875\n",
      "Epoch 263/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 33.0139 - accuracy: 0.4906 - val_loss: 41.6342 - val_accuracy: 0.5750\n",
      "Epoch 264/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 40.7539 - accuracy: 0.6176 - val_loss: 29.2477 - val_accuracy: 0.6062\n",
      "Epoch 265/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 38.6592 - accuracy: 0.5925 - val_loss: 57.0978 - val_accuracy: 0.6062\n",
      "Epoch 266/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 42.8623 - accuracy: 0.5799 - val_loss: 48.2612 - val_accuracy: 0.4688\n",
      "Epoch 267/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 41.8994 - accuracy: 0.5737 - val_loss: 51.6528 - val_accuracy: 0.6000\n",
      "Epoch 268/1000\n",
      "638/638 [==============================] - 0s 48us/step - loss: 39.3073 - accuracy: 0.5878 - val_loss: 47.1743 - val_accuracy: 0.5562\n",
      "Epoch 269/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 40.7862 - accuracy: 0.5658 - val_loss: 44.1253 - val_accuracy: 0.5625\n",
      "Epoch 270/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 42.8069 - accuracy: 0.5517 - val_loss: 16.9621 - val_accuracy: 0.5500\n",
      "Epoch 271/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 39.2752 - accuracy: 0.5251 - val_loss: 50.3189 - val_accuracy: 0.4875\n",
      "Epoch 272/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 47.5629 - accuracy: 0.406 - 0s 42us/step - loss: 41.9330 - accuracy: 0.5470 - val_loss: 58.8409 - val_accuracy: 0.7000\n",
      "Epoch 273/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 42.5052 - accuracy: 0.6223 - val_loss: 40.2989 - val_accuracy: 0.5562\n",
      "Epoch 274/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 41.0470 - accuracy: 0.6019 - val_loss: 19.3535 - val_accuracy: 0.5750\n",
      "Epoch 275/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 38.9644 - accuracy: 0.5690 - val_loss: 52.1431 - val_accuracy: 0.5813\n",
      "Epoch 276/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 40.0952 - accuracy: 0.5784 - val_loss: 39.5053 - val_accuracy: 0.5938\n",
      "Epoch 277/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 39.8776 - accuracy: 0.6176 - val_loss: 47.9224 - val_accuracy: 0.5875\n",
      "Epoch 278/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 41.7213 - accuracy: 0.6097 - val_loss: 22.1208 - val_accuracy: 0.5938\n",
      "Epoch 279/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 37.9909 - accuracy: 0.5674 - val_loss: 46.8559 - val_accuracy: 0.5562\n",
      "Epoch 280/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 40.4624 - accuracy: 0.5517 - val_loss: 43.5270 - val_accuracy: 0.5250\n",
      "Epoch 281/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 40.4879 - accuracy: 0.6270 - val_loss: 32.3887 - val_accuracy: 0.5250\n",
      "Epoch 282/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 39.4607 - accuracy: 0.5846 - val_loss: 61.0312 - val_accuracy: 0.5312\n",
      "Epoch 283/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 40.3884 - accuracy: 0.5799 - val_loss: 56.2229 - val_accuracy: 0.5875\n",
      "Epoch 284/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 39.7151 - accuracy: 0.6395 - val_loss: 56.6969 - val_accuracy: 0.6062\n",
      "Epoch 285/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 37.8364 - accuracy: 0.5658 - val_loss: 41.0412 - val_accuracy: 0.4125\n",
      "Epoch 286/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 36.8441 - accuracy: 0.5705 - val_loss: 39.7278 - val_accuracy: 0.5625\n",
      "Epoch 287/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 37.3670 - accuracy: 0.5940 - val_loss: 22.7076 - val_accuracy: 0.5875\n",
      "Epoch 288/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 39.9672 - accuracy: 0.6003 - val_loss: 25.6199 - val_accuracy: 0.5437\n",
      "Epoch 289/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 39.5989 - accuracy: 0.6238 - val_loss: 15.1567 - val_accuracy: 0.4875\n",
      "Epoch 290/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 36.4419 - accuracy: 0.5721 - val_loss: 57.9583 - val_accuracy: 0.5875\n",
      "Epoch 291/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 39.0098 - accuracy: 0.6348 - val_loss: 42.2427 - val_accuracy: 0.5000\n",
      "Epoch 292/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 35.6425 - accuracy: 0.5721 - val_loss: 60.4796 - val_accuracy: 0.5813\n",
      "Epoch 293/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 38.2053 - accuracy: 0.5846 - val_loss: 44.0083 - val_accuracy: 0.5688\n",
      "Epoch 294/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 38.8264 - accuracy: 0.6489 - val_loss: 42.4537 - val_accuracy: 0.5625\n",
      "Epoch 295/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 39.9101 - accuracy: 0.6505 - val_loss: 45.0152 - val_accuracy: 0.5063\n",
      "Epoch 296/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 37.8333 - accuracy: 0.5972 - val_loss: 43.6803 - val_accuracy: 0.5875\n",
      "Epoch 297/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 38.2779 - accuracy: 0.5611 - val_loss: 38.8246 - val_accuracy: 0.6062\n",
      "Epoch 298/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 36.8671 - accuracy: 0.6097 - val_loss: 38.7717 - val_accuracy: 0.5437\n",
      "Epoch 299/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 37.3437 - accuracy: 0.5313 - val_loss: 33.4539 - val_accuracy: 0.5750\n",
      "Epoch 300/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 37.6004 - accuracy: 0.5627 - val_loss: 38.8739 - val_accuracy: 0.4125\n",
      "Epoch 301/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 35.9928 - accuracy: 0.5329 - val_loss: 41.4130 - val_accuracy: 0.5938\n",
      "Epoch 302/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 39.0510 - accuracy: 0.6332 - val_loss: 28.0869 - val_accuracy: 0.5813\n",
      "Epoch 303/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 37.2991 - accuracy: 0.6003 - val_loss: 28.5357 - val_accuracy: 0.5562\n",
      "Epoch 304/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 37.7384 - accuracy: 0.6238 - val_loss: 42.7147 - val_accuracy: 0.5875\n",
      "Epoch 305/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 38.0211 - accuracy: 0.6379 - val_loss: 38.7270 - val_accuracy: 0.5625\n",
      "Epoch 306/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 37.8059 - accuracy: 0.6160 - val_loss: 16.1857 - val_accuracy: 0.5750\n",
      "Epoch 307/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 34.3926 - accuracy: 0.5392 - val_loss: 35.2422 - val_accuracy: 0.4750\n",
      "Epoch 308/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 37.0096 - accuracy: 0.5940 - val_loss: 48.8498 - val_accuracy: 0.5625\n",
      "Epoch 309/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 36.4630 - accuracy: 0.5705 - val_loss: 40.2488 - val_accuracy: 0.5938\n",
      "Epoch 310/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 34.3508 - accuracy: 0.6113 - val_loss: 18.3390 - val_accuracy: 0.5875\n",
      "Epoch 311/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 34.4909 - accuracy: 0.5768 - val_loss: 54.7162 - val_accuracy: 0.6062\n",
      "Epoch 312/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 34.5185 - accuracy: 0.6082 - val_loss: 45.4665 - val_accuracy: 0.5437\n",
      "Epoch 313/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 37.4907 - accuracy: 0.5721 - val_loss: 17.5374 - val_accuracy: 0.5188\n",
      "Epoch 314/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 34.0631 - accuracy: 0.5987 - val_loss: 27.7850 - val_accuracy: 0.5125\n",
      "Epoch 315/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 37.0124 - accuracy: 0.5815 - val_loss: 31.7623 - val_accuracy: 0.5312\n",
      "Epoch 316/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 37.4194 - accuracy: 0.6050 - val_loss: 15.2308 - val_accuracy: 0.5875\n",
      "Epoch 317/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 31.9240 - accuracy: 0.5721 - val_loss: 26.9953 - val_accuracy: 0.5875\n",
      "Epoch 318/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 35.0491 - accuracy: 0.5799 - val_loss: 49.1783 - val_accuracy: 0.6000\n",
      "Epoch 319/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 47.8291 - accuracy: 0.718 - 0s 45us/step - loss: 35.9818 - accuracy: 0.5815 - val_loss: 38.3798 - val_accuracy: 0.5375\n",
      "Epoch 320/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 36.3187 - accuracy: 0.5705 - val_loss: 28.8220 - val_accuracy: 0.6313\n",
      "Epoch 321/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 36.6406 - accuracy: 0.5674 - val_loss: 22.7181 - val_accuracy: 0.5063\n",
      "Epoch 322/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 34.2212 - accuracy: 0.5846 - val_loss: 69.6113 - val_accuracy: 0.6000\n",
      "Epoch 323/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 34.4132 - accuracy: 0.5987 - val_loss: 24.8476 - val_accuracy: 0.4062\n",
      "Epoch 324/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 35.5790 - accuracy: 0.5768 - val_loss: 28.7865 - val_accuracy: 0.5688\n",
      "Epoch 325/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 29.3303 - accuracy: 0.625 - 0s 44us/step - loss: 35.8248 - accuracy: 0.5658 - val_loss: 25.4719 - val_accuracy: 0.5750\n",
      "Epoch 326/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 33.5920 - accuracy: 0.5799 - val_loss: 60.9012 - val_accuracy: 0.4938\n",
      "Epoch 327/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 35.4391 - accuracy: 0.6144 - val_loss: 48.6486 - val_accuracy: 0.5938\n",
      "Epoch 328/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 35.5389 - accuracy: 0.5831 - val_loss: 19.3826 - val_accuracy: 0.4625\n",
      "Epoch 329/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 33.1953 - accuracy: 0.5580 - val_loss: 49.2871 - val_accuracy: 0.4812\n",
      "Epoch 330/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 34.6102 - accuracy: 0.5799 - val_loss: 29.3299 - val_accuracy: 0.5813\n",
      "Epoch 331/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 35.3882 - accuracy: 0.5721 - val_loss: 41.6357 - val_accuracy: 0.5875\n",
      "Epoch 332/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 32.5892 - accuracy: 0.5940 - val_loss: 50.0290 - val_accuracy: 0.5312\n",
      "Epoch 333/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 32.3623 - accuracy: 0.5329 - val_loss: 28.8201 - val_accuracy: 0.5875\n",
      "Epoch 334/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 35.6814 - accuracy: 0.6254 - val_loss: 39.2834 - val_accuracy: 0.5875\n",
      "Epoch 335/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 34.2216 - accuracy: 0.5455 - val_loss: 38.3420 - val_accuracy: 0.5938\n",
      "Epoch 336/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 34.3080 - accuracy: 0.5987 - val_loss: 47.4998 - val_accuracy: 0.4938\n",
      "Epoch 337/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 36.0154 - accuracy: 0.5674 - val_loss: 14.3411 - val_accuracy: 0.5000\n",
      "Epoch 338/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 31.6992 - accuracy: 0.5439 - val_loss: 42.5731 - val_accuracy: 0.5875\n",
      "Epoch 339/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 34.1386 - accuracy: 0.6144 - val_loss: 45.8062 - val_accuracy: 0.4875\n",
      "Epoch 340/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 33.9813 - accuracy: 0.6144 - val_loss: 32.2023 - val_accuracy: 0.6500\n",
      "Epoch 341/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 33.2918 - accuracy: 0.6129 - val_loss: 38.7824 - val_accuracy: 0.5875\n",
      "Epoch 342/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 33.2795 - accuracy: 0.5925 - val_loss: 48.2778 - val_accuracy: 0.6187\n",
      "Epoch 343/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 36.0269 - accuracy: 0.5972 - val_loss: 22.3542 - val_accuracy: 0.6250\n",
      "Epoch 344/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 32.8717 - accuracy: 0.6129 - val_loss: 23.3799 - val_accuracy: 0.5625\n",
      "Epoch 345/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 33.5202 - accuracy: 0.5862 - val_loss: 38.2627 - val_accuracy: 0.5688\n",
      "Epoch 346/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 34.3295 - accuracy: 0.5987 - val_loss: 50.3514 - val_accuracy: 0.5063\n",
      "Epoch 347/1000\n",
      "638/638 [==============================] - 0s 50us/step - loss: 33.0823 - accuracy: 0.5831 - val_loss: 20.0822 - val_accuracy: 0.5875\n",
      "Epoch 348/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 32.7610 - accuracy: 0.5846 - val_loss: 21.1732 - val_accuracy: 0.5437\n",
      "Epoch 349/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 34.3023 - accuracy: 0.5925 - val_loss: 29.5304 - val_accuracy: 0.4437\n",
      "Epoch 350/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 30.8453 - accuracy: 0.5737 - val_loss: 30.8522 - val_accuracy: 0.5875\n",
      "Epoch 351/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 33.2753 - accuracy: 0.6207 - val_loss: 53.4208 - val_accuracy: 0.5500\n",
      "Epoch 352/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 31.5052 - accuracy: 0.5690 - val_loss: 31.3959 - val_accuracy: 0.5625\n",
      "Epoch 353/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 32.3477 - accuracy: 0.5705 - val_loss: 32.4896 - val_accuracy: 0.6000\n",
      "Epoch 354/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 31.7896 - accuracy: 0.5690 - val_loss: 15.6288 - val_accuracy: 0.4437\n",
      "Epoch 355/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 30.9118 - accuracy: 0.5455 - val_loss: 50.1227 - val_accuracy: 0.5688\n",
      "Epoch 356/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 31.4207 - accuracy: 0.5752 - val_loss: 25.2877 - val_accuracy: 0.5125\n",
      "Epoch 357/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 22.1770 - accuracy: 0.593 - 0s 41us/step - loss: 31.4993 - accuracy: 0.6207 - val_loss: 21.5574 - val_accuracy: 0.5938\n",
      "Epoch 358/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 33.6902 - accuracy: 0.6285 - val_loss: 29.2556 - val_accuracy: 0.5875\n",
      "Epoch 359/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 31.7571 - accuracy: 0.6144 - val_loss: 46.6851 - val_accuracy: 0.5188\n",
      "Epoch 360/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 33.2301 - accuracy: 0.6129 - val_loss: 24.5392 - val_accuracy: 0.5688\n",
      "Epoch 361/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 33.7924 - accuracy: 0.6097 - val_loss: 41.8525 - val_accuracy: 0.4625\n",
      "Epoch 362/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 33.6781 - accuracy: 0.5815 - val_loss: 35.1931 - val_accuracy: 0.5500\n",
      "Epoch 363/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 33.7562 - accuracy: 0.6144 - val_loss: 24.9323 - val_accuracy: 0.5375\n",
      "Epoch 364/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 33.9329 - accuracy: 0.6489 - val_loss: 15.0348 - val_accuracy: 0.6000\n",
      "Epoch 365/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 31.7288 - accuracy: 0.5878 - val_loss: 27.5679 - val_accuracy: 0.4563\n",
      "Epoch 366/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 31.7528 - accuracy: 0.6176 - val_loss: 17.2980 - val_accuracy: 0.6000\n",
      "Epoch 367/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 32.0101 - accuracy: 0.5658 - val_loss: 37.8125 - val_accuracy: 0.5500\n",
      "Epoch 368/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 33.7391 - accuracy: 0.6207 - val_loss: 19.7767 - val_accuracy: 0.6062\n",
      "Epoch 369/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 32.4060 - accuracy: 0.5925 - val_loss: 16.6834 - val_accuracy: 0.5938\n",
      "Epoch 370/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 32.0209 - accuracy: 0.6034 - val_loss: 14.0558 - val_accuracy: 0.5813\n",
      "Epoch 371/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 31.8771 - accuracy: 0.5596 - val_loss: 26.8443 - val_accuracy: 0.6062\n",
      "Epoch 372/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 32.0921 - accuracy: 0.5752 - val_loss: 41.2635 - val_accuracy: 0.5500\n",
      "Epoch 373/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 32.4845 - accuracy: 0.6332 - val_loss: 45.9005 - val_accuracy: 0.5813\n",
      "Epoch 374/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 34.2221 - accuracy: 0.6270 - val_loss: 26.4431 - val_accuracy: 0.5938\n",
      "Epoch 375/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 29.6121 - accuracy: 0.5909 - val_loss: 45.0630 - val_accuracy: 0.6062\n",
      "Epoch 376/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 33.7193 - accuracy: 0.6034 - val_loss: 20.5498 - val_accuracy: 0.5938\n",
      "Epoch 377/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 32.5803 - accuracy: 0.6003 - val_loss: 11.5841 - val_accuracy: 0.5875\n",
      "Epoch 378/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 10.7166 - accuracy: 0.687 - 0s 45us/step - loss: 28.1440 - accuracy: 0.5784 - val_loss: 17.8753 - val_accuracy: 0.5437\n",
      "Epoch 379/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 29.6189 - accuracy: 0.6191 - val_loss: 34.5194 - val_accuracy: 0.5750\n",
      "Epoch 380/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 32.5135 - accuracy: 0.6411 - val_loss: 47.9104 - val_accuracy: 0.5813\n",
      "Epoch 381/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 32.5467 - accuracy: 0.6050 - val_loss: 50.2669 - val_accuracy: 0.6000\n",
      "Epoch 382/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 31.6977 - accuracy: 0.6160 - val_loss: 17.1265 - val_accuracy: 0.5875\n",
      "Epoch 383/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 29.2790 - accuracy: 0.6379 - val_loss: 42.8479 - val_accuracy: 0.6000\n",
      "Epoch 384/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 43.5168 - accuracy: 0.656 - 0s 33us/step - loss: 31.8908 - accuracy: 0.5956 - val_loss: 21.8219 - val_accuracy: 0.6125\n",
      "Epoch 385/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 29.9977 - accuracy: 0.5972 - val_loss: 30.5768 - val_accuracy: 0.6125\n",
      "Epoch 386/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 29.8211 - accuracy: 0.6176 - val_loss: 26.9570 - val_accuracy: 0.6000\n",
      "Epoch 387/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 32.0468 - accuracy: 0.6285 - val_loss: 32.6853 - val_accuracy: 0.5938\n",
      "Epoch 388/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 32.3980 - accuracy: 0.5987 - val_loss: 34.4703 - val_accuracy: 0.5625\n",
      "Epoch 389/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 30.9282 - accuracy: 0.562 - 0s 33us/step - loss: 31.1092 - accuracy: 0.6348 - val_loss: 27.3301 - val_accuracy: 0.5938\n",
      "Epoch 390/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 31.9679 - accuracy: 0.6458 - val_loss: 34.0936 - val_accuracy: 0.5938\n",
      "Epoch 391/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 32.1385 - accuracy: 0.6254 - val_loss: 30.1569 - val_accuracy: 0.6000\n",
      "Epoch 392/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 30.8878 - accuracy: 0.6050 - val_loss: 20.1169 - val_accuracy: 0.5813\n",
      "Epoch 393/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 30.4774 - accuracy: 0.6191 - val_loss: 33.3558 - val_accuracy: 0.4812\n",
      "Epoch 394/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 30.8736 - accuracy: 0.5925 - val_loss: 26.0812 - val_accuracy: 0.5938\n",
      "Epoch 395/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 30.8607 - accuracy: 0.5909 - val_loss: 41.2812 - val_accuracy: 0.5750\n",
      "Epoch 396/1000\n",
      "638/638 [==============================] - 0s 30us/step - loss: 31.7173 - accuracy: 0.6536 - val_loss: 28.7091 - val_accuracy: 0.6000\n",
      "Epoch 397/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 32.1758 - accuracy: 0.6505 - val_loss: 33.2517 - val_accuracy: 0.6000\n",
      "Epoch 398/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 34.3748 - accuracy: 0.625 - 0s 42us/step - loss: 31.1327 - accuracy: 0.6223 - val_loss: 44.0090 - val_accuracy: 0.5063\n",
      "Epoch 399/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 30.0272 - accuracy: 0.6176 - val_loss: 36.4416 - val_accuracy: 0.5625\n",
      "Epoch 400/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 31.0312 - accuracy: 0.5674 - val_loss: 26.5868 - val_accuracy: 0.5938\n",
      "Epoch 401/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 30.4558 - accuracy: 0.5972 - val_loss: 36.6218 - val_accuracy: 0.5938\n",
      "Epoch 402/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 29.8383 - accuracy: 0.5815 - val_loss: 38.3658 - val_accuracy: 0.5875\n",
      "Epoch 403/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 31.2106 - accuracy: 0.6426 - val_loss: 38.2158 - val_accuracy: 0.5875\n",
      "Epoch 404/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 31.3230 - accuracy: 0.6003 - val_loss: 47.2812 - val_accuracy: 0.5875\n",
      "Epoch 405/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 30.0995 - accuracy: 0.6285 - val_loss: 26.8287 - val_accuracy: 0.5625\n",
      "Epoch 406/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 30.5970 - accuracy: 0.6113 - val_loss: 44.0912 - val_accuracy: 0.5938\n",
      "Epoch 407/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 31.2180 - accuracy: 0.6113 - val_loss: 15.6420 - val_accuracy: 0.5875\n",
      "Epoch 408/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 29.0482 - accuracy: 0.5815 - val_loss: 34.1663 - val_accuracy: 0.6000\n",
      "Epoch 409/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 29.3558 - accuracy: 0.6097 - val_loss: 46.1516 - val_accuracy: 0.4938\n",
      "Epoch 410/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 30.7454 - accuracy: 0.6144 - val_loss: 32.5562 - val_accuracy: 0.5437\n",
      "Epoch 411/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 31.1966 - accuracy: 0.6019 - val_loss: 28.4988 - val_accuracy: 0.6125\n",
      "Epoch 412/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 27.8677 - accuracy: 0.6536 - val_loss: 26.9284 - val_accuracy: 0.5250\n",
      "Epoch 413/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 29.3353 - accuracy: 0.6348 - val_loss: 36.2738 - val_accuracy: 0.6187\n",
      "Epoch 414/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 30.5363 - accuracy: 0.6207 - val_loss: 29.2950 - val_accuracy: 0.5938\n",
      "Epoch 415/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 29.9262 - accuracy: 0.6442 - val_loss: 22.9057 - val_accuracy: 0.5750\n",
      "Epoch 416/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 30.3330 - accuracy: 0.6348 - val_loss: 31.0650 - val_accuracy: 0.5938\n",
      "Epoch 417/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 31.0897 - accuracy: 0.6473 - val_loss: 14.8879 - val_accuracy: 0.5625\n",
      "Epoch 418/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 30.1592 - accuracy: 0.6552 - val_loss: 31.5126 - val_accuracy: 0.5125\n",
      "Epoch 419/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 29.9463 - accuracy: 0.6019 - val_loss: 39.3025 - val_accuracy: 0.6000\n",
      "Epoch 420/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 30.1742 - accuracy: 0.6473 - val_loss: 28.8391 - val_accuracy: 0.5562\n",
      "Epoch 421/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 30.4224 - accuracy: 0.6160 - val_loss: 36.5527 - val_accuracy: 0.5625\n",
      "Epoch 422/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 25.3058 - accuracy: 0.5564 - val_loss: 48.8464 - val_accuracy: 0.5938\n",
      "Epoch 423/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 29.6470 - accuracy: 0.6207 - val_loss: 19.9136 - val_accuracy: 0.5938\n",
      "Epoch 424/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 27.6573 - accuracy: 0.6254 - val_loss: 23.5650 - val_accuracy: 0.5938\n",
      "Epoch 425/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 28.9357 - accuracy: 0.6160 - val_loss: 39.4551 - val_accuracy: 0.6062\n",
      "Epoch 426/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 28.3956 - accuracy: 0.6176 - val_loss: 32.5811 - val_accuracy: 0.5750\n",
      "Epoch 427/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 28.5237 - accuracy: 0.6505 - val_loss: 37.8205 - val_accuracy: 0.5063\n",
      "Epoch 428/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 29.9609 - accuracy: 0.6082 - val_loss: 27.4566 - val_accuracy: 0.6000\n",
      "Epoch 429/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 29.2176 - accuracy: 0.6019 - val_loss: 26.1111 - val_accuracy: 0.5750\n",
      "Epoch 430/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 29.4745 - accuracy: 0.6176 - val_loss: 35.6188 - val_accuracy: 0.6000\n",
      "Epoch 431/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 29.7966 - accuracy: 0.6034 - val_loss: 36.0449 - val_accuracy: 0.5688\n",
      "Epoch 432/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 28.9064 - accuracy: 0.5987 - val_loss: 25.3446 - val_accuracy: 0.5688\n",
      "Epoch 433/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 28.0574 - accuracy: 0.6207 - val_loss: 19.0641 - val_accuracy: 0.5938\n",
      "Epoch 434/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 28.3324 - accuracy: 0.6426 - val_loss: 28.9954 - val_accuracy: 0.5875\n",
      "Epoch 435/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 28.2397 - accuracy: 0.6442 - val_loss: 42.2676 - val_accuracy: 0.5000\n",
      "Epoch 436/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 29.0240 - accuracy: 0.5846 - val_loss: 25.8838 - val_accuracy: 0.5562\n",
      "Epoch 437/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 28.0147 - accuracy: 0.6191 - val_loss: 31.7723 - val_accuracy: 0.5312\n",
      "Epoch 438/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 29.3168 - accuracy: 0.5831 - val_loss: 27.7920 - val_accuracy: 0.5688\n",
      "Epoch 439/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 29.2696 - accuracy: 0.5987 - val_loss: 13.0742 - val_accuracy: 0.5125\n",
      "Epoch 440/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 26.4270 - accuracy: 0.6270 - val_loss: 30.0054 - val_accuracy: 0.5375\n",
      "Epoch 441/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 27.9330 - accuracy: 0.6552 - val_loss: 29.0128 - val_accuracy: 0.5875\n",
      "Epoch 442/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 30.0241 - accuracy: 0.6270 - val_loss: 21.2383 - val_accuracy: 0.5938\n",
      "Epoch 443/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 28.8083 - accuracy: 0.5878 - val_loss: 31.4090 - val_accuracy: 0.6313\n",
      "Epoch 444/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 29.1895 - accuracy: 0.6332 - val_loss: 19.5960 - val_accuracy: 0.5750\n",
      "Epoch 445/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 25.6137 - accuracy: 0.6599 - val_loss: 35.8391 - val_accuracy: 0.6000\n",
      "Epoch 446/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 27.0879 - accuracy: 0.5752 - val_loss: 36.6649 - val_accuracy: 0.6187\n",
      "Epoch 447/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 26.9874 - accuracy: 0.6176 - val_loss: 32.7844 - val_accuracy: 0.6000\n",
      "Epoch 448/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 27.6809 - accuracy: 0.6097 - val_loss: 26.9867 - val_accuracy: 0.6000\n",
      "Epoch 449/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 28.3911 - accuracy: 0.6520 - val_loss: 27.0181 - val_accuracy: 0.5312\n",
      "Epoch 450/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 28.0487 - accuracy: 0.5878 - val_loss: 30.9001 - val_accuracy: 0.5625\n",
      "Epoch 451/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 28.8172 - accuracy: 0.6552 - val_loss: 28.0034 - val_accuracy: 0.5500\n",
      "Epoch 452/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 26.8951 - accuracy: 0.5799 - val_loss: 21.0171 - val_accuracy: 0.5875\n",
      "Epoch 453/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 26.1824 - accuracy: 0.6191 - val_loss: 29.4280 - val_accuracy: 0.5875\n",
      "Epoch 454/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 26.6561 - accuracy: 0.6191 - val_loss: 18.2717 - val_accuracy: 0.5938\n",
      "Epoch 455/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 26.6057 - accuracy: 0.6034 - val_loss: 26.7522 - val_accuracy: 0.5188\n",
      "Epoch 456/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 25.5879 - accuracy: 0.6379 - val_loss: 19.9623 - val_accuracy: 0.6250\n",
      "Epoch 457/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 21.0965 - accuracy: 0.531 - 0s 42us/step - loss: 26.6152 - accuracy: 0.6317 - val_loss: 36.5816 - val_accuracy: 0.5375\n",
      "Epoch 458/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 27.0233 - accuracy: 0.6395 - val_loss: 13.5760 - val_accuracy: 0.5813\n",
      "Epoch 459/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 13.8397 - accuracy: 0.718 - 0s 36us/step - loss: 26.9579 - accuracy: 0.6411 - val_loss: 28.0686 - val_accuracy: 0.5875\n",
      "Epoch 460/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 27.5775 - accuracy: 0.6348 - val_loss: 43.4019 - val_accuracy: 0.5938\n",
      "Epoch 461/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 27.8870 - accuracy: 0.6003 - val_loss: 28.1848 - val_accuracy: 0.6375\n",
      "Epoch 462/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 24.7338 - accuracy: 0.6254 - val_loss: 26.2733 - val_accuracy: 0.5312\n",
      "Epoch 463/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 28.3411 - accuracy: 0.6379 - val_loss: 35.2504 - val_accuracy: 0.5750\n",
      "Epoch 464/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 26.9148 - accuracy: 0.6411 - val_loss: 27.7246 - val_accuracy: 0.5750\n",
      "Epoch 465/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 28.2958 - accuracy: 0.6317 - val_loss: 39.0308 - val_accuracy: 0.5500\n",
      "Epoch 466/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 28.6807 - accuracy: 0.6442 - val_loss: 32.4039 - val_accuracy: 0.6000\n",
      "Epoch 467/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 25.2219 - accuracy: 0.6317 - val_loss: 26.3757 - val_accuracy: 0.5938\n",
      "Epoch 468/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 28.5616 - accuracy: 0.6536 - val_loss: 21.9520 - val_accuracy: 0.5938\n",
      "Epoch 469/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 27.7804 - accuracy: 0.6270 - val_loss: 29.3931 - val_accuracy: 0.5500\n",
      "Epoch 470/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 28.6250 - accuracy: 0.6379 - val_loss: 23.0097 - val_accuracy: 0.5250\n",
      "Epoch 471/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 28.1382 - accuracy: 0.6411 - val_loss: 21.5204 - val_accuracy: 0.6000\n",
      "Epoch 472/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 26.3133 - accuracy: 0.6379 - val_loss: 36.3257 - val_accuracy: 0.5250\n",
      "Epoch 473/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 26.7073 - accuracy: 0.6473 - val_loss: 30.7583 - val_accuracy: 0.6187\n",
      "Epoch 474/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 26.0141 - accuracy: 0.5987 - val_loss: 43.5294 - val_accuracy: 0.6062\n",
      "Epoch 475/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 28.3101 - accuracy: 0.6379 - val_loss: 30.5878 - val_accuracy: 0.5875\n",
      "Epoch 476/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 24.9924 - accuracy: 0.6160 - val_loss: 29.5179 - val_accuracy: 0.5875\n",
      "Epoch 477/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 25.9968 - accuracy: 0.6379 - val_loss: 12.8719 - val_accuracy: 0.5562\n",
      "Epoch 478/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 25.6733 - accuracy: 0.6301 - val_loss: 47.4560 - val_accuracy: 0.6187\n",
      "Epoch 479/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 26.8241 - accuracy: 0.6254 - val_loss: 17.8621 - val_accuracy: 0.6187\n",
      "Epoch 480/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 26.8446 - accuracy: 0.5846 - val_loss: 14.4307 - val_accuracy: 0.6187\n",
      "Epoch 481/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 24.2939 - accuracy: 0.6082 - val_loss: 32.8046 - val_accuracy: 0.5688\n",
      "Epoch 482/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 26.0635 - accuracy: 0.6176 - val_loss: 39.8180 - val_accuracy: 0.5938\n",
      "Epoch 483/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 28.0235 - accuracy: 0.6614 - val_loss: 23.5988 - val_accuracy: 0.5750\n",
      "Epoch 484/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 25.9869 - accuracy: 0.6238 - val_loss: 27.5650 - val_accuracy: 0.5938\n",
      "Epoch 485/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 27.1242 - accuracy: 0.6238 - val_loss: 30.1974 - val_accuracy: 0.4875\n",
      "Epoch 486/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 26.6214 - accuracy: 0.6270 - val_loss: 26.6104 - val_accuracy: 0.6125\n",
      "Epoch 487/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 25.8360 - accuracy: 0.6034 - val_loss: 16.0741 - val_accuracy: 0.6125\n",
      "Epoch 488/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 25.1954 - accuracy: 0.6567 - val_loss: 25.7243 - val_accuracy: 0.5312\n",
      "Epoch 489/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 26.7057 - accuracy: 0.6426 - val_loss: 40.8867 - val_accuracy: 0.6000\n",
      "Epoch 490/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 27.6492 - accuracy: 0.6364 - val_loss: 29.4975 - val_accuracy: 0.5125\n",
      "Epoch 491/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 26.2758 - accuracy: 0.6395 - val_loss: 15.1928 - val_accuracy: 0.6187\n",
      "Epoch 492/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 24.0208 - accuracy: 0.6426 - val_loss: 28.1438 - val_accuracy: 0.5688\n",
      "Epoch 493/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 26.1496 - accuracy: 0.6176 - val_loss: 37.8770 - val_accuracy: 0.5813\n",
      "Epoch 494/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 26.6132 - accuracy: 0.6003 - val_loss: 34.0933 - val_accuracy: 0.5938\n",
      "Epoch 495/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 26.7877 - accuracy: 0.6646 - val_loss: 14.8291 - val_accuracy: 0.6000\n",
      "Epoch 496/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 25.5656 - accuracy: 0.6301 - val_loss: 45.0033 - val_accuracy: 0.6062\n",
      "Epoch 497/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 25.2288 - accuracy: 0.6364 - val_loss: 22.4792 - val_accuracy: 0.5625\n",
      "Epoch 498/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 26.7362 - accuracy: 0.6317 - val_loss: 27.2692 - val_accuracy: 0.5312\n",
      "Epoch 499/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 26.6575 - accuracy: 0.6379 - val_loss: 26.8603 - val_accuracy: 0.5938\n",
      "Epoch 500/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 25.8813 - accuracy: 0.6411 - val_loss: 24.6447 - val_accuracy: 0.6187\n",
      "Epoch 501/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 24.3426 - accuracy: 0.6520 - val_loss: 25.9571 - val_accuracy: 0.7437\n",
      "Epoch 502/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 26.0621 - accuracy: 0.6536 - val_loss: 34.5938 - val_accuracy: 0.5750\n",
      "Epoch 503/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 24.8327 - accuracy: 0.6834 - val_loss: 37.1306 - val_accuracy: 0.6187\n",
      "Epoch 504/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 26.0245 - accuracy: 0.6442 - val_loss: 19.4555 - val_accuracy: 0.5688\n",
      "Epoch 505/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 23.2415 - accuracy: 0.6536 - val_loss: 30.0244 - val_accuracy: 0.6000\n",
      "Epoch 506/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 22.8122 - accuracy: 0.6379 - val_loss: 19.6615 - val_accuracy: 0.5375\n",
      "Epoch 507/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 25.0503 - accuracy: 0.6301 - val_loss: 26.8091 - val_accuracy: 0.5938\n",
      "Epoch 508/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 26.4972 - accuracy: 0.6755 - val_loss: 35.6133 - val_accuracy: 0.5688\n",
      "Epoch 509/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 26.4378 - accuracy: 0.6442 - val_loss: 39.2170 - val_accuracy: 0.5750\n",
      "Epoch 510/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 26.8148 - accuracy: 0.6301 - val_loss: 27.9155 - val_accuracy: 0.6062\n",
      "Epoch 511/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 24.4298 - accuracy: 0.6285 - val_loss: 26.5768 - val_accuracy: 0.6125\n",
      "Epoch 512/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 24.0890 - accuracy: 0.6003 - val_loss: 34.8833 - val_accuracy: 0.5813\n",
      "Epoch 513/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 23.5118 - accuracy: 0.6082 - val_loss: 21.4709 - val_accuracy: 0.5500\n",
      "Epoch 514/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 26.4152 - accuracy: 0.6708 - val_loss: 26.4778 - val_accuracy: 0.5500\n",
      "Epoch 515/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 26.4155 - accuracy: 0.6332 - val_loss: 25.1555 - val_accuracy: 0.5938\n",
      "Epoch 516/1000\n",
      "638/638 [==============================] - 0s 30us/step - loss: 25.5555 - accuracy: 0.6552 - val_loss: 31.4928 - val_accuracy: 0.5500\n",
      "Epoch 517/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 25.4619 - accuracy: 0.6364 - val_loss: 31.7654 - val_accuracy: 0.5312\n",
      "Epoch 518/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 25.4497 - accuracy: 0.5705 - val_loss: 28.6691 - val_accuracy: 0.5750\n",
      "Epoch 519/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 25.9316 - accuracy: 0.6614 - val_loss: 21.5122 - val_accuracy: 0.5875\n",
      "Epoch 520/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 26.1449 - accuracy: 0.6520 - val_loss: 12.8283 - val_accuracy: 0.5875\n",
      "Epoch 521/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 22.9254 - accuracy: 0.6505 - val_loss: 21.4557 - val_accuracy: 0.5688\n",
      "Epoch 522/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 25.8130 - accuracy: 0.6317 - val_loss: 35.5188 - val_accuracy: 0.6000\n",
      "Epoch 523/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 26.0237 - accuracy: 0.6865 - val_loss: 15.4689 - val_accuracy: 0.5875\n",
      "Epoch 524/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 24.9303 - accuracy: 0.6505 - val_loss: 19.9284 - val_accuracy: 0.5813\n",
      "Epoch 525/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 26.0203 - accuracy: 0.6646 - val_loss: 12.3104 - val_accuracy: 0.5875\n",
      "Epoch 526/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 23.9051 - accuracy: 0.6787 - val_loss: 32.7519 - val_accuracy: 0.5500\n",
      "Epoch 527/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 24.1748 - accuracy: 0.6693 - val_loss: 42.6166 - val_accuracy: 0.5750\n",
      "Epoch 528/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 25.6596 - accuracy: 0.6567 - val_loss: 32.9251 - val_accuracy: 0.6187\n",
      "Epoch 529/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 25.1879 - accuracy: 0.6850 - val_loss: 29.9657 - val_accuracy: 0.5938\n",
      "Epoch 530/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 24.8490 - accuracy: 0.6567 - val_loss: 9.8687 - val_accuracy: 0.5688\n",
      "Epoch 531/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 23.5948 - accuracy: 0.6332 - val_loss: 31.7776 - val_accuracy: 0.5375\n",
      "Epoch 532/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 24.1520 - accuracy: 0.6489 - val_loss: 39.2090 - val_accuracy: 0.6000\n",
      "Epoch 533/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 42.7618 - accuracy: 0.781 - 0s 41us/step - loss: 26.3653 - accuracy: 0.6536 - val_loss: 31.1738 - val_accuracy: 0.5312\n",
      "Epoch 534/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 23.8562 - accuracy: 0.6364 - val_loss: 27.4277 - val_accuracy: 0.5938\n",
      "Epoch 535/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 25.4051 - accuracy: 0.6458 - val_loss: 33.6239 - val_accuracy: 0.5813\n",
      "Epoch 536/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 24.6517 - accuracy: 0.6270 - val_loss: 13.1402 - val_accuracy: 0.5875\n",
      "Epoch 537/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 24.3795 - accuracy: 0.6317 - val_loss: 21.8108 - val_accuracy: 0.5938\n",
      "Epoch 538/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 24.2586 - accuracy: 0.6254 - val_loss: 33.6236 - val_accuracy: 0.6062\n",
      "Epoch 539/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 24.7406 - accuracy: 0.6254 - val_loss: 19.4313 - val_accuracy: 0.6187\n",
      "Epoch 540/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 23.5158 - accuracy: 0.6238 - val_loss: 19.5868 - val_accuracy: 0.5688\n",
      "Epoch 541/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 24.7246 - accuracy: 0.6411 - val_loss: 26.1458 - val_accuracy: 0.6000\n",
      "Epoch 542/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 23.3890 - accuracy: 0.6724 - val_loss: 21.4213 - val_accuracy: 0.5875\n",
      "Epoch 543/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 23.7135 - accuracy: 0.6458 - val_loss: 30.6785 - val_accuracy: 0.6187\n",
      "Epoch 544/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 25.2750 - accuracy: 0.6442 - val_loss: 19.4951 - val_accuracy: 0.5875\n",
      "Epoch 545/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 22.7670 - accuracy: 0.6129 - val_loss: 39.2457 - val_accuracy: 0.5875\n",
      "Epoch 546/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 24.7140 - accuracy: 0.6693 - val_loss: 35.4293 - val_accuracy: 0.5688\n",
      "Epoch 547/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 24.8880 - accuracy: 0.6458 - val_loss: 18.8178 - val_accuracy: 0.5562\n",
      "Epoch 548/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 23.8458 - accuracy: 0.6442 - val_loss: 33.7100 - val_accuracy: 0.5750\n",
      "Epoch 549/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 24.1436 - accuracy: 0.6646 - val_loss: 36.6774 - val_accuracy: 0.5875\n",
      "Epoch 550/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 23.6853 - accuracy: 0.6583 - val_loss: 29.1023 - val_accuracy: 0.5813\n",
      "Epoch 551/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 24.4896 - accuracy: 0.6505 - val_loss: 34.8057 - val_accuracy: 0.6375\n",
      "Epoch 552/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 23.2270 - accuracy: 0.6850 - val_loss: 23.4352 - val_accuracy: 0.5875\n",
      "Epoch 553/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 24.9163 - accuracy: 0.6661 - val_loss: 26.7677 - val_accuracy: 0.6125\n",
      "Epoch 554/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 24.8272 - accuracy: 0.6834 - val_loss: 20.9366 - val_accuracy: 0.6125\n",
      "Epoch 555/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 23.6658 - accuracy: 0.6661 - val_loss: 27.7022 - val_accuracy: 0.6125\n",
      "Epoch 556/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 22.2121 - accuracy: 0.6897 - val_loss: 26.3794 - val_accuracy: 0.5750\n",
      "Epoch 557/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 24.1289 - accuracy: 0.6818 - val_loss: 34.7452 - val_accuracy: 0.5562\n",
      "Epoch 558/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 23.5175 - accuracy: 0.6458 - val_loss: 13.1406 - val_accuracy: 0.4875\n",
      "Epoch 559/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 23.9991 - accuracy: 0.6426 - val_loss: 31.4864 - val_accuracy: 0.5938\n",
      "Epoch 560/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 24.2224 - accuracy: 0.6912 - val_loss: 19.3254 - val_accuracy: 0.5813\n",
      "Epoch 561/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 24.8590 - accuracy: 0.6771 - val_loss: 26.2938 - val_accuracy: 0.5813\n",
      "Epoch 562/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 24.0508 - accuracy: 0.6646 - val_loss: 34.9104 - val_accuracy: 0.5688\n",
      "Epoch 563/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 23.6151 - accuracy: 0.6771 - val_loss: 12.8190 - val_accuracy: 0.6562\n",
      "Epoch 564/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 23.2040 - accuracy: 0.6442 - val_loss: 36.3673 - val_accuracy: 0.6000\n",
      "Epoch 565/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 24.0519 - accuracy: 0.6458 - val_loss: 34.7108 - val_accuracy: 0.5750\n",
      "Epoch 566/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 23.1379 - accuracy: 0.6520 - val_loss: 19.7050 - val_accuracy: 0.5938\n",
      "Epoch 567/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 21.9026 - accuracy: 0.6379 - val_loss: 32.9074 - val_accuracy: 0.5750\n",
      "Epoch 568/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 24.0995 - accuracy: 0.6771 - val_loss: 16.2928 - val_accuracy: 0.5562\n",
      "Epoch 569/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 22.2971 - accuracy: 0.6552 - val_loss: 34.6190 - val_accuracy: 0.6187\n",
      "Epoch 570/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 24.4911 - accuracy: 0.6787 - val_loss: 30.2706 - val_accuracy: 0.6187\n",
      "Epoch 571/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 24.9911 - accuracy: 0.6865 - val_loss: 27.2016 - val_accuracy: 0.6125\n",
      "Epoch 572/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 24.6698 - accuracy: 0.6803 - val_loss: 22.6222 - val_accuracy: 0.6500\n",
      "Epoch 573/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 23.6458 - accuracy: 0.6771 - val_loss: 29.1683 - val_accuracy: 0.5500\n",
      "Epoch 574/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 23.9172 - accuracy: 0.6803 - val_loss: 24.4650 - val_accuracy: 0.6375\n",
      "Epoch 575/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 23.6772 - accuracy: 0.6505 - val_loss: 11.7031 - val_accuracy: 0.6562\n",
      "Epoch 576/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 19.4835 - accuracy: 0.6520 - val_loss: 19.5275 - val_accuracy: 0.5437\n",
      "Epoch 577/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 23.4916 - accuracy: 0.6317 - val_loss: 21.4900 - val_accuracy: 0.5813\n",
      "Epoch 578/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 22.9950 - accuracy: 0.6505 - val_loss: 28.1168 - val_accuracy: 0.5562\n",
      "Epoch 579/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 23.5078 - accuracy: 0.6818 - val_loss: 20.7951 - val_accuracy: 0.6000\n",
      "Epoch 580/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 24.1447 - accuracy: 0.6755 - val_loss: 13.0807 - val_accuracy: 0.5688\n",
      "Epoch 581/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 23.3012 - accuracy: 0.6755 - val_loss: 27.5499 - val_accuracy: 0.5813\n",
      "Epoch 582/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 23.3740 - accuracy: 0.6536 - val_loss: 25.1146 - val_accuracy: 0.5688\n",
      "Epoch 583/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 23.6507 - accuracy: 0.6881 - val_loss: 19.3704 - val_accuracy: 0.5750\n",
      "Epoch 584/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 20.8006 - accuracy: 0.468 - 0s 41us/step - loss: 23.9010 - accuracy: 0.6850 - val_loss: 13.3543 - val_accuracy: 0.6375\n",
      "Epoch 585/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 22.6143 - accuracy: 0.6787 - val_loss: 26.3765 - val_accuracy: 0.6313\n",
      "Epoch 586/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 22.1545 - accuracy: 0.6411 - val_loss: 37.5204 - val_accuracy: 0.6750\n",
      "Epoch 587/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 23.8238 - accuracy: 0.6411 - val_loss: 35.1256 - val_accuracy: 0.6125\n",
      "Epoch 588/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 24.4444 - accuracy: 0.6991 - val_loss: 18.5499 - val_accuracy: 0.5875\n",
      "Epoch 589/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 21.1027 - accuracy: 0.6458 - val_loss: 31.5650 - val_accuracy: 0.6562\n",
      "Epoch 590/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 24.0939 - accuracy: 0.6818 - val_loss: 25.8887 - val_accuracy: 0.5688\n",
      "Epoch 591/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 22.3373 - accuracy: 0.6708 - val_loss: 23.3628 - val_accuracy: 0.6125\n",
      "Epoch 592/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 22.0286 - accuracy: 0.6583 - val_loss: 27.6417 - val_accuracy: 0.5750\n",
      "Epoch 593/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 23.2016 - accuracy: 0.6661 - val_loss: 30.2782 - val_accuracy: 0.6187\n",
      "Epoch 594/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 29.6395 - accuracy: 0.781 - 0s 31us/step - loss: 21.9658 - accuracy: 0.6975 - val_loss: 21.9204 - val_accuracy: 0.6500\n",
      "Epoch 595/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 21.9832 - accuracy: 0.7022 - val_loss: 15.7089 - val_accuracy: 0.6062\n",
      "Epoch 596/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 22.1166 - accuracy: 0.6599 - val_loss: 19.2320 - val_accuracy: 0.5938\n",
      "Epoch 597/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 22.0500 - accuracy: 0.6207 - val_loss: 32.3783 - val_accuracy: 0.5938\n",
      "Epoch 598/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 23.2049 - accuracy: 0.6677 - val_loss: 15.9202 - val_accuracy: 0.5188\n",
      "Epoch 599/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 22.2835 - accuracy: 0.6332 - val_loss: 31.7566 - val_accuracy: 0.6125\n",
      "Epoch 600/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 22.2932 - accuracy: 0.7022 - val_loss: 24.1375 - val_accuracy: 0.5813\n",
      "Epoch 601/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 23.0514 - accuracy: 0.6599 - val_loss: 32.5115 - val_accuracy: 0.6187\n",
      "Epoch 602/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 23.0012 - accuracy: 0.6755 - val_loss: 12.7497 - val_accuracy: 0.6000\n",
      "Epoch 603/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 22.2129 - accuracy: 0.6708 - val_loss: 24.1703 - val_accuracy: 0.6313\n",
      "Epoch 604/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 23.3039 - accuracy: 0.7053 - val_loss: 15.9088 - val_accuracy: 0.6187\n",
      "Epoch 605/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 21.4690 - accuracy: 0.6583 - val_loss: 26.4096 - val_accuracy: 0.6187\n",
      "Epoch 606/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 22.9570 - accuracy: 0.6865 - val_loss: 22.5950 - val_accuracy: 0.5125\n",
      "Epoch 607/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 23.0357 - accuracy: 0.6677 - val_loss: 22.6758 - val_accuracy: 0.5813\n",
      "Epoch 608/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 22.9880 - accuracy: 0.6755 - val_loss: 21.0950 - val_accuracy: 0.5938\n",
      "Epoch 609/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 20.6565 - accuracy: 0.6599 - val_loss: 26.6532 - val_accuracy: 0.5688\n",
      "Epoch 610/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 22.8253 - accuracy: 0.6364 - val_loss: 33.8178 - val_accuracy: 0.5938\n",
      "Epoch 611/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 21.6661 - accuracy: 0.6646 - val_loss: 27.1331 - val_accuracy: 0.6062\n",
      "Epoch 612/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 23.0101 - accuracy: 0.6473 - val_loss: 23.5661 - val_accuracy: 0.5938\n",
      "Epoch 613/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 22.5355 - accuracy: 0.6865 - val_loss: 32.1791 - val_accuracy: 0.5125\n",
      "Epoch 614/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 22.9739 - accuracy: 0.6708 - val_loss: 13.9616 - val_accuracy: 0.5688\n",
      "Epoch 615/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 22.6496 - accuracy: 0.6583 - val_loss: 26.9575 - val_accuracy: 0.6250\n",
      "Epoch 616/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 22.7779 - accuracy: 0.6724 - val_loss: 33.2903 - val_accuracy: 0.5938\n",
      "Epoch 617/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 22.3468 - accuracy: 0.6944 - val_loss: 30.6922 - val_accuracy: 0.6062\n",
      "Epoch 618/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 22.4046 - accuracy: 0.6850 - val_loss: 18.3252 - val_accuracy: 0.5688\n",
      "Epoch 619/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 22.1877 - accuracy: 0.6708 - val_loss: 25.2841 - val_accuracy: 0.5938\n",
      "Epoch 620/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 22.3470 - accuracy: 0.7006 - val_loss: 32.1121 - val_accuracy: 0.6062\n",
      "Epoch 621/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 22.9113 - accuracy: 0.6755 - val_loss: 9.3262 - val_accuracy: 0.5875\n",
      "Epoch 622/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 21.1774 - accuracy: 0.6740 - val_loss: 13.9456 - val_accuracy: 0.5813\n",
      "Epoch 623/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 20.9601 - accuracy: 0.7006 - val_loss: 16.1516 - val_accuracy: 0.5750\n",
      "Epoch 624/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 22.5221 - accuracy: 0.6254 - val_loss: 15.9674 - val_accuracy: 0.5938\n",
      "Epoch 625/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 20.5925 - accuracy: 0.6755 - val_loss: 22.6429 - val_accuracy: 0.5562\n",
      "Epoch 626/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 22.3585 - accuracy: 0.6301 - val_loss: 25.9959 - val_accuracy: 0.5437\n",
      "Epoch 627/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 22.6458 - accuracy: 0.6850 - val_loss: 34.6203 - val_accuracy: 0.6375\n",
      "Epoch 628/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 21.0600 - accuracy: 0.7226 - val_loss: 28.4452 - val_accuracy: 0.5688\n",
      "Epoch 629/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 21.1229 - accuracy: 0.6442 - val_loss: 25.5553 - val_accuracy: 0.5938\n",
      "Epoch 630/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 21.9380 - accuracy: 0.875 - 0s 39us/step - loss: 22.5813 - accuracy: 0.7241 - val_loss: 29.5869 - val_accuracy: 0.6375\n",
      "Epoch 631/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 22.0577 - accuracy: 0.6599 - val_loss: 11.3229 - val_accuracy: 0.6438\n",
      "Epoch 632/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 19.9371 - accuracy: 0.6740 - val_loss: 20.1385 - val_accuracy: 0.5688\n",
      "Epoch 633/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 21.0634 - accuracy: 0.7022 - val_loss: 22.5874 - val_accuracy: 0.5875\n",
      "Epoch 634/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 21.6363 - accuracy: 0.6897 - val_loss: 13.4181 - val_accuracy: 0.6313\n",
      "Epoch 635/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 22.0513 - accuracy: 0.7241 - val_loss: 26.9860 - val_accuracy: 0.6062\n",
      "Epoch 636/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 22.9954 - accuracy: 0.6724 - val_loss: 18.9620 - val_accuracy: 0.6250\n",
      "Epoch 637/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 21.7875 - accuracy: 0.6897 - val_loss: 19.6493 - val_accuracy: 0.5250\n",
      "Epoch 638/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 22.6876 - accuracy: 0.7069 - val_loss: 21.7975 - val_accuracy: 0.6313\n",
      "Epoch 639/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 21.8508 - accuracy: 0.6865 - val_loss: 18.4651 - val_accuracy: 0.5625\n",
      "Epoch 640/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 21.3076 - accuracy: 0.6959 - val_loss: 21.8790 - val_accuracy: 0.6062\n",
      "Epoch 641/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 21.7057 - accuracy: 0.7038 - val_loss: 19.1612 - val_accuracy: 0.5188\n",
      "Epoch 642/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 21.3373 - accuracy: 0.6285 - val_loss: 35.5968 - val_accuracy: 0.5312\n",
      "Epoch 643/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 20.8028 - accuracy: 0.6395 - val_loss: 11.3473 - val_accuracy: 0.6000\n",
      "Epoch 644/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 20.9383 - accuracy: 0.6630 - val_loss: 17.6016 - val_accuracy: 0.5562\n",
      "Epoch 645/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 21.6759 - accuracy: 0.6661 - val_loss: 26.3549 - val_accuracy: 0.6500\n",
      "Epoch 646/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 21.3409 - accuracy: 0.7038 - val_loss: 10.1932 - val_accuracy: 0.5125\n",
      "Epoch 647/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.8221 - accuracy: 0.6270 - val_loss: 33.0489 - val_accuracy: 0.5312\n",
      "Epoch 648/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 22.1290 - accuracy: 0.6442 - val_loss: 18.3533 - val_accuracy: 0.5938\n",
      "Epoch 649/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 22.5452 - accuracy: 0.6928 - val_loss: 9.2308 - val_accuracy: 0.5688\n",
      "Epoch 650/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 20.3953 - accuracy: 0.7288 - val_loss: 22.0703 - val_accuracy: 0.6438\n",
      "Epoch 651/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 22.3174 - accuracy: 0.6677 - val_loss: 16.0423 - val_accuracy: 0.5938\n",
      "Epoch 652/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 20.5960 - accuracy: 0.6583 - val_loss: 28.8581 - val_accuracy: 0.6250\n",
      "Epoch 653/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 21.4972 - accuracy: 0.6442 - val_loss: 26.3574 - val_accuracy: 0.5750\n",
      "Epoch 654/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 20.7473 - accuracy: 0.6897 - val_loss: 19.2249 - val_accuracy: 0.5938\n",
      "Epoch 655/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 21.3786 - accuracy: 0.7069 - val_loss: 23.7824 - val_accuracy: 0.5312\n",
      "Epoch 656/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 21.5096 - accuracy: 0.6270 - val_loss: 24.1438 - val_accuracy: 0.5813\n",
      "Epoch 657/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 20.5683 - accuracy: 0.6505 - val_loss: 18.7456 - val_accuracy: 0.6000\n",
      "Epoch 658/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 20.6299 - accuracy: 0.6818 - val_loss: 21.4614 - val_accuracy: 0.6125\n",
      "Epoch 659/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 22.2269 - accuracy: 0.7100 - val_loss: 14.2323 - val_accuracy: 0.5938\n",
      "Epoch 660/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 21.9736 - accuracy: 0.6755 - val_loss: 27.8518 - val_accuracy: 0.5500\n",
      "Epoch 661/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 21.1591 - accuracy: 0.6520 - val_loss: 24.6304 - val_accuracy: 0.6000\n",
      "Epoch 662/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 21.9936 - accuracy: 0.6897 - val_loss: 10.8712 - val_accuracy: 0.6375\n",
      "Epoch 663/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 19.3831 - accuracy: 0.7241 - val_loss: 37.6464 - val_accuracy: 0.6562\n",
      "Epoch 664/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 19.6901 - accuracy: 0.6834 - val_loss: 19.8139 - val_accuracy: 0.4938\n",
      "Epoch 665/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 21.4105 - accuracy: 0.6552 - val_loss: 33.9168 - val_accuracy: 0.5625\n",
      "Epoch 666/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 21.3409 - accuracy: 0.7351 - val_loss: 9.6776 - val_accuracy: 0.6500\n",
      "Epoch 667/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 9.8932 - accuracy: 0.59 - 0s 38us/step - loss: 19.8132 - accuracy: 0.7022 - val_loss: 18.9215 - val_accuracy: 0.6250\n",
      "Epoch 668/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 20.9640 - accuracy: 0.6834 - val_loss: 20.3028 - val_accuracy: 0.6062\n",
      "Epoch 669/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 20.0273 - accuracy: 0.7273 - val_loss: 35.6581 - val_accuracy: 0.6313\n",
      "Epoch 670/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 21.3068 - accuracy: 0.6912 - val_loss: 23.4716 - val_accuracy: 0.6313\n",
      "Epoch 671/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 22.3941 - accuracy: 0.6991 - val_loss: 13.0537 - val_accuracy: 0.6062\n",
      "Epoch 672/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 20.7824 - accuracy: 0.7053 - val_loss: 16.0183 - val_accuracy: 0.6000\n",
      "Epoch 673/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 20.7146 - accuracy: 0.7038 - val_loss: 11.3009 - val_accuracy: 0.6187\n",
      "Epoch 674/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 20.0230 - accuracy: 0.7006 - val_loss: 33.0965 - val_accuracy: 0.5688\n",
      "Epoch 675/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 22.0367 - accuracy: 0.6755 - val_loss: 19.3554 - val_accuracy: 0.6187\n",
      "Epoch 676/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 20.9325 - accuracy: 0.6411 - val_loss: 15.3432 - val_accuracy: 0.5250\n",
      "Epoch 677/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 20.4232 - accuracy: 0.6332 - val_loss: 24.2057 - val_accuracy: 0.6625\n",
      "Epoch 678/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 21.2132 - accuracy: 0.6740 - val_loss: 16.3069 - val_accuracy: 0.5500\n",
      "Epoch 679/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 21.1206 - accuracy: 0.6473 - val_loss: 20.6747 - val_accuracy: 0.6000\n",
      "Epoch 680/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 21.2404 - accuracy: 0.6693 - val_loss: 15.1236 - val_accuracy: 0.6625\n",
      "Epoch 681/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 19.9076 - accuracy: 0.7069 - val_loss: 27.9665 - val_accuracy: 0.5813\n",
      "Epoch 682/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 20.7418 - accuracy: 0.6630 - val_loss: 19.0387 - val_accuracy: 0.5875\n",
      "Epoch 683/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 21.5861 - accuracy: 0.6803 - val_loss: 12.1160 - val_accuracy: 0.5625\n",
      "Epoch 684/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 19.7900 - accuracy: 0.7038 - val_loss: 28.0270 - val_accuracy: 0.5875\n",
      "Epoch 685/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 20.1516 - accuracy: 0.6583 - val_loss: 24.4589 - val_accuracy: 0.5813\n",
      "Epoch 686/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 21.3864 - accuracy: 0.7100 - val_loss: 28.8758 - val_accuracy: 0.6375\n",
      "Epoch 687/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 21.4800 - accuracy: 0.6520 - val_loss: 9.1752 - val_accuracy: 0.5688\n",
      "Epoch 688/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 21.0509 - accuracy: 0.6724 - val_loss: 15.0241 - val_accuracy: 0.6062\n",
      "Epoch 689/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 23.3064 - accuracy: 0.6379 - val_loss: 26.0037 - val_accuracy: 0.6625\n",
      "Epoch 690/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 20.3672 - accuracy: 0.6975 - val_loss: 30.8896 - val_accuracy: 0.6187\n",
      "Epoch 691/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 20.4750 - accuracy: 0.6881 - val_loss: 25.4923 - val_accuracy: 0.6125\n",
      "Epoch 692/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 21.0389 - accuracy: 0.6850 - val_loss: 22.7093 - val_accuracy: 0.5437\n",
      "Epoch 693/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 21.4361 - accuracy: 0.6473 - val_loss: 19.0958 - val_accuracy: 0.5312\n",
      "Epoch 694/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 20.5089 - accuracy: 0.6693 - val_loss: 15.7402 - val_accuracy: 0.5750\n",
      "Epoch 695/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 20.8264 - accuracy: 0.6944 - val_loss: 16.0683 - val_accuracy: 0.5500\n",
      "Epoch 696/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 20.6393 - accuracy: 0.7179 - val_loss: 26.6942 - val_accuracy: 0.6125\n",
      "Epoch 697/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 21.5461 - accuracy: 0.6301 - val_loss: 21.4525 - val_accuracy: 0.5875\n",
      "Epoch 698/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 21.0753 - accuracy: 0.7053 - val_loss: 12.3716 - val_accuracy: 0.6187\n",
      "Epoch 699/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 20.5086 - accuracy: 0.7273 - val_loss: 26.3911 - val_accuracy: 0.5750\n",
      "Epoch 700/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 20.3975 - accuracy: 0.7226 - val_loss: 28.3193 - val_accuracy: 0.6125\n",
      "Epoch 701/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 19.5032 - accuracy: 0.6944 - val_loss: 25.9115 - val_accuracy: 0.5750\n",
      "Epoch 702/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 19.7534 - accuracy: 0.7053 - val_loss: 15.6596 - val_accuracy: 0.5875\n",
      "Epoch 703/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 20.0453 - accuracy: 0.7335 - val_loss: 11.6562 - val_accuracy: 0.5875\n",
      "Epoch 704/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 12.3177 - accuracy: 0.656 - 0s 45us/step - loss: 18.7416 - accuracy: 0.7304 - val_loss: 26.6389 - val_accuracy: 0.5437\n",
      "Epoch 705/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 19.9890 - accuracy: 0.6552 - val_loss: 23.9369 - val_accuracy: 0.5938\n",
      "Epoch 706/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 20.0266 - accuracy: 0.6897 - val_loss: 9.3999 - val_accuracy: 0.5938\n",
      "Epoch 707/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 19.5355 - accuracy: 0.6740 - val_loss: 23.1090 - val_accuracy: 0.5750\n",
      "Epoch 708/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 21.2127 - accuracy: 0.6755 - val_loss: 13.2375 - val_accuracy: 0.6500\n",
      "Epoch 709/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 20.7737 - accuracy: 0.7398 - val_loss: 25.5516 - val_accuracy: 0.6000\n",
      "Epoch 710/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 20.6879 - accuracy: 0.6818 - val_loss: 14.5278 - val_accuracy: 0.6313\n",
      "Epoch 711/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 20.3014 - accuracy: 0.6928 - val_loss: 15.2944 - val_accuracy: 0.5188\n",
      "Epoch 712/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 19.4035 - accuracy: 0.6991 - val_loss: 24.1701 - val_accuracy: 0.5813\n",
      "Epoch 713/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 19.6954 - accuracy: 0.6991 - val_loss: 21.0116 - val_accuracy: 0.5938\n",
      "Epoch 714/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 20.8253 - accuracy: 0.6803 - val_loss: 12.6403 - val_accuracy: 0.6125\n",
      "Epoch 715/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 18.9508 - accuracy: 0.7273 - val_loss: 13.8544 - val_accuracy: 0.6875\n",
      "Epoch 716/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 14.4727 - accuracy: 0.718 - 0s 39us/step - loss: 19.4430 - accuracy: 0.7038 - val_loss: 14.8722 - val_accuracy: 0.5625\n",
      "Epoch 717/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 19.9494 - accuracy: 0.6536 - val_loss: 13.5778 - val_accuracy: 0.5750\n",
      "Epoch 718/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 19.5579 - accuracy: 0.6944 - val_loss: 26.5663 - val_accuracy: 0.6062\n",
      "Epoch 719/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 20.1066 - accuracy: 0.6834 - val_loss: 23.2758 - val_accuracy: 0.5312\n",
      "Epoch 720/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 19.9657 - accuracy: 0.6865 - val_loss: 20.2991 - val_accuracy: 0.5938\n",
      "Epoch 721/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 20.5534 - accuracy: 0.6803 - val_loss: 20.9692 - val_accuracy: 0.6562\n",
      "Epoch 722/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 19.7758 - accuracy: 0.6959 - val_loss: 23.5130 - val_accuracy: 0.5562\n",
      "Epoch 723/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 19.4944 - accuracy: 0.6803 - val_loss: 20.2307 - val_accuracy: 0.6187\n",
      "Epoch 724/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 19.9258 - accuracy: 0.6661 - val_loss: 36.5430 - val_accuracy: 0.6313\n",
      "Epoch 725/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 20.2874 - accuracy: 0.6803 - val_loss: 23.9787 - val_accuracy: 0.6500\n",
      "Epoch 726/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 25.3609 - accuracy: 0.687 - 0s 41us/step - loss: 20.7522 - accuracy: 0.6520 - val_loss: 14.6510 - val_accuracy: 0.6250\n",
      "Epoch 727/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 18.6707 - accuracy: 0.6881 - val_loss: 31.1182 - val_accuracy: 0.6187\n",
      "Epoch 728/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 19.4466 - accuracy: 0.6411 - val_loss: 17.4083 - val_accuracy: 0.6125\n",
      "Epoch 729/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 20.0055 - accuracy: 0.6850 - val_loss: 14.0579 - val_accuracy: 0.6313\n",
      "Epoch 730/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 20.1110 - accuracy: 0.6865 - val_loss: 28.5023 - val_accuracy: 0.6125\n",
      "Epoch 731/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 19.5540 - accuracy: 0.6536 - val_loss: 27.6204 - val_accuracy: 0.5875\n",
      "Epoch 732/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.5453 - accuracy: 0.7100 - val_loss: 16.2110 - val_accuracy: 0.6375\n",
      "Epoch 733/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 20.5837 - accuracy: 0.6693 - val_loss: 15.6919 - val_accuracy: 0.6875\n",
      "Epoch 734/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 19.8953 - accuracy: 0.6708 - val_loss: 27.3425 - val_accuracy: 0.6187\n",
      "Epoch 735/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 20.2545 - accuracy: 0.6019 - val_loss: 18.9032 - val_accuracy: 0.5312\n",
      "Epoch 736/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 19.9133 - accuracy: 0.7273 - val_loss: 27.1333 - val_accuracy: 0.5312\n",
      "Epoch 737/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 19.3535 - accuracy: 0.6724 - val_loss: 25.2858 - val_accuracy: 0.5500\n",
      "Epoch 738/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 20.4473 - accuracy: 0.6991 - val_loss: 17.0195 - val_accuracy: 0.6562\n",
      "Epoch 739/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 19.8837 - accuracy: 0.7335 - val_loss: 15.7595 - val_accuracy: 0.6125\n",
      "Epoch 740/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 20.8968 - accuracy: 0.7147 - val_loss: 19.9754 - val_accuracy: 0.5938\n",
      "Epoch 741/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 19.0417 - accuracy: 0.6834 - val_loss: 19.0256 - val_accuracy: 0.6187\n",
      "Epoch 742/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 18.9447 - accuracy: 0.6270 - val_loss: 31.2511 - val_accuracy: 0.5750\n",
      "Epoch 743/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 19.9278 - accuracy: 0.7194 - val_loss: 23.2940 - val_accuracy: 0.5188\n",
      "Epoch 744/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 19.5381 - accuracy: 0.6755 - val_loss: 10.9871 - val_accuracy: 0.5375\n",
      "Epoch 745/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 18.1441 - accuracy: 0.6567 - val_loss: 28.2267 - val_accuracy: 0.6500\n",
      "Epoch 746/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 19.9366 - accuracy: 0.7038 - val_loss: 29.1587 - val_accuracy: 0.5813\n",
      "Epoch 747/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 19.1437 - accuracy: 0.6959 - val_loss: 24.1416 - val_accuracy: 0.5938\n",
      "Epoch 748/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 19.5042 - accuracy: 0.7147 - val_loss: 25.9342 - val_accuracy: 0.6250\n",
      "Epoch 749/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 19.5373 - accuracy: 0.6834 - val_loss: 15.7285 - val_accuracy: 0.6062\n",
      "Epoch 750/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.1233 - accuracy: 0.6975 - val_loss: 24.9054 - val_accuracy: 0.6062\n",
      "Epoch 751/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 19.0279 - accuracy: 0.6693 - val_loss: 24.1771 - val_accuracy: 0.5750\n",
      "Epoch 752/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 19.2442 - accuracy: 0.6411 - val_loss: 22.2638 - val_accuracy: 0.6000\n",
      "Epoch 753/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 20.1951 - accuracy: 0.7085 - val_loss: 11.4076 - val_accuracy: 0.6000\n",
      "Epoch 754/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 19.8442 - accuracy: 0.6411 - val_loss: 19.7907 - val_accuracy: 0.5875\n",
      "Epoch 755/1000\n",
      "638/638 [==============================] - 0s 49us/step - loss: 19.2583 - accuracy: 0.6803 - val_loss: 12.4888 - val_accuracy: 0.5125\n",
      "Epoch 756/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 19.5402 - accuracy: 0.6395 - val_loss: 23.7865 - val_accuracy: 0.4812\n",
      "Epoch 757/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 19.0493 - accuracy: 0.6426 - val_loss: 29.6372 - val_accuracy: 0.5938\n",
      "Epoch 758/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 18.9945 - accuracy: 0.6740 - val_loss: 16.8680 - val_accuracy: 0.6250\n",
      "Epoch 759/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 19.5731 - accuracy: 0.7006 - val_loss: 13.8569 - val_accuracy: 0.5813\n",
      "Epoch 760/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 19.3348 - accuracy: 0.6646 - val_loss: 10.4220 - val_accuracy: 0.6187\n",
      "Epoch 761/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 19.4595 - accuracy: 0.6489 - val_loss: 21.7234 - val_accuracy: 0.6000\n",
      "Epoch 762/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 18.6893 - accuracy: 0.6552 - val_loss: 14.5040 - val_accuracy: 0.5250\n",
      "Epoch 763/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 18.6738 - accuracy: 0.6630 - val_loss: 31.5274 - val_accuracy: 0.5813\n",
      "Epoch 764/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 19.4085 - accuracy: 0.7116 - val_loss: 13.1596 - val_accuracy: 0.6187\n",
      "Epoch 765/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 19.5577 - accuracy: 0.6317 - val_loss: 31.2849 - val_accuracy: 0.5938\n",
      "Epoch 766/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 19.4918 - accuracy: 0.7241 - val_loss: 27.1121 - val_accuracy: 0.5750\n",
      "Epoch 767/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 19.1299 - accuracy: 0.6755 - val_loss: 26.6967 - val_accuracy: 0.4938\n",
      "Epoch 768/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 19.3771 - accuracy: 0.6614 - val_loss: 15.2076 - val_accuracy: 0.6187\n",
      "Epoch 769/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 17.3526 - accuracy: 0.6646 - val_loss: 12.7771 - val_accuracy: 0.6313\n",
      "Epoch 770/1000\n",
      "638/638 [==============================] - 0s 47us/step - loss: 19.3125 - accuracy: 0.6850 - val_loss: 25.6044 - val_accuracy: 0.6000\n",
      "Epoch 771/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 19.4308 - accuracy: 0.6755 - val_loss: 16.6433 - val_accuracy: 0.6062\n",
      "Epoch 772/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 19.8835 - accuracy: 0.6818 - val_loss: 12.9300 - val_accuracy: 0.5875\n",
      "Epoch 773/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 19.9205 - accuracy: 0.6803 - val_loss: 9.0160 - val_accuracy: 0.6375\n",
      "Epoch 774/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 17.2725 - accuracy: 0.7288 - val_loss: 19.4989 - val_accuracy: 0.6125\n",
      "Epoch 775/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 18.4011 - accuracy: 0.7257 - val_loss: 11.0119 - val_accuracy: 0.5938\n",
      "Epoch 776/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 18.3014 - accuracy: 0.7210 - val_loss: 9.6182 - val_accuracy: 0.5750\n",
      "Epoch 777/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 18.7919 - accuracy: 0.6677 - val_loss: 17.2329 - val_accuracy: 0.6375\n",
      "Epoch 778/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 17.8999 - accuracy: 0.7132 - val_loss: 24.5911 - val_accuracy: 0.6750\n",
      "Epoch 779/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 19.2521 - accuracy: 0.6614 - val_loss: 20.9213 - val_accuracy: 0.6000\n",
      "Epoch 780/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 19.0963 - accuracy: 0.6991 - val_loss: 32.0836 - val_accuracy: 0.6187\n",
      "Epoch 781/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 20.1327 - accuracy: 0.7179 - val_loss: 10.0233 - val_accuracy: 0.6375\n",
      "Epoch 782/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.6484 - accuracy: 0.6959 - val_loss: 23.1382 - val_accuracy: 0.6313\n",
      "Epoch 783/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 18.7386 - accuracy: 0.7022 - val_loss: 21.8342 - val_accuracy: 0.6125\n",
      "Epoch 784/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 20.0590 - accuracy: 0.6881 - val_loss: 20.1569 - val_accuracy: 0.6187\n",
      "Epoch 785/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 18.5021 - accuracy: 0.6834 - val_loss: 19.7819 - val_accuracy: 0.5813\n",
      "Epoch 786/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 19.0409 - accuracy: 0.7069 - val_loss: 18.7470 - val_accuracy: 0.5875\n",
      "Epoch 787/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 19.3198 - accuracy: 0.7053 - val_loss: 15.8382 - val_accuracy: 0.5875\n",
      "Epoch 788/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 19.3949 - accuracy: 0.7006 - val_loss: 25.0380 - val_accuracy: 0.6500\n",
      "Epoch 789/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 19.3604 - accuracy: 0.7053 - val_loss: 26.9282 - val_accuracy: 0.6438\n",
      "Epoch 790/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 19.2460 - accuracy: 0.6912 - val_loss: 13.8335 - val_accuracy: 0.5437\n",
      "Epoch 791/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 18.9842 - accuracy: 0.6834 - val_loss: 21.1876 - val_accuracy: 0.5813\n",
      "Epoch 792/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 19.1566 - accuracy: 0.7132 - val_loss: 25.7036 - val_accuracy: 0.6125\n",
      "Epoch 793/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 18.7188 - accuracy: 0.6395 - val_loss: 9.7001 - val_accuracy: 0.5000\n",
      "Epoch 794/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 18.4700 - accuracy: 0.6677 - val_loss: 16.3872 - val_accuracy: 0.6562\n",
      "Epoch 795/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 18.1462 - accuracy: 0.6834 - val_loss: 12.8879 - val_accuracy: 0.5875\n",
      "Epoch 796/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.1208 - accuracy: 0.6379 - val_loss: 11.7053 - val_accuracy: 0.6062\n",
      "Epoch 797/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 17.1774 - accuracy: 0.6348 - val_loss: 12.3999 - val_accuracy: 0.6000\n",
      "Epoch 798/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 18.8557 - accuracy: 0.6850 - val_loss: 32.8367 - val_accuracy: 0.6000\n",
      "Epoch 799/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 19.5192 - accuracy: 0.6646 - val_loss: 14.1371 - val_accuracy: 0.5875\n",
      "Epoch 800/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 13.1588 - accuracy: 0.781 - 0s 34us/step - loss: 18.7435 - accuracy: 0.7257 - val_loss: 19.8904 - val_accuracy: 0.6000\n",
      "Epoch 801/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.2984 - accuracy: 0.6520 - val_loss: 22.3673 - val_accuracy: 0.6750\n",
      "Epoch 802/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.8947 - accuracy: 0.7288 - val_loss: 28.9078 - val_accuracy: 0.6438\n",
      "Epoch 803/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 15.3020 - accuracy: 0.6881 - val_loss: 24.1524 - val_accuracy: 0.6000\n",
      "Epoch 804/1000\n",
      "638/638 [==============================] - 0s 28us/step - loss: 19.8168 - accuracy: 0.6897 - val_loss: 17.4488 - val_accuracy: 0.6625\n",
      "Epoch 805/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.9278 - accuracy: 0.7022 - val_loss: 15.7570 - val_accuracy: 0.5688\n",
      "Epoch 806/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.5153 - accuracy: 0.7100 - val_loss: 12.0523 - val_accuracy: 0.6000\n",
      "Epoch 807/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 10.9311 - accuracy: 0.531 - 0s 42us/step - loss: 19.2836 - accuracy: 0.6755 - val_loss: 11.2933 - val_accuracy: 0.6062\n",
      "Epoch 808/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.1185 - accuracy: 0.6944 - val_loss: 25.4075 - val_accuracy: 0.5688\n",
      "Epoch 809/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 19.3246 - accuracy: 0.6646 - val_loss: 18.1133 - val_accuracy: 0.6313\n",
      "Epoch 810/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 18.4492 - accuracy: 0.7288 - val_loss: 17.5441 - val_accuracy: 0.6000\n",
      "Epoch 811/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 19.2885 - accuracy: 0.7194 - val_loss: 14.5191 - val_accuracy: 0.6250\n",
      "Epoch 812/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.7471 - accuracy: 0.6881 - val_loss: 23.1530 - val_accuracy: 0.4625\n",
      "Epoch 813/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 20.0232 - accuracy: 0.6897 - val_loss: 9.3797 - val_accuracy: 0.5938\n",
      "Epoch 814/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.9801 - accuracy: 0.6599 - val_loss: 24.7420 - val_accuracy: 0.6250\n",
      "Epoch 815/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 18.8691 - accuracy: 0.6520 - val_loss: 18.9821 - val_accuracy: 0.6125\n",
      "Epoch 816/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.6527 - accuracy: 0.6991 - val_loss: 18.4570 - val_accuracy: 0.6562\n",
      "Epoch 817/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.1648 - accuracy: 0.6818 - val_loss: 19.1194 - val_accuracy: 0.7000\n",
      "Epoch 818/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.3639 - accuracy: 0.7226 - val_loss: 14.5792 - val_accuracy: 0.6187\n",
      "Epoch 819/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.9379 - accuracy: 0.7132 - val_loss: 27.7847 - val_accuracy: 0.5688\n",
      "Epoch 820/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.9122 - accuracy: 0.6865 - val_loss: 34.0361 - val_accuracy: 0.5562\n",
      "Epoch 821/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.6688 - accuracy: 0.6991 - val_loss: 16.3429 - val_accuracy: 0.6125\n",
      "Epoch 822/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 19.1376 - accuracy: 0.6959 - val_loss: 15.3679 - val_accuracy: 0.6187\n",
      "Epoch 823/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.6486 - accuracy: 0.6944 - val_loss: 17.1721 - val_accuracy: 0.6062\n",
      "Epoch 824/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.9713 - accuracy: 0.6740 - val_loss: 24.3544 - val_accuracy: 0.5750\n",
      "Epoch 825/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 18.6652 - accuracy: 0.7053 - val_loss: 12.2851 - val_accuracy: 0.6250\n",
      "Epoch 826/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.4085 - accuracy: 0.6614 - val_loss: 21.5233 - val_accuracy: 0.5813\n",
      "Epoch 827/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 19.0552 - accuracy: 0.6803 - val_loss: 17.5424 - val_accuracy: 0.5437\n",
      "Epoch 828/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 19.1209 - accuracy: 0.6411 - val_loss: 8.5949 - val_accuracy: 0.6313\n",
      "Epoch 829/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 16.3194 - accuracy: 0.6301 - val_loss: 24.4548 - val_accuracy: 0.5750\n",
      "Epoch 830/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.1360 - accuracy: 0.7132 - val_loss: 19.7228 - val_accuracy: 0.6750\n",
      "Epoch 831/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 19.1930 - accuracy: 0.6818 - val_loss: 13.4184 - val_accuracy: 0.5188\n",
      "Epoch 832/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.3638 - accuracy: 0.6740 - val_loss: 17.2472 - val_accuracy: 0.5500\n",
      "Epoch 833/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.5961 - accuracy: 0.6724 - val_loss: 30.3250 - val_accuracy: 0.6750\n",
      "Epoch 834/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 18.3846 - accuracy: 0.6944 - val_loss: 18.6716 - val_accuracy: 0.5938\n",
      "Epoch 835/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 17.7262 - accuracy: 0.7053 - val_loss: 12.6343 - val_accuracy: 0.6500\n",
      "Epoch 836/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.3460 - accuracy: 0.6881 - val_loss: 28.8479 - val_accuracy: 0.6313\n",
      "Epoch 837/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.1573 - accuracy: 0.6803 - val_loss: 27.5019 - val_accuracy: 0.6187\n",
      "Epoch 838/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 19.3866 - accuracy: 0.6944 - val_loss: 8.8331 - val_accuracy: 0.6375\n",
      "Epoch 839/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 18.1973 - accuracy: 0.6661 - val_loss: 18.1692 - val_accuracy: 0.6500\n",
      "Epoch 840/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.1751 - accuracy: 0.7492 - val_loss: 22.4988 - val_accuracy: 0.6500\n",
      "Epoch 841/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.1450 - accuracy: 0.7132 - val_loss: 19.0842 - val_accuracy: 0.6250\n",
      "Epoch 842/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.4206 - accuracy: 0.5972 - val_loss: 17.5312 - val_accuracy: 0.6187\n",
      "Epoch 843/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 17.9356 - accuracy: 0.7053 - val_loss: 23.5311 - val_accuracy: 0.6438\n",
      "Epoch 844/1000\n",
      "638/638 [==============================] - 0s 30us/step - loss: 18.4754 - accuracy: 0.6944 - val_loss: 13.1492 - val_accuracy: 0.6625\n",
      "Epoch 845/1000\n",
      "638/638 [==============================] - 0s 30us/step - loss: 18.7456 - accuracy: 0.7038 - val_loss: 19.5540 - val_accuracy: 0.5750\n",
      "Epoch 846/1000\n",
      "638/638 [==============================] - 0s 30us/step - loss: 17.4039 - accuracy: 0.6567 - val_loss: 11.3568 - val_accuracy: 0.6750\n",
      "Epoch 847/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 18.3915 - accuracy: 0.6661 - val_loss: 25.7576 - val_accuracy: 0.5562\n",
      "Epoch 848/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 18.5095 - accuracy: 0.6583 - val_loss: 9.0562 - val_accuracy: 0.5750\n",
      "Epoch 849/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 18.3128 - accuracy: 0.6865 - val_loss: 13.4491 - val_accuracy: 0.6250\n",
      "Epoch 850/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 16.6847 - accuracy: 0.6646 - val_loss: 23.0785 - val_accuracy: 0.5813\n",
      "Epoch 851/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.9116 - accuracy: 0.6536 - val_loss: 10.7663 - val_accuracy: 0.5875\n",
      "Epoch 852/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.3685 - accuracy: 0.6520 - val_loss: 20.2128 - val_accuracy: 0.6062\n",
      "Epoch 853/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 19.4249 - accuracy: 0.7100 - val_loss: 11.4714 - val_accuracy: 0.6562\n",
      "Epoch 854/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 17.6468 - accuracy: 0.7053 - val_loss: 19.6729 - val_accuracy: 0.6687\n",
      "Epoch 855/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.9254 - accuracy: 0.6818 - val_loss: 19.7412 - val_accuracy: 0.5875\n",
      "Epoch 856/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 17.0512 - accuracy: 0.6708 - val_loss: 28.3441 - val_accuracy: 0.6500\n",
      "Epoch 857/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.1273 - accuracy: 0.6771 - val_loss: 32.1641 - val_accuracy: 0.6000\n",
      "Epoch 858/1000\n",
      "638/638 [==============================] - 0s 37us/step - loss: 17.3444 - accuracy: 0.6818 - val_loss: 19.7211 - val_accuracy: 0.6250\n",
      "Epoch 859/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 18.5745 - accuracy: 0.6520 - val_loss: 24.7212 - val_accuracy: 0.5312\n",
      "Epoch 860/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.0685 - accuracy: 0.7053 - val_loss: 15.3774 - val_accuracy: 0.6313\n",
      "Epoch 861/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.6057 - accuracy: 0.6724 - val_loss: 27.7401 - val_accuracy: 0.4812\n",
      "Epoch 862/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 17.2416 - accuracy: 0.7210 - val_loss: 8.2047 - val_accuracy: 0.5938\n",
      "Epoch 863/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 16.7882 - accuracy: 0.6928 - val_loss: 19.9253 - val_accuracy: 0.6000\n",
      "Epoch 864/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.0013 - accuracy: 0.7006 - val_loss: 17.1163 - val_accuracy: 0.5750\n",
      "Epoch 865/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 18.4622 - accuracy: 0.6897 - val_loss: 17.2279 - val_accuracy: 0.5625\n",
      "Epoch 866/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 17.8716 - accuracy: 0.6536 - val_loss: 20.3196 - val_accuracy: 0.6062\n",
      "Epoch 867/1000\n",
      "638/638 [==============================] - 0s 45us/step - loss: 18.5731 - accuracy: 0.7210 - val_loss: 19.7233 - val_accuracy: 0.6750\n",
      "Epoch 868/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 16.1181 - accuracy: 0.6897 - val_loss: 24.4698 - val_accuracy: 0.6938\n",
      "Epoch 869/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 18.5458 - accuracy: 0.7508 - val_loss: 24.5123 - val_accuracy: 0.6000\n",
      "Epoch 870/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 18.3308 - accuracy: 0.7367 - val_loss: 26.4016 - val_accuracy: 0.6250\n",
      "Epoch 871/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 16.9826 - accuracy: 0.7116 - val_loss: 27.2910 - val_accuracy: 0.6625\n",
      "Epoch 872/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 17.7625 - accuracy: 0.7790 - val_loss: 16.0140 - val_accuracy: 0.6812\n",
      "Epoch 873/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.7376 - accuracy: 0.7445 - val_loss: 11.3096 - val_accuracy: 0.6938\n",
      "Epoch 874/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 17.2336 - accuracy: 0.7163 - val_loss: 23.3552 - val_accuracy: 0.6938\n",
      "Epoch 875/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 18.0430 - accuracy: 0.7022 - val_loss: 19.0394 - val_accuracy: 0.6438\n",
      "Epoch 876/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 18.6966 - accuracy: 0.7116 - val_loss: 18.2225 - val_accuracy: 0.6062\n",
      "Epoch 877/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 16.9650 - accuracy: 0.6912 - val_loss: 10.3404 - val_accuracy: 0.6313\n",
      "Epoch 878/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 18.1965 - accuracy: 0.6630 - val_loss: 9.6511 - val_accuracy: 0.5250\n",
      "Epoch 879/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.0993 - accuracy: 0.7085 - val_loss: 20.0035 - val_accuracy: 0.6938\n",
      "Epoch 880/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.2445 - accuracy: 0.7022 - val_loss: 16.9376 - val_accuracy: 0.5750\n",
      "Epoch 881/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.0961 - accuracy: 0.6944 - val_loss: 11.6735 - val_accuracy: 0.6062\n",
      "Epoch 882/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.3364 - accuracy: 0.6897 - val_loss: 10.4976 - val_accuracy: 0.6062\n",
      "Epoch 883/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 17.5385 - accuracy: 0.7085 - val_loss: 24.0230 - val_accuracy: 0.5938\n",
      "Epoch 884/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 18.8787 - accuracy: 0.6787 - val_loss: 20.0522 - val_accuracy: 0.5875\n",
      "Epoch 885/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.8993 - accuracy: 0.6614 - val_loss: 13.9778 - val_accuracy: 0.5750\n",
      "Epoch 886/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.2961 - accuracy: 0.6834 - val_loss: 27.3577 - val_accuracy: 0.6000\n",
      "Epoch 887/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 18.1275 - accuracy: 0.6708 - val_loss: 20.0622 - val_accuracy: 0.6313\n",
      "Epoch 888/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 16.7753 - accuracy: 0.6975 - val_loss: 18.7817 - val_accuracy: 0.6562\n",
      "Epoch 889/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.6242 - accuracy: 0.6755 - val_loss: 19.3813 - val_accuracy: 0.5500\n",
      "Epoch 890/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.5181 - accuracy: 0.6803 - val_loss: 16.8604 - val_accuracy: 0.5688\n",
      "Epoch 891/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.4366 - accuracy: 0.6411 - val_loss: 9.1483 - val_accuracy: 0.6438\n",
      "Epoch 892/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.8523 - accuracy: 0.7226 - val_loss: 22.0134 - val_accuracy: 0.6625\n",
      "Epoch 893/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 18.0382 - accuracy: 0.7163 - val_loss: 12.6112 - val_accuracy: 0.6062\n",
      "Epoch 894/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.8827 - accuracy: 0.7163 - val_loss: 29.5774 - val_accuracy: 0.6250\n",
      "Epoch 895/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 18.7085 - accuracy: 0.7100 - val_loss: 16.0925 - val_accuracy: 0.6500\n",
      "Epoch 896/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 16.8751 - accuracy: 0.7241 - val_loss: 15.6785 - val_accuracy: 0.6187\n",
      "Epoch 897/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 17.1107 - accuracy: 0.6991 - val_loss: 22.6597 - val_accuracy: 0.6000\n",
      "Epoch 898/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 18.0444 - accuracy: 0.7132 - val_loss: 29.4035 - val_accuracy: 0.6375\n",
      "Epoch 899/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.4799 - accuracy: 0.6975 - val_loss: 16.5412 - val_accuracy: 0.6000\n",
      "Epoch 900/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.3029 - accuracy: 0.6818 - val_loss: 18.3015 - val_accuracy: 0.5562\n",
      "Epoch 901/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 18.6030 - accuracy: 0.6646 - val_loss: 12.1272 - val_accuracy: 0.6250\n",
      "Epoch 902/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.4931 - accuracy: 0.6865 - val_loss: 16.7362 - val_accuracy: 0.6625\n",
      "Epoch 903/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.4654 - accuracy: 0.7116 - val_loss: 22.6711 - val_accuracy: 0.6562\n",
      "Epoch 904/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 18.3641 - accuracy: 0.7414 - val_loss: 17.5341 - val_accuracy: 0.6250\n",
      "Epoch 905/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.7068 - accuracy: 0.7351 - val_loss: 16.4234 - val_accuracy: 0.5250\n",
      "Epoch 906/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.2777 - accuracy: 0.7085 - val_loss: 9.3566 - val_accuracy: 0.6250\n",
      "Epoch 907/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 15.9383 - accuracy: 0.6897 - val_loss: 13.3530 - val_accuracy: 0.6062\n",
      "Epoch 908/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 18.1861 - accuracy: 0.6881 - val_loss: 19.8234 - val_accuracy: 0.6750\n",
      "Epoch 909/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.2839 - accuracy: 0.7085 - val_loss: 16.8049 - val_accuracy: 0.5437\n",
      "Epoch 910/1000\n",
      "638/638 [==============================] - 0s 30us/step - loss: 17.3217 - accuracy: 0.7179 - val_loss: 23.4438 - val_accuracy: 0.6375\n",
      "Epoch 911/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 17.6688 - accuracy: 0.7085 - val_loss: 11.6702 - val_accuracy: 0.5375\n",
      "Epoch 912/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.4119 - accuracy: 0.6646 - val_loss: 24.6551 - val_accuracy: 0.7000\n",
      "Epoch 913/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 17.5841 - accuracy: 0.7398 - val_loss: 19.2744 - val_accuracy: 0.5688\n",
      "Epoch 914/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.1967 - accuracy: 0.6834 - val_loss: 12.7797 - val_accuracy: 0.6250\n",
      "Epoch 915/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.3273 - accuracy: 0.7210 - val_loss: 27.3931 - val_accuracy: 0.6687\n",
      "Epoch 916/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 17.2365 - accuracy: 0.6944 - val_loss: 25.0948 - val_accuracy: 0.6812\n",
      "Epoch 917/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.1065 - accuracy: 0.7006 - val_loss: 15.2610 - val_accuracy: 0.6438\n",
      "Epoch 918/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 15.7643 - accuracy: 0.656 - 0s 38us/step - loss: 17.5826 - accuracy: 0.6771 - val_loss: 10.1532 - val_accuracy: 0.6625\n",
      "Epoch 919/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.4552 - accuracy: 0.6975 - val_loss: 22.1789 - val_accuracy: 0.6562\n",
      "Epoch 920/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.0101 - accuracy: 0.7069 - val_loss: 21.3043 - val_accuracy: 0.6500\n",
      "Epoch 921/1000\n",
      "638/638 [==============================] - 0s 42us/step - loss: 15.8222 - accuracy: 0.6693 - val_loss: 16.0573 - val_accuracy: 0.5813\n",
      "Epoch 922/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 16.2117 - accuracy: 0.7147 - val_loss: 19.5944 - val_accuracy: 0.6687\n",
      "Epoch 923/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.4110 - accuracy: 0.7069 - val_loss: 12.0801 - val_accuracy: 0.6125\n",
      "Epoch 924/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 15.4368 - accuracy: 0.6677 - val_loss: 11.5227 - val_accuracy: 0.5938\n",
      "Epoch 925/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 17.8023 - accuracy: 0.6834 - val_loss: 9.8394 - val_accuracy: 0.5938\n",
      "Epoch 926/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 17.5620 - accuracy: 0.7226 - val_loss: 22.9686 - val_accuracy: 0.6187\n",
      "Epoch 927/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.5299 - accuracy: 0.6803 - val_loss: 26.4975 - val_accuracy: 0.6687\n",
      "Epoch 928/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.2160 - accuracy: 0.7038 - val_loss: 20.4409 - val_accuracy: 0.6812\n",
      "Epoch 929/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.3206 - accuracy: 0.6458 - val_loss: 16.4906 - val_accuracy: 0.6250\n",
      "Epoch 930/1000\n",
      "638/638 [==============================] - 0s 44us/step - loss: 16.3834 - accuracy: 0.6881 - val_loss: 15.7389 - val_accuracy: 0.6687\n",
      "Epoch 931/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 17.4235 - accuracy: 0.6834 - val_loss: 23.7739 - val_accuracy: 0.6250\n",
      "Epoch 932/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 18.4706 - accuracy: 0.6630 - val_loss: 19.2980 - val_accuracy: 0.5437\n",
      "Epoch 933/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.1721 - accuracy: 0.7210 - val_loss: 21.7202 - val_accuracy: 0.5750\n",
      "Epoch 934/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 17.6567 - accuracy: 0.6959 - val_loss: 14.8354 - val_accuracy: 0.6562\n",
      "Epoch 935/1000\n",
      "638/638 [==============================] - 0s 41us/step - loss: 17.2807 - accuracy: 0.6661 - val_loss: 8.9463 - val_accuracy: 0.6313\n",
      "Epoch 936/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 16.5609 - accuracy: 0.7038 - val_loss: 23.5507 - val_accuracy: 0.5750\n",
      "Epoch 937/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.6171 - accuracy: 0.7132 - val_loss: 26.6262 - val_accuracy: 0.6750\n",
      "Epoch 938/1000\n",
      "638/638 [==============================] - ETA: 0s - loss: 23.5195 - accuracy: 0.812 - 0s 34us/step - loss: 16.8155 - accuracy: 0.7414 - val_loss: 14.2989 - val_accuracy: 0.6687\n",
      "Epoch 939/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.1865 - accuracy: 0.6473 - val_loss: 17.4936 - val_accuracy: 0.5188\n",
      "Epoch 940/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 17.2477 - accuracy: 0.6975 - val_loss: 18.9420 - val_accuracy: 0.6875\n",
      "Epoch 941/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 17.1166 - accuracy: 0.7006 - val_loss: 20.0940 - val_accuracy: 0.6375\n",
      "Epoch 942/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 18.1811 - accuracy: 0.6959 - val_loss: 14.9360 - val_accuracy: 0.6500\n",
      "Epoch 943/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.4813 - accuracy: 0.7053 - val_loss: 20.5326 - val_accuracy: 0.5063\n",
      "Epoch 944/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.4574 - accuracy: 0.6771 - val_loss: 11.6657 - val_accuracy: 0.6313\n",
      "Epoch 945/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.8901 - accuracy: 0.7163 - val_loss: 11.7272 - val_accuracy: 0.7000\n",
      "Epoch 946/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.6684 - accuracy: 0.6834 - val_loss: 12.5592 - val_accuracy: 0.6250\n",
      "Epoch 947/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 15.8092 - accuracy: 0.6693 - val_loss: 21.8345 - val_accuracy: 0.5625\n",
      "Epoch 948/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.9541 - accuracy: 0.7022 - val_loss: 25.4875 - val_accuracy: 0.6562\n",
      "Epoch 949/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 16.5321 - accuracy: 0.7116 - val_loss: 14.7560 - val_accuracy: 0.6562\n",
      "Epoch 950/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 15.6816 - accuracy: 0.6740 - val_loss: 16.5376 - val_accuracy: 0.6062\n",
      "Epoch 951/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.3078 - accuracy: 0.7179 - val_loss: 15.0604 - val_accuracy: 0.6313\n",
      "Epoch 952/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 16.4925 - accuracy: 0.6928 - val_loss: 21.9212 - val_accuracy: 0.6750\n",
      "Epoch 953/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.8603 - accuracy: 0.7508 - val_loss: 9.7905 - val_accuracy: 0.5875\n",
      "Epoch 954/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 15.9215 - accuracy: 0.6552 - val_loss: 17.3696 - val_accuracy: 0.6313\n",
      "Epoch 955/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.4101 - accuracy: 0.6724 - val_loss: 23.8685 - val_accuracy: 0.6438\n",
      "Epoch 956/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 17.3340 - accuracy: 0.6787 - val_loss: 21.1575 - val_accuracy: 0.6000\n",
      "Epoch 957/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 17.4696 - accuracy: 0.6646 - val_loss: 13.6350 - val_accuracy: 0.6187\n",
      "Epoch 958/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 16.2327 - accuracy: 0.6567 - val_loss: 14.8119 - val_accuracy: 0.7063\n",
      "Epoch 959/1000\n",
      "638/638 [==============================] - 0s 39us/step - loss: 15.4771 - accuracy: 0.6614 - val_loss: 19.4425 - val_accuracy: 0.6438\n",
      "Epoch 960/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 16.1375 - accuracy: 0.6928 - val_loss: 22.6643 - val_accuracy: 0.6438\n",
      "Epoch 961/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 17.4901 - accuracy: 0.6787 - val_loss: 19.4516 - val_accuracy: 0.5375\n",
      "Epoch 962/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.8390 - accuracy: 0.6803 - val_loss: 24.3271 - val_accuracy: 0.4938\n",
      "Epoch 963/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 14.7897 - accuracy: 0.6803 - val_loss: 17.1972 - val_accuracy: 0.5938\n",
      "Epoch 964/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.6861 - accuracy: 0.6959 - val_loss: 18.6725 - val_accuracy: 0.6000\n",
      "Epoch 965/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.4247 - accuracy: 0.6630 - val_loss: 13.9711 - val_accuracy: 0.6250\n",
      "Epoch 966/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 16.6122 - accuracy: 0.6677 - val_loss: 13.5371 - val_accuracy: 0.6375\n",
      "Epoch 967/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.7343 - accuracy: 0.7320 - val_loss: 54.2045 - val_accuracy: 0.2875\n",
      "Epoch 968/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.7859 - accuracy: 0.6050 - val_loss: 24.1492 - val_accuracy: 0.5875\n",
      "Epoch 969/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.7731 - accuracy: 0.6975 - val_loss: 19.0558 - val_accuracy: 0.5813\n",
      "Epoch 970/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 16.6632 - accuracy: 0.6473 - val_loss: 25.0063 - val_accuracy: 0.6750\n",
      "Epoch 971/1000\n",
      "638/638 [==============================] - 0s 31us/step - loss: 17.3921 - accuracy: 0.6630 - val_loss: 8.0853 - val_accuracy: 0.6125\n",
      "Epoch 972/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 16.1255 - accuracy: 0.7194 - val_loss: 20.6590 - val_accuracy: 0.6562\n",
      "Epoch 973/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.5642 - accuracy: 0.6426 - val_loss: 21.9035 - val_accuracy: 0.5500\n",
      "Epoch 974/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.2634 - accuracy: 0.6536 - val_loss: 17.6812 - val_accuracy: 0.6750\n",
      "Epoch 975/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.7709 - accuracy: 0.6583 - val_loss: 21.2951 - val_accuracy: 0.5437\n",
      "Epoch 976/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.3504 - accuracy: 0.6897 - val_loss: 16.7313 - val_accuracy: 0.5938\n",
      "Epoch 977/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.3819 - accuracy: 0.6991 - val_loss: 16.2362 - val_accuracy: 0.6250\n",
      "Epoch 978/1000\n",
      "638/638 [==============================] - 0s 38us/step - loss: 15.9345 - accuracy: 0.6803 - val_loss: 12.2304 - val_accuracy: 0.6062\n",
      "Epoch 979/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.8434 - accuracy: 0.6740 - val_loss: 10.2199 - val_accuracy: 0.6313\n",
      "Epoch 980/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.0271 - accuracy: 0.7445 - val_loss: 15.5443 - val_accuracy: 0.6125\n",
      "Epoch 981/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.5708 - accuracy: 0.6912 - val_loss: 14.2620 - val_accuracy: 0.5688\n",
      "Epoch 982/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.7093 - accuracy: 0.6865 - val_loss: 10.9513 - val_accuracy: 0.6250\n",
      "Epoch 983/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.5226 - accuracy: 0.7210 - val_loss: 17.2656 - val_accuracy: 0.6125\n",
      "Epoch 984/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 15.6693 - accuracy: 0.6959 - val_loss: 20.1257 - val_accuracy: 0.6062\n",
      "Epoch 985/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.1163 - accuracy: 0.7132 - val_loss: 12.0949 - val_accuracy: 0.6250\n",
      "Epoch 986/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 17.3760 - accuracy: 0.7163 - val_loss: 21.4480 - val_accuracy: 0.6625\n",
      "Epoch 987/1000\n",
      "638/638 [==============================] - 0s 30us/step - loss: 16.9497 - accuracy: 0.7038 - val_loss: 13.2649 - val_accuracy: 0.6187\n",
      "Epoch 988/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 16.8558 - accuracy: 0.7335 - val_loss: 23.2138 - val_accuracy: 0.6500\n",
      "Epoch 989/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.2528 - accuracy: 0.7304 - val_loss: 13.7877 - val_accuracy: 0.6812\n",
      "Epoch 990/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 16.7984 - accuracy: 0.6787 - val_loss: 17.7455 - val_accuracy: 0.5750\n",
      "Epoch 991/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 17.4941 - accuracy: 0.6395 - val_loss: 24.9205 - val_accuracy: 0.6750\n",
      "Epoch 992/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 17.1812 - accuracy: 0.7147 - val_loss: 15.1652 - val_accuracy: 0.5875\n",
      "Epoch 993/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 14.8064 - accuracy: 0.7038 - val_loss: 24.0880 - val_accuracy: 0.5938\n",
      "Epoch 994/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 17.0665 - accuracy: 0.6881 - val_loss: 11.5587 - val_accuracy: 0.6687\n",
      "Epoch 995/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 15.6611 - accuracy: 0.6803 - val_loss: 14.7376 - val_accuracy: 0.6000\n",
      "Epoch 996/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 15.9101 - accuracy: 0.6724 - val_loss: 14.4217 - val_accuracy: 0.6125\n",
      "Epoch 997/1000\n",
      "638/638 [==============================] - 0s 36us/step - loss: 15.8797 - accuracy: 0.6771 - val_loss: 11.6926 - val_accuracy: 0.5813\n",
      "Epoch 998/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.7701 - accuracy: 0.7335 - val_loss: 11.7308 - val_accuracy: 0.5875\n",
      "Epoch 999/1000\n",
      "638/638 [==============================] - 0s 34us/step - loss: 16.4680 - accuracy: 0.6818 - val_loss: 19.3997 - val_accuracy: 0.6000\n",
      "Epoch 1000/1000\n",
      "638/638 [==============================] - 0s 33us/step - loss: 16.0102 - accuracy: 0.6708 - val_loss: 8.4907 - val_accuracy: 0.6625\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_test,y_test), epochs=training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3959.79927654   88.38690687 3833.36343175   73.86084999 3720.26202803\n",
      "   80.89942707 3875.73527218   89.76332243 3984.65855778   61.87102125\n",
      " 3719.09628146  182.6610984  3820.20357804  120.41394201 3784.76640989\n",
      "  129.26342628 4008.75510365   97.71687554 3794.63708076  176.50269565\n",
      " 4028.530833     41.54125543 3908.45807054   92.69808456 3837.47288001\n",
      "  158.77981372 3639.46159761  148.1763349  3754.47286213  171.42434555]\n",
      "[3960.7578    100.152374 3852.027      69.33155  3737.2583     71.16962\n",
      " 3887.0532     79.31982  3964.9758     47.215065 3699.0647    160.28706\n",
      " 3814.3845    122.85479  3789.8206    125.994095 4012.2092     96.01089\n",
      " 3805.9556    170.78809  4012.113      49.387844 3889.7249     81.48714\n",
      " 3843.043     148.57962  3647.4678    150.5106   3748.323     160.23267 ]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "print(y_test[0])\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict n images (frame, next n frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[92.43681939593179, 22.50037668652832, 141.306...</td>\n",
       "      <td>[70.80009877085162, 22.786106233538195, 115.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[104.5978211529732, 22.04513325984048, 153.672...</td>\n",
       "      <td>[78.53846153846153, 23.79722075869336, 130.692...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[114.654700661428, 17.817739838317607, 167.487...</td>\n",
       "      <td>[92.43681939593179, 22.50037668652832, 141.306...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[130.2578821609651, 15.584474619733086, 177.42...</td>\n",
       "      <td>[104.5978211529732, 22.04513325984048, 153.672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[137.88253604193972, 15.713794233289649, 183.3...</td>\n",
       "      <td>[114.654700661428, 17.817739838317607, 167.487...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>[8963.531319216798, 166.59466643040668, 8880.4...</td>\n",
       "      <td>[8940.46656641604, 155.12932330827067, 8861.91...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>[8971.234802590348, 161.42225471763803, 8889.3...</td>\n",
       "      <td>[8954.623459439574, 159.4531853577779, 8867.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>[8981.526291116494, 162.7594343308071, 8905.46...</td>\n",
       "      <td>[8963.531319216798, 166.59466643040668, 8880.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>[8990.157367074604, 162.0370600843532, 8920.93...</td>\n",
       "      <td>[8971.234802590348, 161.42225471763803, 8889.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>[9007.540353356892, 162.55978798586568, 8926.0...</td>\n",
       "      <td>[8981.526291116494, 162.7594343308071, 8905.46...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     X  \\\n",
       "0    [92.43681939593179, 22.50037668652832, 141.306...   \n",
       "1    [104.5978211529732, 22.04513325984048, 153.672...   \n",
       "2    [114.654700661428, 17.817739838317607, 167.487...   \n",
       "3    [130.2578821609651, 15.584474619733086, 177.42...   \n",
       "4    [137.88253604193972, 15.713794233289649, 183.3...   \n",
       "..                                                 ...   \n",
       "790  [8963.531319216798, 166.59466643040668, 8880.4...   \n",
       "791  [8971.234802590348, 161.42225471763803, 8889.3...   \n",
       "792  [8981.526291116494, 162.7594343308071, 8905.46...   \n",
       "793  [8990.157367074604, 162.0370600843532, 8920.93...   \n",
       "794  [9007.540353356892, 162.55978798586568, 8926.0...   \n",
       "\n",
       "                                                     y  \n",
       "0    [70.80009877085162, 22.786106233538195, 115.03...  \n",
       "1    [78.53846153846153, 23.79722075869336, 130.692...  \n",
       "2    [92.43681939593179, 22.50037668652832, 141.306...  \n",
       "3    [104.5978211529732, 22.04513325984048, 153.672...  \n",
       "4    [114.654700661428, 17.817739838317607, 167.487...  \n",
       "..                                                 ...  \n",
       "790  [8940.46656641604, 155.12932330827067, 8861.91...  \n",
       "791  [8954.623459439574, 159.4531853577779, 8867.34...  \n",
       "792  [8963.531319216798, 166.59466643040668, 8880.4...  \n",
       "793  [8971.234802590348, 161.42225471763803, 8889.3...  \n",
       "794  [8981.526291116494, 162.7594343308071, 8905.46...  \n",
       "\n",
       "[795 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5\n",
    "\n",
    "# data in format (frame, next n frames)\n",
    "dfn = pd.DataFrame(columns=['X', 'y'])\n",
    "\n",
    "for i in range(n, len(frames)):\n",
    "    target = []\n",
    "    for j in range(n-1, -1, -1):\n",
    "        target = target + frames[i-j].tolist()\n",
    "    dfn = dfn.append({'X': frames[i-2], 'y': target}, ignore_index=True)\n",
    "\n",
    "dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dfn['X'].tolist()\n",
    "y = dfn['y'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (636, 30)\n",
      "y_train shape: (636, 150)\n",
      "X_test shape: (159, 30)\n",
      "y_test shape: (159, 150)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"y_train shape: \" + str(y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "\n",
    "optimizer = 'NAdam'\n",
    "loss = 'mae'\n",
    "metrics = ['accuracy']\n",
    "training_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "n_input = 2 * no_skyrmions\n",
    "n_hidden_1 = 64\n",
    "n_hidden_2 = 64\n",
    "n_hidden_3 = 64\n",
    "n_output = 2 * no_skyrmions * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "\n",
    "# Neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_1, input_dim=n_input, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_hidden_2, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_hidden_3, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 636 samples, validate on 159 samples\n",
      "Epoch 1/1000\n",
      "636/636 [==============================] - 0s 282us/step - loss: 1502.5858 - accuracy: 0.0016 - val_loss: 423.4865 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "636/636 [==============================] - 0s 53us/step - loss: 272.5306 - accuracy: 0.0252 - val_loss: 157.3624 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 175.8958 - accuracy: 0.1682 - val_loss: 228.6469 - val_accuracy: 0.3774\n",
      "Epoch 4/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 167.3197 - accuracy: 0.0314 - val_loss: 173.6216 - val_accuracy: 0.0692\n",
      "Epoch 5/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 158.5492 - accuracy: 0.0063 - val_loss: 126.2378 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 142.2317 - accuracy: 0.0000e+00 - val_loss: 149.4156 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 153.6730 - accuracy: 0.0016 - val_loss: 152.6318 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 144.9384 - accuracy: 0.1478 - val_loss: 255.9664 - val_accuracy: 0.3774\n",
      "Epoch 9/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 124.8678 - accuracy: 0.0487 - val_loss: 106.5943 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 125.6541 - accuracy: 0.0000e+00 - val_loss: 250.5863 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 146.9137 - accuracy: 0.1761 - val_loss: 186.9853 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 146.5401 - accuracy: 0.1006 - val_loss: 88.1366 - val_accuracy: 0.3585\n",
      "Epoch 13/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 132.1639 - accuracy: 0.1006 - val_loss: 67.0716 - val_accuracy: 0.0063\n",
      "Epoch 14/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 135.1363 - accuracy: 0.1305 - val_loss: 116.8168 - val_accuracy: 0.0126\n",
      "Epoch 15/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 137.9857 - accuracy: 0.1258 - val_loss: 167.9969 - val_accuracy: 0.2830\n",
      "Epoch 16/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 132.1035 - accuracy: 0.1164 - val_loss: 153.3050 - val_accuracy: 0.2956\n",
      "Epoch 17/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 134.6002 - accuracy: 0.0833 - val_loss: 99.5445 - val_accuracy: 0.0126\n",
      "Epoch 18/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 132.0440 - accuracy: 0.0252 - val_loss: 176.6843 - val_accuracy: 0.0126\n",
      "Epoch 19/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 132.7775 - accuracy: 0.1085 - val_loss: 83.3086 - val_accuracy: 0.0126\n",
      "Epoch 20/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 125.9760 - accuracy: 0.0314 - val_loss: 144.2439 - val_accuracy: 0.1006\n",
      "Epoch 21/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 128.6351 - accuracy: 0.1840 - val_loss: 142.0419 - val_accuracy: 0.0440\n",
      "Epoch 22/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 119.8908 - accuracy: 0.0975 - val_loss: 163.5943 - val_accuracy: 0.0126\n",
      "Epoch 23/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 128.6482 - accuracy: 0.0708 - val_loss: 42.0043 - val_accuracy: 0.2642\n",
      "Epoch 24/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 123.0281 - accuracy: 0.1950 - val_loss: 146.8846 - val_accuracy: 0.4403\n",
      "Epoch 25/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 120.2835 - accuracy: 0.1855 - val_loss: 53.5949 - val_accuracy: 0.0377\n",
      "Epoch 26/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 120.7853 - accuracy: 0.2547 - val_loss: 67.7706 - val_accuracy: 0.2075\n",
      "Epoch 27/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 111.1776 - accuracy: 0.1478 - val_loss: 128.2622 - val_accuracy: 0.4025\n",
      "Epoch 28/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 121.6727 - accuracy: 0.2374 - val_loss: 96.7651 - val_accuracy: 0.0314\n",
      "Epoch 29/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 115.0340 - accuracy: 0.1651 - val_loss: 95.1219 - val_accuracy: 0.0440\n",
      "Epoch 30/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 112.1920 - accuracy: 0.1588 - val_loss: 140.5381 - val_accuracy: 0.3459\n",
      "Epoch 31/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 109.3537 - accuracy: 0.1509 - val_loss: 104.8614 - val_accuracy: 0.0126\n",
      "Epoch 32/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 116.2986 - accuracy: 0.3349 - val_loss: 149.4561 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 110.7972 - accuracy: 0.1981 - val_loss: 161.4169 - val_accuracy: 0.2579\n",
      "Epoch 34/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 100.4379 - accuracy: 0.2406 - val_loss: 122.0424 - val_accuracy: 0.3962\n",
      "Epoch 35/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 105.7788 - accuracy: 0.1509 - val_loss: 154.5158 - val_accuracy: 0.1132\n",
      "Epoch 36/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 117.3128 - accuracy: 0.1053 - val_loss: 94.1243 - val_accuracy: 0.0252\n",
      "Epoch 37/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 108.7576 - accuracy: 0.1368 - val_loss: 122.7180 - val_accuracy: 0.1761\n",
      "Epoch 38/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 109.0408 - accuracy: 0.2814 - val_loss: 38.8424 - val_accuracy: 0.1824\n",
      "Epoch 39/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 101.1836 - accuracy: 0.3208 - val_loss: 151.7682 - val_accuracy: 0.4025\n",
      "Epoch 40/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 111.0176 - accuracy: 0.3585 - val_loss: 100.3871 - val_accuracy: 0.5597\n",
      "Epoch 41/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 102.8643 - accuracy: 0.2736 - val_loss: 97.5732 - val_accuracy: 0.3962\n",
      "Epoch 42/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 108.3194 - accuracy: 0.3821 - val_loss: 105.0216 - val_accuracy: 0.3396\n",
      "Epoch 43/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 108.8052 - accuracy: 0.3947 - val_loss: 148.0813 - val_accuracy: 0.4591\n",
      "Epoch 44/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 100.8231 - accuracy: 0.3538 - val_loss: 51.9836 - val_accuracy: 0.6289\n",
      "Epoch 45/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 95.6730 - accuracy: 0.2783 - val_loss: 143.2956 - val_accuracy: 0.0252\n",
      "Epoch 46/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 100.1523 - accuracy: 0.2044 - val_loss: 72.2269 - val_accuracy: 0.1824\n",
      "Epoch 47/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 102.2454 - accuracy: 0.1572 - val_loss: 120.9189 - val_accuracy: 0.1321\n",
      "Epoch 48/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 98.8547 - accuracy: 0.2170 - val_loss: 178.1379 - val_accuracy: 0.1572\n",
      "Epoch 49/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 103.0120 - accuracy: 0.3318 - val_loss: 64.2877 - val_accuracy: 0.2516\n",
      "Epoch 50/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 87.6254 - accuracy: 0.2437 - val_loss: 131.0666 - val_accuracy: 0.2013\n",
      "Epoch 51/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 100.2354 - accuracy: 0.2830 - val_loss: 102.6332 - val_accuracy: 0.3145\n",
      "Epoch 52/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 99.3047 - accuracy: 0.2469 - val_loss: 41.1587 - val_accuracy: 0.1195\n",
      "Epoch 53/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 94.9178 - accuracy: 0.2783 - val_loss: 136.0745 - val_accuracy: 0.4843\n",
      "Epoch 54/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 91.3295 - accuracy: 0.4890 - val_loss: 87.0635 - val_accuracy: 0.5094\n",
      "Epoch 55/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 97.3834 - accuracy: 0.4575 - val_loss: 30.3134 - val_accuracy: 0.5094\n",
      "Epoch 56/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 85.2267 - accuracy: 0.2673 - val_loss: 152.8689 - val_accuracy: 0.2453\n",
      "Epoch 57/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 93.4421 - accuracy: 0.2233 - val_loss: 126.9293 - val_accuracy: 0.2390\n",
      "Epoch 58/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 89.9831 - accuracy: 0.2406 - val_loss: 150.2186 - val_accuracy: 0.2075\n",
      "Epoch 59/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 98.4147 - accuracy: 0.2830 - val_loss: 107.9657 - val_accuracy: 0.3459\n",
      "Epoch 60/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 96.6127 - accuracy: 0.2925 - val_loss: 48.7870 - val_accuracy: 0.0189\n",
      "Epoch 61/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 93.6798 - accuracy: 0.2437 - val_loss: 74.4366 - val_accuracy: 0.1447\n",
      "Epoch 62/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 93.0431 - accuracy: 0.3428 - val_loss: 79.9544 - val_accuracy: 0.2264\n",
      "Epoch 63/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 85.0242 - accuracy: 0.3239 - val_loss: 88.0274 - val_accuracy: 0.2830\n",
      "Epoch 64/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 92.5062 - accuracy: 0.2186 - val_loss: 129.7238 - val_accuracy: 0.4465\n",
      "Epoch 65/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 90.3737 - accuracy: 0.4497 - val_loss: 153.7851 - val_accuracy: 0.1824\n",
      "Epoch 66/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 94.6467 - accuracy: 0.2987 - val_loss: 81.1559 - val_accuracy: 0.2075\n",
      "Epoch 67/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 86.7690 - accuracy: 0.4575 - val_loss: 75.6225 - val_accuracy: 0.3082\n",
      "Epoch 68/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 82.4052 - accuracy: 0.2972 - val_loss: 118.6181 - val_accuracy: 0.4528\n",
      "Epoch 69/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 88.6998 - accuracy: 0.3255 - val_loss: 89.7618 - val_accuracy: 0.1824\n",
      "Epoch 70/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 82.5472 - accuracy: 0.3097 - val_loss: 47.8636 - val_accuracy: 0.0377\n",
      "Epoch 71/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 86.4745 - accuracy: 0.2987 - val_loss: 88.5897 - val_accuracy: 0.2453\n",
      "Epoch 72/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 88.4168 - accuracy: 0.4182 - val_loss: 90.5194 - val_accuracy: 0.4906\n",
      "Epoch 73/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 86.3620 - accuracy: 0.5739 - val_loss: 120.8982 - val_accuracy: 0.2390\n",
      "Epoch 74/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 88.7111 - accuracy: 0.3223 - val_loss: 85.0305 - val_accuracy: 0.1006\n",
      "Epoch 75/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 86.9145 - accuracy: 0.3176 - val_loss: 47.2931 - val_accuracy: 0.1635\n",
      "Epoch 76/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 87.2675 - accuracy: 0.3962 - val_loss: 31.5311 - val_accuracy: 0.3082\n",
      "Epoch 77/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 83.6973 - accuracy: 0.4072 - val_loss: 70.2193 - val_accuracy: 0.3774\n",
      "Epoch 78/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 80.4099 - accuracy: 0.4701 - val_loss: 115.4387 - val_accuracy: 0.1761\n",
      "Epoch 79/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 85.5198 - accuracy: 0.4308 - val_loss: 116.0756 - val_accuracy: 0.1321\n",
      "Epoch 80/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 87.9178 - accuracy: 0.3821 - val_loss: 30.9209 - val_accuracy: 0.5472\n",
      "Epoch 81/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 80.9890 - accuracy: 0.3443 - val_loss: 38.5360 - val_accuracy: 0.2830\n",
      "Epoch 82/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 79.9848 - accuracy: 0.4686 - val_loss: 66.3040 - val_accuracy: 0.4465\n",
      "Epoch 83/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 78.8849 - accuracy: 0.4355 - val_loss: 106.8859 - val_accuracy: 0.4591\n",
      "Epoch 84/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 84.0874 - accuracy: 0.3884 - val_loss: 66.2108 - val_accuracy: 0.1572\n",
      "Epoch 85/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 78.4236 - accuracy: 0.2626 - val_loss: 116.1106 - val_accuracy: 0.2704\n",
      "Epoch 86/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 83.3415 - accuracy: 0.2956 - val_loss: 52.5082 - val_accuracy: 0.2453\n",
      "Epoch 87/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 76.8564 - accuracy: 0.3978 - val_loss: 107.9026 - val_accuracy: 0.5723\n",
      "Epoch 88/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 81.5793 - accuracy: 0.4403 - val_loss: 122.1956 - val_accuracy: 0.5786\n",
      "Epoch 89/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 79.1089 - accuracy: 0.5314 - val_loss: 89.8922 - val_accuracy: 0.6164\n",
      "Epoch 90/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 77.3841 - accuracy: 0.5314 - val_loss: 106.0592 - val_accuracy: 0.3333\n",
      "Epoch 91/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 77.6594 - accuracy: 0.3003 - val_loss: 81.7223 - val_accuracy: 0.1698\n",
      "Epoch 92/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 80.6619 - accuracy: 0.4387 - val_loss: 124.0408 - val_accuracy: 0.2201\n",
      "Epoch 93/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 77.9195 - accuracy: 0.3333 - val_loss: 100.8705 - val_accuracy: 0.1887\n",
      "Epoch 94/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 72.5876 - accuracy: 0.3774 - val_loss: 74.4093 - val_accuracy: 0.2075\n",
      "Epoch 95/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 73.6871 - accuracy: 0.2358 - val_loss: 54.6823 - val_accuracy: 0.3899\n",
      "Epoch 96/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 76.7540 - accuracy: 0.3805 - val_loss: 95.5114 - val_accuracy: 0.1258\n",
      "Epoch 97/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 76.9353 - accuracy: 0.2123 - val_loss: 32.4935 - val_accuracy: 0.2642\n",
      "Epoch 98/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 76.1063 - accuracy: 0.3428 - val_loss: 94.9916 - val_accuracy: 0.4591\n",
      "Epoch 99/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 74.2657 - accuracy: 0.3789 - val_loss: 56.7268 - val_accuracy: 0.2642\n",
      "Epoch 100/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 78.3309 - accuracy: 0.3538 - val_loss: 33.0227 - val_accuracy: 0.2830\n",
      "Epoch 101/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 72.3873 - accuracy: 0.3318 - val_loss: 75.3794 - val_accuracy: 0.1447\n",
      "Epoch 102/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 74.9964 - accuracy: 0.3333 - val_loss: 26.7373 - val_accuracy: 0.6415\n",
      "Epoch 103/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 69.7330 - accuracy: 0.3836 - val_loss: 77.2618 - val_accuracy: 0.2767\n",
      "Epoch 104/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 76.4643 - accuracy: 0.3192 - val_loss: 68.6029 - val_accuracy: 0.4277\n",
      "Epoch 105/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 70.0325 - accuracy: 0.4072 - val_loss: 114.0232 - val_accuracy: 0.2893\n",
      "Epoch 106/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 73.5649 - accuracy: 0.2343 - val_loss: 37.1448 - val_accuracy: 0.5031\n",
      "Epoch 107/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 72.1877 - accuracy: 0.4308 - val_loss: 69.1133 - val_accuracy: 0.6352\n",
      "Epoch 108/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 75.9404 - accuracy: 0.5299 - val_loss: 42.7682 - val_accuracy: 0.4780\n",
      "Epoch 109/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 73.2559 - accuracy: 0.4858 - val_loss: 27.0216 - val_accuracy: 0.5535\n",
      "Epoch 110/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 75.0066 - accuracy: 0.4245 - val_loss: 53.1499 - val_accuracy: 0.4277\n",
      "Epoch 111/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 62.9556 - accuracy: 0.3774 - val_loss: 107.1538 - val_accuracy: 0.3585\n",
      "Epoch 112/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 67.4592 - accuracy: 0.3443 - val_loss: 89.3705 - val_accuracy: 0.0755\n",
      "Epoch 113/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 70.0214 - accuracy: 0.3711 - val_loss: 94.8109 - val_accuracy: 0.3836\n",
      "Epoch 114/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 73.5239 - accuracy: 0.3978 - val_loss: 61.5596 - val_accuracy: 0.1950\n",
      "Epoch 115/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 68.4559 - accuracy: 0.4497 - val_loss: 93.5463 - val_accuracy: 0.2704\n",
      "Epoch 116/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 72.3069 - accuracy: 0.4214 - val_loss: 50.4666 - val_accuracy: 0.5597\n",
      "Epoch 117/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 70.5492 - accuracy: 0.5881 - val_loss: 104.1849 - val_accuracy: 0.2767\n",
      "Epoch 118/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 71.4158 - accuracy: 0.4843 - val_loss: 110.0433 - val_accuracy: 0.2390\n",
      "Epoch 119/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 72.6506 - accuracy: 0.3145 - val_loss: 76.0840 - val_accuracy: 0.2956\n",
      "Epoch 120/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 71.0763 - accuracy: 0.4119 - val_loss: 60.1697 - val_accuracy: 0.3899\n",
      "Epoch 121/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 68.7518 - accuracy: 0.4780 - val_loss: 72.4063 - val_accuracy: 0.5597\n",
      "Epoch 122/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 70.0066 - accuracy: 0.4764 - val_loss: 76.2651 - val_accuracy: 0.5912\n",
      "Epoch 123/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 69.5267 - accuracy: 0.4937 - val_loss: 62.5017 - val_accuracy: 0.5723\n",
      "Epoch 124/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 69.2142 - accuracy: 0.3852 - val_loss: 66.6909 - val_accuracy: 0.2264\n",
      "Epoch 125/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 67.4513 - accuracy: 0.3097 - val_loss: 58.4158 - val_accuracy: 0.2201\n",
      "Epoch 126/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 63.2837 - accuracy: 0.4009 - val_loss: 106.4791 - val_accuracy: 0.5409\n",
      "Epoch 127/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 63.7912 - accuracy: 0.3553 - val_loss: 99.7621 - val_accuracy: 0.2579\n",
      "Epoch 128/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 68.1240 - accuracy: 0.1792 - val_loss: 66.1016 - val_accuracy: 0.2956\n",
      "Epoch 129/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 69.3910 - accuracy: 0.3113 - val_loss: 51.1141 - val_accuracy: 0.2327\n",
      "Epoch 130/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 67.5787 - accuracy: 0.3789 - val_loss: 86.1278 - val_accuracy: 0.5660\n",
      "Epoch 131/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 64.0786 - accuracy: 0.4119 - val_loss: 38.8415 - val_accuracy: 0.5283\n",
      "Epoch 132/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 66.3188 - accuracy: 0.6085 - val_loss: 46.3610 - val_accuracy: 0.5094\n",
      "Epoch 133/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 63.8411 - accuracy: 0.2704 - val_loss: 115.1184 - val_accuracy: 0.0692\n",
      "Epoch 134/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 66.3448 - accuracy: 0.3789 - val_loss: 44.0303 - val_accuracy: 0.4340\n",
      "Epoch 135/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 66.0231 - accuracy: 0.4717 - val_loss: 33.2534 - val_accuracy: 0.4151\n",
      "Epoch 136/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 63.7244 - accuracy: 0.3742 - val_loss: 67.0093 - val_accuracy: 0.4088\n",
      "Epoch 137/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 61.9905 - accuracy: 0.3538 - val_loss: 38.2933 - val_accuracy: 0.1635\n",
      "Epoch 138/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 64.8950 - accuracy: 0.4591 - val_loss: 47.3405 - val_accuracy: 0.4843\n",
      "Epoch 139/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 62.6317 - accuracy: 0.4355 - val_loss: 62.9247 - val_accuracy: 0.4465\n",
      "Epoch 140/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 63.8577 - accuracy: 0.2940 - val_loss: 83.7209 - val_accuracy: 0.4214\n",
      "Epoch 141/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 65.2783 - accuracy: 0.6242 - val_loss: 66.2593 - val_accuracy: 0.5535\n",
      "Epoch 142/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 62.7058 - accuracy: 0.5267 - val_loss: 62.5384 - val_accuracy: 0.6038\n",
      "Epoch 143/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 56.0077 - accuracy: 0.3994 - val_loss: 79.6907 - val_accuracy: 0.3774\n",
      "Epoch 144/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 63.2700 - accuracy: 0.5629 - val_loss: 47.1134 - val_accuracy: 0.4214\n",
      "Epoch 145/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 45.5435 - accuracy: 0.406 - 0s 47us/step - loss: 59.9777 - accuracy: 0.3475 - val_loss: 82.1797 - val_accuracy: 0.5597\n",
      "Epoch 146/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 60.3087 - accuracy: 0.5063 - val_loss: 61.1167 - val_accuracy: 0.3459\n",
      "Epoch 147/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 57.3216 - accuracy: 0.3664 - val_loss: 59.2783 - val_accuracy: 0.2956\n",
      "Epoch 148/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 59.0714 - accuracy: 0.3695 - val_loss: 37.8344 - val_accuracy: 0.5535\n",
      "Epoch 149/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 60.5721 - accuracy: 0.4528 - val_loss: 53.5693 - val_accuracy: 0.5283\n",
      "Epoch 150/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 60.4462 - accuracy: 0.4811 - val_loss: 70.8580 - val_accuracy: 0.5283\n",
      "Epoch 151/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 61.7444 - accuracy: 0.3019 - val_loss: 43.6024 - val_accuracy: 0.4277\n",
      "Epoch 152/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 56.0199 - accuracy: 0.4214 - val_loss: 29.3159 - val_accuracy: 0.4906\n",
      "Epoch 153/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 53.6419 - accuracy: 0.5110 - val_loss: 27.2383 - val_accuracy: 0.5786\n",
      "Epoch 154/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 57.5317 - accuracy: 0.2846 - val_loss: 65.6317 - val_accuracy: 0.2327\n",
      "Epoch 155/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 59.5005 - accuracy: 0.3192 - val_loss: 62.4343 - val_accuracy: 0.3396\n",
      "Epoch 156/1000\n",
      "636/636 [==============================] - 0s 64us/step - loss: 61.9905 - accuracy: 0.4308 - val_loss: 52.3202 - val_accuracy: 0.3522\n",
      "Epoch 157/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 58.6371 - accuracy: 0.3003 - val_loss: 68.8964 - val_accuracy: 0.4088\n",
      "Epoch 158/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 61.9413 - accuracy: 0.4088 - val_loss: 42.3988 - val_accuracy: 0.3774\n",
      "Epoch 159/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 58.5603 - accuracy: 0.4324 - val_loss: 52.7769 - val_accuracy: 0.3145\n",
      "Epoch 160/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 58.1315 - accuracy: 0.3758 - val_loss: 82.1937 - val_accuracy: 0.5346\n",
      "Epoch 161/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 57.6355 - accuracy: 0.5110 - val_loss: 72.2621 - val_accuracy: 0.5723\n",
      "Epoch 162/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 57.7297 - accuracy: 0.5739 - val_loss: 59.8220 - val_accuracy: 0.5535\n",
      "Epoch 163/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 55.1655 - accuracy: 0.5818 - val_loss: 27.8718 - val_accuracy: 0.6352\n",
      "Epoch 164/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 54.9593 - accuracy: 0.4748 - val_loss: 83.9063 - val_accuracy: 0.2453\n",
      "Epoch 165/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 56.9548 - accuracy: 0.4591 - val_loss: 52.1122 - val_accuracy: 0.4277\n",
      "Epoch 166/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 59.4397 - accuracy: 0.5189 - val_loss: 65.2411 - val_accuracy: 0.3899\n",
      "Epoch 167/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 56.8619 - accuracy: 0.5031 - val_loss: 63.8504 - val_accuracy: 0.5786\n",
      "Epoch 168/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 58.1460 - accuracy: 0.4984 - val_loss: 74.0386 - val_accuracy: 0.2327\n",
      "Epoch 169/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 57.5309 - accuracy: 0.3915 - val_loss: 80.9625 - val_accuracy: 0.2830\n",
      "Epoch 170/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 56.4343 - accuracy: 0.4670 - val_loss: 29.7162 - val_accuracy: 0.3333\n",
      "Epoch 171/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 53.4100 - accuracy: 0.4811 - val_loss: 68.2701 - val_accuracy: 0.4843\n",
      "Epoch 172/1000\n",
      "636/636 [==============================] - 0s 46us/step - loss: 58.1491 - accuracy: 0.4009 - val_loss: 55.3538 - val_accuracy: 0.3208\n",
      "Epoch 173/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 57.1504 - accuracy: 0.4811 - val_loss: 43.3680 - val_accuracy: 0.5157\n",
      "Epoch 174/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 56.4916 - accuracy: 0.5708 - val_loss: 44.8659 - val_accuracy: 0.5409\n",
      "Epoch 175/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 54.4426 - accuracy: 0.5456 - val_loss: 67.3348 - val_accuracy: 0.5723\n",
      "Epoch 176/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 52.9465 - accuracy: 0.4009 - val_loss: 38.1496 - val_accuracy: 0.2264\n",
      "Epoch 177/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 55.1696 - accuracy: 0.4104 - val_loss: 37.9359 - val_accuracy: 0.3019\n",
      "Epoch 178/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 53.9482 - accuracy: 0.3475 - val_loss: 57.6061 - val_accuracy: 0.2704\n",
      "Epoch 179/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 54.8135 - accuracy: 0.2877 - val_loss: 40.7826 - val_accuracy: 0.0755\n",
      "Epoch 180/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 52.7768 - accuracy: 0.4041 - val_loss: 45.6541 - val_accuracy: 0.4151\n",
      "Epoch 181/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 51.5933 - accuracy: 0.4937 - val_loss: 49.4317 - val_accuracy: 0.4025\n",
      "Epoch 182/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 50.9945 - accuracy: 0.4654 - val_loss: 57.7769 - val_accuracy: 0.2138\n",
      "Epoch 183/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 51.8935 - accuracy: 0.4009 - val_loss: 48.3146 - val_accuracy: 0.4403\n",
      "Epoch 184/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 49.8653 - accuracy: 0.5236 - val_loss: 59.9719 - val_accuracy: 0.4591\n",
      "Epoch 185/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 53.9953 - accuracy: 0.4969 - val_loss: 33.1373 - val_accuracy: 0.2327\n",
      "Epoch 186/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 51.7395 - accuracy: 0.4890 - val_loss: 39.8572 - val_accuracy: 0.5723\n",
      "Epoch 187/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 53.2309 - accuracy: 0.4638 - val_loss: 25.4936 - val_accuracy: 0.3208\n",
      "Epoch 188/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 49.2110 - accuracy: 0.4434 - val_loss: 49.2748 - val_accuracy: 0.5660\n",
      "Epoch 189/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 53.5900 - accuracy: 0.4072 - val_loss: 53.4771 - val_accuracy: 0.4277\n",
      "Epoch 190/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 47.9235 - accuracy: 0.5047 - val_loss: 33.5591 - val_accuracy: 0.3082\n",
      "Epoch 191/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 52.4352 - accuracy: 0.3286 - val_loss: 78.8677 - val_accuracy: 0.2390\n",
      "Epoch 192/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 53.0957 - accuracy: 0.5173 - val_loss: 26.8328 - val_accuracy: 0.4780\n",
      "Epoch 193/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 50.3445 - accuracy: 0.3160 - val_loss: 65.2899 - val_accuracy: 0.4403\n",
      "Epoch 194/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 52.8800 - accuracy: 0.5456 - val_loss: 62.8042 - val_accuracy: 0.4088\n",
      "Epoch 195/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 51.7737 - accuracy: 0.4308 - val_loss: 41.4637 - val_accuracy: 0.3208\n",
      "Epoch 196/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 46.0954 - accuracy: 0.4701 - val_loss: 57.7255 - val_accuracy: 0.5660\n",
      "Epoch 197/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 50.9212 - accuracy: 0.5896 - val_loss: 37.0044 - val_accuracy: 0.3836\n",
      "Epoch 198/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 51.1568 - accuracy: 0.4119 - val_loss: 51.4314 - val_accuracy: 0.3145\n",
      "Epoch 199/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 49.9743 - accuracy: 0.4827 - val_loss: 34.8167 - val_accuracy: 0.6415\n",
      "Epoch 200/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 51.3014 - accuracy: 0.4890 - val_loss: 46.9501 - val_accuracy: 0.2956\n",
      "Epoch 201/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 50.8479 - accuracy: 0.4072 - val_loss: 63.2744 - val_accuracy: 0.5346\n",
      "Epoch 202/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 51.6413 - accuracy: 0.5629 - val_loss: 56.0200 - val_accuracy: 0.5597\n",
      "Epoch 203/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 51.8601 - accuracy: 0.6226 - val_loss: 21.8690 - val_accuracy: 0.5723\n",
      "Epoch 204/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 48.2828 - accuracy: 0.5425 - val_loss: 36.0883 - val_accuracy: 0.3899\n",
      "Epoch 205/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 48.1612 - accuracy: 0.4843 - val_loss: 40.3165 - val_accuracy: 0.5472\n",
      "Epoch 206/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 49.5469 - accuracy: 0.5425 - val_loss: 39.3701 - val_accuracy: 0.5409\n",
      "Epoch 207/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 50.1726 - accuracy: 0.6289 - val_loss: 75.2947 - val_accuracy: 0.5660\n",
      "Epoch 208/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 49.8603 - accuracy: 0.5079 - val_loss: 48.2110 - val_accuracy: 0.5157\n",
      "Epoch 209/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 49.4455 - accuracy: 0.4041 - val_loss: 81.6945 - val_accuracy: 0.5409\n",
      "Epoch 210/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 49.2747 - accuracy: 0.4969 - val_loss: 40.4542 - val_accuracy: 0.5157\n",
      "Epoch 211/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 46.4476 - accuracy: 0.4780 - val_loss: 54.0676 - val_accuracy: 0.5597\n",
      "Epoch 212/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 47.1309 - accuracy: 0.5126 - val_loss: 61.1545 - val_accuracy: 0.5157\n",
      "Epoch 213/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 45.0783 - accuracy: 0.5157 - val_loss: 51.6238 - val_accuracy: 0.4528\n",
      "Epoch 214/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 48.9408 - accuracy: 0.4513 - val_loss: 47.5750 - val_accuracy: 0.3522\n",
      "Epoch 215/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 41.7709 - accuracy: 0.4434 - val_loss: 84.4418 - val_accuracy: 0.5409\n",
      "Epoch 216/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 45.7008 - accuracy: 0.6384 - val_loss: 64.7737 - val_accuracy: 0.4151\n",
      "Epoch 217/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 46.6133 - accuracy: 0.4937 - val_loss: 20.4195 - val_accuracy: 0.2893\n",
      "Epoch 218/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 45.5406 - accuracy: 0.5157 - val_loss: 48.3836 - val_accuracy: 0.6415\n",
      "Epoch 219/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 47.9110 - accuracy: 0.5582 - val_loss: 28.9721 - val_accuracy: 0.3899\n",
      "Epoch 220/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 47.9496 - accuracy: 0.4371 - val_loss: 44.2752 - val_accuracy: 0.2956\n",
      "Epoch 221/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 46.0438 - accuracy: 0.4544 - val_loss: 44.8899 - val_accuracy: 0.6226\n",
      "Epoch 222/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 47.9475 - accuracy: 0.5440 - val_loss: 44.5252 - val_accuracy: 0.3585\n",
      "Epoch 223/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 45.5645 - accuracy: 0.5283 - val_loss: 42.7974 - val_accuracy: 0.5723\n",
      "Epoch 224/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 47.8863 - accuracy: 0.4670 - val_loss: 34.7135 - val_accuracy: 0.3648\n",
      "Epoch 225/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 45.4304 - accuracy: 0.4575 - val_loss: 34.4464 - val_accuracy: 0.3208\n",
      "Epoch 226/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 42.7168 - accuracy: 0.5173 - val_loss: 29.6485 - val_accuracy: 0.4591\n",
      "Epoch 227/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 47.4744 - accuracy: 0.4277 - val_loss: 23.3175 - val_accuracy: 0.6164\n",
      "Epoch 228/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 45.1601 - accuracy: 0.5031 - val_loss: 53.0532 - val_accuracy: 0.3333\n",
      "Epoch 229/1000\n",
      "636/636 [==============================] - 0s 46us/step - loss: 46.4271 - accuracy: 0.5566 - val_loss: 68.3675 - val_accuracy: 0.5472\n",
      "Epoch 230/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 45.9165 - accuracy: 0.6053 - val_loss: 68.3786 - val_accuracy: 0.4025\n",
      "Epoch 231/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 46.1916 - accuracy: 0.4874 - val_loss: 43.6837 - val_accuracy: 0.5472\n",
      "Epoch 232/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 46.8718 - accuracy: 0.5676 - val_loss: 45.5546 - val_accuracy: 0.5535\n",
      "Epoch 233/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 45.6930 - accuracy: 0.4434 - val_loss: 59.0374 - val_accuracy: 0.4214\n",
      "Epoch 234/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 46.9980 - accuracy: 0.4670 - val_loss: 27.3580 - val_accuracy: 0.5472\n",
      "Epoch 235/1000\n",
      "636/636 [==============================] - 0s 57us/step - loss: 42.7499 - accuracy: 0.4780 - val_loss: 38.4418 - val_accuracy: 0.5535\n",
      "Epoch 236/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 40.7860 - accuracy: 0.781 - 0s 49us/step - loss: 44.6754 - accuracy: 0.4230 - val_loss: 32.3785 - val_accuracy: 0.3208\n",
      "Epoch 237/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 41.0268 - accuracy: 0.5063 - val_loss: 59.5454 - val_accuracy: 0.3962\n",
      "Epoch 238/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 43.0057 - accuracy: 0.5393 - val_loss: 46.6250 - val_accuracy: 0.2201\n",
      "Epoch 239/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 45.8824 - accuracy: 0.4890 - val_loss: 62.3409 - val_accuracy: 0.6226\n",
      "Epoch 240/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 44.4363 - accuracy: 0.5472 - val_loss: 53.8937 - val_accuracy: 0.3774\n",
      "Epoch 241/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 44.5104 - accuracy: 0.4858 - val_loss: 33.7213 - val_accuracy: 0.5283\n",
      "Epoch 242/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 43.3004 - accuracy: 0.4528 - val_loss: 66.6144 - val_accuracy: 0.5660\n",
      "Epoch 243/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 41.2940 - accuracy: 0.5079 - val_loss: 47.1965 - val_accuracy: 0.5283\n",
      "Epoch 244/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 45.1549 - accuracy: 0.6132 - val_loss: 69.1636 - val_accuracy: 0.5409\n",
      "Epoch 245/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 41.2064 - accuracy: 0.4355 - val_loss: 34.3260 - val_accuracy: 0.5283\n",
      "Epoch 246/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 42.5936 - accuracy: 0.5314 - val_loss: 55.2632 - val_accuracy: 0.4025\n",
      "Epoch 247/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 44.4110 - accuracy: 0.5157 - val_loss: 44.9856 - val_accuracy: 0.3333\n",
      "Epoch 248/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 42.9323 - accuracy: 0.4701 - val_loss: 47.1214 - val_accuracy: 0.2390\n",
      "Epoch 249/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 42.9588 - accuracy: 0.5031 - val_loss: 29.7488 - val_accuracy: 0.4780\n",
      "Epoch 250/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 28.5362 - accuracy: 0.437 - 0s 50us/step - loss: 41.4403 - accuracy: 0.5299 - val_loss: 44.3553 - val_accuracy: 0.5912\n",
      "Epoch 251/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 43.7733 - accuracy: 0.5975 - val_loss: 19.9635 - val_accuracy: 0.3711\n",
      "Epoch 252/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 40.7699 - accuracy: 0.5283 - val_loss: 28.2235 - val_accuracy: 0.4214\n",
      "Epoch 253/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 43.0254 - accuracy: 0.5566 - val_loss: 42.6588 - val_accuracy: 0.5723\n",
      "Epoch 254/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 42.2012 - accuracy: 0.5692 - val_loss: 51.5955 - val_accuracy: 0.5597\n",
      "Epoch 255/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 41.1132 - accuracy: 0.6132 - val_loss: 48.6390 - val_accuracy: 0.5157\n",
      "Epoch 256/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 42.4953 - accuracy: 0.6242 - val_loss: 59.4173 - val_accuracy: 0.3962\n",
      "Epoch 257/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 44.4299 - accuracy: 0.5503 - val_loss: 14.6896 - val_accuracy: 0.5157\n",
      "Epoch 258/1000\n",
      "636/636 [==============================] - 0s 53us/step - loss: 39.6276 - accuracy: 0.5252 - val_loss: 41.4191 - val_accuracy: 0.4151\n",
      "Epoch 259/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 38.5807 - accuracy: 0.5283 - val_loss: 57.0150 - val_accuracy: 0.4780\n",
      "Epoch 260/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 41.6590 - accuracy: 0.5047 - val_loss: 43.4223 - val_accuracy: 0.5786\n",
      "Epoch 261/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 40.7704 - accuracy: 0.5550 - val_loss: 59.8464 - val_accuracy: 0.3208\n",
      "Epoch 262/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 41.8715 - accuracy: 0.5110 - val_loss: 59.6611 - val_accuracy: 0.2893\n",
      "Epoch 263/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 41.7758 - accuracy: 0.5723 - val_loss: 36.3776 - val_accuracy: 0.4025\n",
      "Epoch 264/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 40.1234 - accuracy: 0.5204 - val_loss: 40.0075 - val_accuracy: 0.5975\n",
      "Epoch 265/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 39.8299 - accuracy: 0.4497 - val_loss: 43.5657 - val_accuracy: 0.4025\n",
      "Epoch 266/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 42.5037 - accuracy: 0.4513 - val_loss: 38.7365 - val_accuracy: 0.6164\n",
      "Epoch 267/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 41.1754 - accuracy: 0.5770 - val_loss: 66.3760 - val_accuracy: 0.5472\n",
      "Epoch 268/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 40.0832 - accuracy: 0.5865 - val_loss: 25.3455 - val_accuracy: 0.3836\n",
      "Epoch 269/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 38.5052 - accuracy: 0.5881 - val_loss: 49.1061 - val_accuracy: 0.5346\n",
      "Epoch 270/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 41.9496 - accuracy: 0.6336 - val_loss: 57.0674 - val_accuracy: 0.5409\n",
      "Epoch 271/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 37.9667 - accuracy: 0.5597 - val_loss: 29.3381 - val_accuracy: 0.5409\n",
      "Epoch 272/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 39.7299 - accuracy: 0.3868 - val_loss: 55.0254 - val_accuracy: 0.2579\n",
      "Epoch 273/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 41.1296 - accuracy: 0.4418 - val_loss: 53.0145 - val_accuracy: 0.3082\n",
      "Epoch 274/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 39.6683 - accuracy: 0.5173 - val_loss: 19.7421 - val_accuracy: 0.4717\n",
      "Epoch 275/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 38.0768 - accuracy: 0.4843 - val_loss: 19.9131 - val_accuracy: 0.5723\n",
      "Epoch 276/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 36.3051 - accuracy: 0.5692 - val_loss: 46.0492 - val_accuracy: 0.5660\n",
      "Epoch 277/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 38.8147 - accuracy: 0.5959 - val_loss: 53.7051 - val_accuracy: 0.5849\n",
      "Epoch 278/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 40.3073 - accuracy: 0.6242 - val_loss: 29.3665 - val_accuracy: 0.6226\n",
      "Epoch 279/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 39.0628 - accuracy: 0.5346 - val_loss: 52.9636 - val_accuracy: 0.6415\n",
      "Epoch 280/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 39.6213 - accuracy: 0.5566 - val_loss: 42.4719 - val_accuracy: 0.5723\n",
      "Epoch 281/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 38.5019 - accuracy: 0.5739 - val_loss: 42.9613 - val_accuracy: 0.5346\n",
      "Epoch 282/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 37.4575 - accuracy: 0.5377 - val_loss: 53.5810 - val_accuracy: 0.5346\n",
      "Epoch 283/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 39.6837 - accuracy: 0.5975 - val_loss: 24.1937 - val_accuracy: 0.4214\n",
      "Epoch 284/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 36.3697 - accuracy: 0.4355 - val_loss: 36.1726 - val_accuracy: 0.5472\n",
      "Epoch 285/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 39.3465 - accuracy: 0.6132 - val_loss: 36.7038 - val_accuracy: 0.5535\n",
      "Epoch 286/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 37.3145 - accuracy: 0.4308 - val_loss: 45.8294 - val_accuracy: 0.4465\n",
      "Epoch 287/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 38.6875 - accuracy: 0.5252 - val_loss: 76.9222 - val_accuracy: 0.5472\n",
      "Epoch 288/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 38.2447 - accuracy: 0.4528 - val_loss: 35.8488 - val_accuracy: 0.4843\n",
      "Epoch 289/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 35.7952 - accuracy: 0.5377 - val_loss: 52.4433 - val_accuracy: 0.4591\n",
      "Epoch 290/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 38.8858 - accuracy: 0.4450 - val_loss: 35.5897 - val_accuracy: 0.2830\n",
      "Epoch 291/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 34.1732 - accuracy: 0.5550 - val_loss: 43.1943 - val_accuracy: 0.3270\n",
      "Epoch 292/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 36.9918 - accuracy: 0.5157 - val_loss: 25.5633 - val_accuracy: 0.4528\n",
      "Epoch 293/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 36.6178 - accuracy: 0.5314 - val_loss: 26.0997 - val_accuracy: 0.5346\n",
      "Epoch 294/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 35.4652 - accuracy: 0.6289 - val_loss: 55.1934 - val_accuracy: 0.5472\n",
      "Epoch 295/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 38.6198 - accuracy: 0.6132 - val_loss: 49.5763 - val_accuracy: 0.5535\n",
      "Epoch 296/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 36.2876 - accuracy: 0.6006 - val_loss: 31.9627 - val_accuracy: 0.5660\n",
      "Epoch 297/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 37.5366 - accuracy: 0.4953 - val_loss: 37.0237 - val_accuracy: 0.5157\n",
      "Epoch 298/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 38.7251 - accuracy: 0.5550 - val_loss: 32.9703 - val_accuracy: 0.5472\n",
      "Epoch 299/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 35.7259 - accuracy: 0.5267 - val_loss: 45.1815 - val_accuracy: 0.5660\n",
      "Epoch 300/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 36.6334 - accuracy: 0.4513 - val_loss: 18.2124 - val_accuracy: 0.5220\n",
      "Epoch 301/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 34.3214 - accuracy: 0.6431 - val_loss: 39.2376 - val_accuracy: 0.5346\n",
      "Epoch 302/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 36.8295 - accuracy: 0.6242 - val_loss: 28.1897 - val_accuracy: 0.5283\n",
      "Epoch 303/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 36.3335 - accuracy: 0.5912 - val_loss: 51.8612 - val_accuracy: 0.5597\n",
      "Epoch 304/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 36.9411 - accuracy: 0.6226 - val_loss: 34.4251 - val_accuracy: 0.6164\n",
      "Epoch 305/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 36.3949 - accuracy: 0.6384 - val_loss: 44.2848 - val_accuracy: 0.5409\n",
      "Epoch 306/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 36.5764 - accuracy: 0.5802 - val_loss: 51.2295 - val_accuracy: 0.6101\n",
      "Epoch 307/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 35.9106 - accuracy: 0.5723 - val_loss: 41.5241 - val_accuracy: 0.6164\n",
      "Epoch 308/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 36.7983 - accuracy: 0.5943 - val_loss: 13.9171 - val_accuracy: 0.6289\n",
      "Epoch 309/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 31.9056 - accuracy: 0.6164 - val_loss: 30.0183 - val_accuracy: 0.6038\n",
      "Epoch 310/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 33.2546 - accuracy: 0.6226 - val_loss: 43.6206 - val_accuracy: 0.5723\n",
      "Epoch 311/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 35.9733 - accuracy: 0.6116 - val_loss: 41.5705 - val_accuracy: 0.3585\n",
      "Epoch 312/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 37.6950 - accuracy: 0.5566 - val_loss: 43.4785 - val_accuracy: 0.5723\n",
      "Epoch 313/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 35.7398 - accuracy: 0.6195 - val_loss: 27.1655 - val_accuracy: 0.6604\n",
      "Epoch 314/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 34.3349 - accuracy: 0.6226 - val_loss: 30.3955 - val_accuracy: 0.5220\n",
      "Epoch 315/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 35.2702 - accuracy: 0.5047 - val_loss: 32.3870 - val_accuracy: 0.4088\n",
      "Epoch 316/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 37.0201 - accuracy: 0.5142 - val_loss: 32.1700 - val_accuracy: 0.5723\n",
      "Epoch 317/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 33.8580 - accuracy: 0.5456 - val_loss: 38.2900 - val_accuracy: 0.5597\n",
      "Epoch 318/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 33.9395 - accuracy: 0.5660 - val_loss: 11.9282 - val_accuracy: 0.5535\n",
      "Epoch 319/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 34.0374 - accuracy: 0.5393 - val_loss: 23.7431 - val_accuracy: 0.5472\n",
      "Epoch 320/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 35.3404 - accuracy: 0.5739 - val_loss: 37.1233 - val_accuracy: 0.5912\n",
      "Epoch 321/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 35.8618 - accuracy: 0.5204 - val_loss: 20.8087 - val_accuracy: 0.5723\n",
      "Epoch 322/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 32.0494 - accuracy: 0.5881 - val_loss: 26.8766 - val_accuracy: 0.5786\n",
      "Epoch 323/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 35.1637 - accuracy: 0.6069 - val_loss: 25.7623 - val_accuracy: 0.5283\n",
      "Epoch 324/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 33.6691 - accuracy: 0.5063 - val_loss: 40.6641 - val_accuracy: 0.5912\n",
      "Epoch 325/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 34.6810 - accuracy: 0.6195 - val_loss: 24.5720 - val_accuracy: 0.6101\n",
      "Epoch 326/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 33.6473 - accuracy: 0.6478 - val_loss: 51.4325 - val_accuracy: 0.5975\n",
      "Epoch 327/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 34.8936 - accuracy: 0.5723 - val_loss: 35.2948 - val_accuracy: 0.6101\n",
      "Epoch 328/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 35.1101 - accuracy: 0.6384 - val_loss: 15.0763 - val_accuracy: 0.5723\n",
      "Epoch 329/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 29.8424 - accuracy: 0.4953 - val_loss: 49.6039 - val_accuracy: 0.5975\n",
      "Epoch 330/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 32.1523 - accuracy: 0.5896 - val_loss: 32.1346 - val_accuracy: 0.6226\n",
      "Epoch 331/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 32.5849 - accuracy: 0.6148 - val_loss: 39.2130 - val_accuracy: 0.5535\n",
      "Epoch 332/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 35.4511 - accuracy: 0.6525 - val_loss: 33.6441 - val_accuracy: 0.5660\n",
      "Epoch 333/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 34.1593 - accuracy: 0.6494 - val_loss: 28.8164 - val_accuracy: 0.5472\n",
      "Epoch 334/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 34.8579 - accuracy: 0.5881 - val_loss: 32.2237 - val_accuracy: 0.6981\n",
      "Epoch 335/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 33.4204 - accuracy: 0.5299 - val_loss: 26.3777 - val_accuracy: 0.3270\n",
      "Epoch 336/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 34.0074 - accuracy: 0.5802 - val_loss: 14.0525 - val_accuracy: 0.6038\n",
      "Epoch 337/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 33.1080 - accuracy: 0.6085 - val_loss: 17.8476 - val_accuracy: 0.5660\n",
      "Epoch 338/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 32.3640 - accuracy: 0.5346 - val_loss: 46.8346 - val_accuracy: 0.5597\n",
      "Epoch 339/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 35.0892 - accuracy: 0.6164 - val_loss: 41.3953 - val_accuracy: 0.5472\n",
      "Epoch 340/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 28.8598 - accuracy: 0.5692 - val_loss: 53.8808 - val_accuracy: 0.2767\n",
      "Epoch 341/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 33.7776 - accuracy: 0.6069 - val_loss: 20.1825 - val_accuracy: 0.5220\n",
      "Epoch 342/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 34.0018 - accuracy: 0.6226 - val_loss: 20.7081 - val_accuracy: 0.5094\n",
      "Epoch 343/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 33.9372 - accuracy: 0.5220 - val_loss: 15.8996 - val_accuracy: 0.5723\n",
      "Epoch 344/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 33.7961 - accuracy: 0.6101 - val_loss: 32.0195 - val_accuracy: 0.6038\n",
      "Epoch 345/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 32.6713 - accuracy: 0.5708 - val_loss: 33.1941 - val_accuracy: 0.6164\n",
      "Epoch 346/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 32.1120 - accuracy: 0.6635 - val_loss: 46.4133 - val_accuracy: 0.5597\n",
      "Epoch 347/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 32.0023 - accuracy: 0.4638 - val_loss: 39.6381 - val_accuracy: 0.5535\n",
      "Epoch 348/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 33.4949 - accuracy: 0.6164 - val_loss: 42.8641 - val_accuracy: 0.6289\n",
      "Epoch 349/1000\n",
      "636/636 [==============================] - 0s 40us/step - loss: 34.4920 - accuracy: 0.5802 - val_loss: 40.5468 - val_accuracy: 0.5786\n",
      "Epoch 350/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 33.5005 - accuracy: 0.5079 - val_loss: 17.2242 - val_accuracy: 0.3019\n",
      "Epoch 351/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 31.9957 - accuracy: 0.5503 - val_loss: 31.7319 - val_accuracy: 0.5157\n",
      "Epoch 352/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 31.7627 - accuracy: 0.5613 - val_loss: 41.0219 - val_accuracy: 0.6415\n",
      "Epoch 353/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 31.9443 - accuracy: 0.5487 - val_loss: 50.9761 - val_accuracy: 0.6164\n",
      "Epoch 354/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 30.3183 - accuracy: 0.6336 - val_loss: 57.4099 - val_accuracy: 0.7044\n",
      "Epoch 355/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 29.5489 - accuracy: 0.5723 - val_loss: 43.4439 - val_accuracy: 0.5157\n",
      "Epoch 356/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 30.6590 - accuracy: 0.5645 - val_loss: 39.1762 - val_accuracy: 0.5786\n",
      "Epoch 357/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 32.8019 - accuracy: 0.4843 - val_loss: 31.4818 - val_accuracy: 0.6604\n",
      "Epoch 358/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 33.4341 - accuracy: 0.6038 - val_loss: 13.2640 - val_accuracy: 0.4088\n",
      "Epoch 359/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 30.7345 - accuracy: 0.5094 - val_loss: 27.4021 - val_accuracy: 0.5535\n",
      "Epoch 360/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 32.3376 - accuracy: 0.6541 - val_loss: 30.4654 - val_accuracy: 0.5472\n",
      "Epoch 361/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 32.5055 - accuracy: 0.5236 - val_loss: 33.5223 - val_accuracy: 0.2767\n",
      "Epoch 362/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 30.7655 - accuracy: 0.4764 - val_loss: 38.1035 - val_accuracy: 0.4151\n",
      "Epoch 363/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 31.8578 - accuracy: 0.5991 - val_loss: 17.6125 - val_accuracy: 0.3962\n",
      "Epoch 364/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 27.7574 - accuracy: 0.6069 - val_loss: 21.1744 - val_accuracy: 0.5723\n",
      "Epoch 365/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 32.1060 - accuracy: 0.6022 - val_loss: 22.1599 - val_accuracy: 0.5535\n",
      "Epoch 366/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 31.8699 - accuracy: 0.6226 - val_loss: 24.8689 - val_accuracy: 0.5535\n",
      "Epoch 367/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 31.3138 - accuracy: 0.5425 - val_loss: 47.8616 - val_accuracy: 0.6478\n",
      "Epoch 368/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 30.8509 - accuracy: 0.6022 - val_loss: 14.0462 - val_accuracy: 0.6101\n",
      "Epoch 369/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 28.7300 - accuracy: 0.5739 - val_loss: 17.5970 - val_accuracy: 0.5031\n",
      "Epoch 370/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 31.1022 - accuracy: 0.5660 - val_loss: 51.2950 - val_accuracy: 0.5723\n",
      "Epoch 371/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 30.1305 - accuracy: 0.5943 - val_loss: 24.5771 - val_accuracy: 0.5472\n",
      "Epoch 372/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 31.7040 - accuracy: 0.6022 - val_loss: 16.0394 - val_accuracy: 0.5975\n",
      "Epoch 373/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 29.5778 - accuracy: 0.6179 - val_loss: 24.9969 - val_accuracy: 0.4654\n",
      "Epoch 374/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 31.3144 - accuracy: 0.6242 - val_loss: 16.9301 - val_accuracy: 0.5723\n",
      "Epoch 375/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 30.5641 - accuracy: 0.5142 - val_loss: 46.6404 - val_accuracy: 0.1887\n",
      "Epoch 376/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 31.2570 - accuracy: 0.5409 - val_loss: 23.5363 - val_accuracy: 0.2767\n",
      "Epoch 377/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 28.0375 - accuracy: 0.4308 - val_loss: 36.3668 - val_accuracy: 0.5723\n",
      "Epoch 378/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 30.7610 - accuracy: 0.6368 - val_loss: 34.5777 - val_accuracy: 0.5597\n",
      "Epoch 379/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 29.7044 - accuracy: 0.6085 - val_loss: 35.3046 - val_accuracy: 0.5472\n",
      "Epoch 380/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 30.8947 - accuracy: 0.5126 - val_loss: 23.7338 - val_accuracy: 0.5912\n",
      "Epoch 381/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 31.1086 - accuracy: 0.5943 - val_loss: 16.7287 - val_accuracy: 0.5660\n",
      "Epoch 382/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 30.9790 - accuracy: 0.5645 - val_loss: 24.2723 - val_accuracy: 0.6478\n",
      "Epoch 383/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 30.2820 - accuracy: 0.5818 - val_loss: 22.5842 - val_accuracy: 0.5535\n",
      "Epoch 384/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 29.7387 - accuracy: 0.5991 - val_loss: 27.3736 - val_accuracy: 0.5723\n",
      "Epoch 385/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 28.5352 - accuracy: 0.5818 - val_loss: 36.8207 - val_accuracy: 0.5786\n",
      "Epoch 386/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 29.7015 - accuracy: 0.6305 - val_loss: 34.9354 - val_accuracy: 0.5975\n",
      "Epoch 387/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 30.5653 - accuracy: 0.6541 - val_loss: 27.4213 - val_accuracy: 0.5660\n",
      "Epoch 388/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 30.9620 - accuracy: 0.6038 - val_loss: 14.3514 - val_accuracy: 0.6226\n",
      "Epoch 389/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 28.3835 - accuracy: 0.6431 - val_loss: 50.3438 - val_accuracy: 0.5660\n",
      "Epoch 390/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 29.8927 - accuracy: 0.6101 - val_loss: 34.8307 - val_accuracy: 0.5723\n",
      "Epoch 391/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 27.0235 - accuracy: 0.5157 - val_loss: 37.2662 - val_accuracy: 0.6478\n",
      "Epoch 392/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 28.6564 - accuracy: 0.5991 - val_loss: 31.5614 - val_accuracy: 0.5660\n",
      "Epoch 393/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 29.9695 - accuracy: 0.5770 - val_loss: 12.9215 - val_accuracy: 0.5849\n",
      "Epoch 394/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 28.2443 - accuracy: 0.6447 - val_loss: 28.6547 - val_accuracy: 0.5723\n",
      "Epoch 395/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 28.3133 - accuracy: 0.5660 - val_loss: 28.1187 - val_accuracy: 0.5157\n",
      "Epoch 396/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 29.6716 - accuracy: 0.6053 - val_loss: 14.1049 - val_accuracy: 0.5723\n",
      "Epoch 397/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 27.8782 - accuracy: 0.5849 - val_loss: 38.5780 - val_accuracy: 0.6101\n",
      "Epoch 398/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 27.3319 - accuracy: 0.6022 - val_loss: 24.6696 - val_accuracy: 0.2516\n",
      "Epoch 399/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 29.4814 - accuracy: 0.5425 - val_loss: 22.7264 - val_accuracy: 0.6226\n",
      "Epoch 400/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 28.0798 - accuracy: 0.5896 - val_loss: 23.5056 - val_accuracy: 0.5723\n",
      "Epoch 401/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 30.8445 - accuracy: 0.6415 - val_loss: 29.0494 - val_accuracy: 0.5723\n",
      "Epoch 402/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 29.9671 - accuracy: 0.6274 - val_loss: 21.4941 - val_accuracy: 0.5660\n",
      "Epoch 403/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 25.6757 - accuracy: 0.468 - 0s 38us/step - loss: 28.4946 - accuracy: 0.6305 - val_loss: 34.8569 - val_accuracy: 0.5723\n",
      "Epoch 404/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 28.8853 - accuracy: 0.6305 - val_loss: 36.9013 - val_accuracy: 0.5597\n",
      "Epoch 405/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 28.5427 - accuracy: 0.6399 - val_loss: 29.1434 - val_accuracy: 0.6352\n",
      "Epoch 406/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 27.1582 - accuracy: 0.5991 - val_loss: 38.3430 - val_accuracy: 0.5220\n",
      "Epoch 407/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 29.5946 - accuracy: 0.6274 - val_loss: 42.8482 - val_accuracy: 0.6352\n",
      "Epoch 408/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 28.8247 - accuracy: 0.5299 - val_loss: 13.8305 - val_accuracy: 0.6667\n",
      "Epoch 409/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 12.5522 - accuracy: 0.625 - 0s 42us/step - loss: 27.6639 - accuracy: 0.6494 - val_loss: 38.8252 - val_accuracy: 0.5723\n",
      "Epoch 410/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 27.7195 - accuracy: 0.6572 - val_loss: 29.5687 - val_accuracy: 0.6478\n",
      "Epoch 411/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 28.6240 - accuracy: 0.6541 - val_loss: 38.0074 - val_accuracy: 0.5597\n",
      "Epoch 412/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 27.0151 - accuracy: 0.5786 - val_loss: 48.9680 - val_accuracy: 0.5346\n",
      "Epoch 413/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 28.8129 - accuracy: 0.5393 - val_loss: 25.5491 - val_accuracy: 0.4591\n",
      "Epoch 414/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 27.9308 - accuracy: 0.5849 - val_loss: 31.9953 - val_accuracy: 0.6604\n",
      "Epoch 415/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 29.6935 - accuracy: 0.5456 - val_loss: 26.1137 - val_accuracy: 0.5723\n",
      "Epoch 416/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 28.2842 - accuracy: 0.6447 - val_loss: 27.7726 - val_accuracy: 0.6792\n",
      "Epoch 417/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 28.9410 - accuracy: 0.5676 - val_loss: 11.9358 - val_accuracy: 0.6667\n",
      "Epoch 418/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 27.7749 - accuracy: 0.6462 - val_loss: 26.7512 - val_accuracy: 0.6415\n",
      "Epoch 419/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 29.2369 - accuracy: 0.6085 - val_loss: 34.3932 - val_accuracy: 0.5409\n",
      "Epoch 420/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 28.8043 - accuracy: 0.5991 - val_loss: 35.8564 - val_accuracy: 0.3082\n",
      "Epoch 421/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 28.4595 - accuracy: 0.5142 - val_loss: 17.1114 - val_accuracy: 0.5786\n",
      "Epoch 422/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 27.7892 - accuracy: 0.5928 - val_loss: 40.0355 - val_accuracy: 0.5660\n",
      "Epoch 423/1000\n",
      "636/636 [==============================] - 0s 46us/step - loss: 27.4394 - accuracy: 0.6258 - val_loss: 33.9713 - val_accuracy: 0.6918\n",
      "Epoch 424/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 26.7875 - accuracy: 0.6006 - val_loss: 20.9538 - val_accuracy: 0.3899\n",
      "Epoch 425/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 28.0149 - accuracy: 0.4670 - val_loss: 22.7829 - val_accuracy: 0.5786\n",
      "Epoch 426/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 28.2603 - accuracy: 0.6022 - val_loss: 12.7546 - val_accuracy: 0.6101\n",
      "Epoch 427/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 26.4681 - accuracy: 0.5975 - val_loss: 33.2925 - val_accuracy: 0.5472\n",
      "Epoch 428/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 26.3885 - accuracy: 0.6305 - val_loss: 16.8705 - val_accuracy: 0.4780\n",
      "Epoch 429/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 28.4486 - accuracy: 0.5487 - val_loss: 30.6948 - val_accuracy: 0.5723\n",
      "Epoch 430/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 27.2463 - accuracy: 0.5975 - val_loss: 28.6817 - val_accuracy: 0.5660\n",
      "Epoch 431/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 27.5274 - accuracy: 0.6415 - val_loss: 29.7283 - val_accuracy: 0.5786\n",
      "Epoch 432/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 27.0414 - accuracy: 0.6415 - val_loss: 33.2129 - val_accuracy: 0.5157\n",
      "Epoch 433/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 26.0024 - accuracy: 0.6069 - val_loss: 28.8883 - val_accuracy: 0.4088\n",
      "Epoch 434/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 28.3160 - accuracy: 0.5802 - val_loss: 10.7872 - val_accuracy: 0.5786\n",
      "Epoch 435/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 24.6368 - accuracy: 0.5723 - val_loss: 26.5131 - val_accuracy: 0.5660\n",
      "Epoch 436/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 28.7874 - accuracy: 0.5629 - val_loss: 27.9642 - val_accuracy: 0.6101\n",
      "Epoch 437/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 27.4222 - accuracy: 0.5833 - val_loss: 29.1359 - val_accuracy: 0.6667\n",
      "Epoch 438/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 25.9550 - accuracy: 0.5991 - val_loss: 22.3890 - val_accuracy: 0.5283\n",
      "Epoch 439/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 27.8218 - accuracy: 0.5330 - val_loss: 20.9919 - val_accuracy: 0.5723\n",
      "Epoch 440/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 24.2527 - accuracy: 0.6116 - val_loss: 30.8198 - val_accuracy: 0.5975\n",
      "Epoch 441/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 26.5345 - accuracy: 0.5330 - val_loss: 24.5306 - val_accuracy: 0.5912\n",
      "Epoch 442/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 26.6182 - accuracy: 0.5566 - val_loss: 15.6683 - val_accuracy: 0.4340\n",
      "Epoch 443/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 26.8315 - accuracy: 0.5094 - val_loss: 16.8165 - val_accuracy: 0.5597\n",
      "Epoch 444/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 26.0845 - accuracy: 0.5786 - val_loss: 19.5873 - val_accuracy: 0.5786\n",
      "Epoch 445/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 22.8494 - accuracy: 0.5849 - val_loss: 30.1189 - val_accuracy: 0.6289\n",
      "Epoch 446/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 28.2848 - accuracy: 0.6305 - val_loss: 14.5693 - val_accuracy: 0.5660\n",
      "Epoch 447/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 25.2473 - accuracy: 0.6038 - val_loss: 11.8373 - val_accuracy: 0.4717\n",
      "Epoch 448/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 25.9617 - accuracy: 0.6211 - val_loss: 29.7321 - val_accuracy: 0.6730\n",
      "Epoch 449/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 26.3972 - accuracy: 0.6336 - val_loss: 23.3593 - val_accuracy: 0.5660\n",
      "Epoch 450/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 26.4958 - accuracy: 0.6274 - val_loss: 27.0249 - val_accuracy: 0.5723\n",
      "Epoch 451/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 25.9789 - accuracy: 0.6164 - val_loss: 18.2062 - val_accuracy: 0.5723\n",
      "Epoch 452/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 26.0756 - accuracy: 0.5928 - val_loss: 28.6588 - val_accuracy: 0.5723\n",
      "Epoch 453/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 25.2107 - accuracy: 0.6116 - val_loss: 31.6285 - val_accuracy: 0.5975\n",
      "Epoch 454/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 27.3902 - accuracy: 0.6525 - val_loss: 27.1822 - val_accuracy: 0.5660\n",
      "Epoch 455/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 27.4724 - accuracy: 0.6541 - val_loss: 19.0785 - val_accuracy: 0.6415\n",
      "Epoch 456/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 24.6898 - accuracy: 0.5267 - val_loss: 43.9837 - val_accuracy: 0.5786\n",
      "Epoch 457/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 26.6708 - accuracy: 0.6509 - val_loss: 16.7922 - val_accuracy: 0.5786\n",
      "Epoch 458/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 26.0740 - accuracy: 0.6399 - val_loss: 28.0781 - val_accuracy: 0.5597\n",
      "Epoch 459/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 24.0909 - accuracy: 0.6101 - val_loss: 38.6741 - val_accuracy: 0.5786\n",
      "Epoch 460/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 26.6522 - accuracy: 0.5582 - val_loss: 28.9295 - val_accuracy: 0.5786\n",
      "Epoch 461/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 26.8778 - accuracy: 0.5755 - val_loss: 41.5648 - val_accuracy: 0.6101\n",
      "Epoch 462/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 25.5057 - accuracy: 0.6682 - val_loss: 42.2783 - val_accuracy: 0.6101\n",
      "Epoch 463/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 26.1022 - accuracy: 0.6305 - val_loss: 29.0052 - val_accuracy: 0.6226\n",
      "Epoch 464/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 24.5548 - accuracy: 0.6258 - val_loss: 36.8000 - val_accuracy: 0.6478\n",
      "Epoch 465/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 25.6524 - accuracy: 0.6132 - val_loss: 14.2241 - val_accuracy: 0.6415\n",
      "Epoch 466/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 26.6168 - accuracy: 0.6289 - val_loss: 19.5224 - val_accuracy: 0.6478\n",
      "Epoch 467/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 24.8763 - accuracy: 0.5975 - val_loss: 32.0447 - val_accuracy: 0.5660\n",
      "Epoch 468/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 25.6229 - accuracy: 0.6242 - val_loss: 46.5556 - val_accuracy: 0.5723\n",
      "Epoch 469/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 25.8749 - accuracy: 0.5566 - val_loss: 33.6467 - val_accuracy: 0.6415\n",
      "Epoch 470/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 24.2032 - accuracy: 0.6384 - val_loss: 36.0884 - val_accuracy: 0.5786\n",
      "Epoch 471/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 25.8385 - accuracy: 0.6651 - val_loss: 36.5761 - val_accuracy: 0.6478\n",
      "Epoch 472/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 26.6153 - accuracy: 0.6274 - val_loss: 36.9377 - val_accuracy: 0.5786\n",
      "Epoch 473/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 26.5989 - accuracy: 0.6179 - val_loss: 27.7622 - val_accuracy: 0.5597\n",
      "Epoch 474/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 25.6428 - accuracy: 0.6101 - val_loss: 26.0104 - val_accuracy: 0.5723\n",
      "Epoch 475/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 25.7945 - accuracy: 0.6038 - val_loss: 40.8198 - val_accuracy: 0.5786\n",
      "Epoch 476/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 23.2727 - accuracy: 0.5928 - val_loss: 46.7418 - val_accuracy: 0.5660\n",
      "Epoch 477/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 25.4327 - accuracy: 0.6006 - val_loss: 18.3872 - val_accuracy: 0.6226\n",
      "Epoch 478/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 25.4451 - accuracy: 0.6399 - val_loss: 28.2259 - val_accuracy: 0.5723\n",
      "Epoch 479/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 24.5359 - accuracy: 0.6447 - val_loss: 28.1648 - val_accuracy: 0.2767\n",
      "Epoch 480/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 25.5323 - accuracy: 0.5676 - val_loss: 37.9486 - val_accuracy: 0.5786\n",
      "Epoch 481/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 25.8846 - accuracy: 0.4654 - val_loss: 27.2783 - val_accuracy: 0.6604\n",
      "Epoch 482/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 25.9279 - accuracy: 0.6179 - val_loss: 22.6157 - val_accuracy: 0.5723\n",
      "Epoch 483/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 26.3754 - accuracy: 0.6494 - val_loss: 10.8307 - val_accuracy: 0.5723\n",
      "Epoch 484/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 23.1478 - accuracy: 0.5991 - val_loss: 40.1998 - val_accuracy: 0.5723\n",
      "Epoch 485/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 24.9090 - accuracy: 0.6101 - val_loss: 26.9017 - val_accuracy: 0.5723\n",
      "Epoch 486/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 25.9055 - accuracy: 0.5849 - val_loss: 31.7613 - val_accuracy: 0.6415\n",
      "Epoch 487/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 24.9417 - accuracy: 0.5597 - val_loss: 44.8387 - val_accuracy: 0.5723\n",
      "Epoch 488/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 25.0591 - accuracy: 0.5833 - val_loss: 26.7321 - val_accuracy: 0.4780\n",
      "Epoch 489/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 24.7612 - accuracy: 0.6085 - val_loss: 21.3561 - val_accuracy: 0.5472\n",
      "Epoch 490/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 24.3798 - accuracy: 0.6226 - val_loss: 36.2199 - val_accuracy: 0.2642\n",
      "Epoch 491/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 24.2255 - accuracy: 0.6336 - val_loss: 27.9407 - val_accuracy: 0.5660\n",
      "Epoch 492/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 23.2596 - accuracy: 0.6588 - val_loss: 26.5571 - val_accuracy: 0.7044\n",
      "Epoch 493/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 24.2900 - accuracy: 0.6855 - val_loss: 26.3088 - val_accuracy: 0.6352\n",
      "Epoch 494/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 24.4525 - accuracy: 0.6242 - val_loss: 18.1646 - val_accuracy: 0.6981\n",
      "Epoch 495/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 24.3554 - accuracy: 0.6604 - val_loss: 23.3735 - val_accuracy: 0.5535\n",
      "Epoch 496/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 25.3127 - accuracy: 0.6399 - val_loss: 20.4910 - val_accuracy: 0.5094\n",
      "Epoch 497/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 25.2017 - accuracy: 0.6289 - val_loss: 12.0275 - val_accuracy: 0.5786\n",
      "Epoch 498/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 23.4055 - accuracy: 0.6053 - val_loss: 28.6747 - val_accuracy: 0.5786\n",
      "Epoch 499/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 24.6341 - accuracy: 0.6478 - val_loss: 33.0513 - val_accuracy: 0.5849\n",
      "Epoch 500/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 25.2933 - accuracy: 0.6226 - val_loss: 17.3379 - val_accuracy: 0.5535\n",
      "Epoch 501/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 23.9846 - accuracy: 0.5865 - val_loss: 12.8430 - val_accuracy: 0.5346\n",
      "Epoch 502/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 23.4592 - accuracy: 0.6321 - val_loss: 35.3737 - val_accuracy: 0.6667\n",
      "Epoch 503/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 24.1771 - accuracy: 0.6164 - val_loss: 46.9463 - val_accuracy: 0.4403\n",
      "Epoch 504/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 25.1259 - accuracy: 0.5896 - val_loss: 12.2515 - val_accuracy: 0.4654\n",
      "Epoch 505/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 12.0924 - accuracy: 0.500 - 0s 52us/step - loss: 22.3741 - accuracy: 0.5362 - val_loss: 18.6248 - val_accuracy: 0.6541\n",
      "Epoch 506/1000\n",
      "636/636 [==============================] - 0s 46us/step - loss: 25.4723 - accuracy: 0.5425 - val_loss: 22.9492 - val_accuracy: 0.5723\n",
      "Epoch 507/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 24.8339 - accuracy: 0.6494 - val_loss: 20.2408 - val_accuracy: 0.5409\n",
      "Epoch 508/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 23.1468 - accuracy: 0.5928 - val_loss: 34.9072 - val_accuracy: 0.5786\n",
      "Epoch 509/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 23.2551 - accuracy: 0.5456 - val_loss: 23.3322 - val_accuracy: 0.2642\n",
      "Epoch 510/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 24.5525 - accuracy: 0.5110 - val_loss: 24.3005 - val_accuracy: 0.5786\n",
      "Epoch 511/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 23.5748 - accuracy: 0.6336 - val_loss: 11.5783 - val_accuracy: 0.5786\n",
      "Epoch 512/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 20.6274 - accuracy: 0.6038 - val_loss: 30.8941 - val_accuracy: 0.5723\n",
      "Epoch 513/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 24.1435 - accuracy: 0.6053 - val_loss: 11.9393 - val_accuracy: 0.4654\n",
      "Epoch 514/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 23.2173 - accuracy: 0.5770 - val_loss: 28.2145 - val_accuracy: 0.5849\n",
      "Epoch 515/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 24.2408 - accuracy: 0.6132 - val_loss: 25.0233 - val_accuracy: 0.5660\n",
      "Epoch 516/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 23.1259 - accuracy: 0.6336 - val_loss: 18.9634 - val_accuracy: 0.5472\n",
      "Epoch 517/1000\n",
      "636/636 [==============================] - 0s 53us/step - loss: 24.0209 - accuracy: 0.6289 - val_loss: 23.8166 - val_accuracy: 0.6352\n",
      "Epoch 518/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 24.6347 - accuracy: 0.6289 - val_loss: 20.8643 - val_accuracy: 0.5723\n",
      "Epoch 519/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 22.1399 - accuracy: 0.6258 - val_loss: 12.1673 - val_accuracy: 0.5723\n",
      "Epoch 520/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 21.3647 - accuracy: 0.6415 - val_loss: 28.1061 - val_accuracy: 0.6478\n",
      "Epoch 521/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 24.0978 - accuracy: 0.6289 - val_loss: 19.6831 - val_accuracy: 0.3774\n",
      "Epoch 522/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 24.0318 - accuracy: 0.6101 - val_loss: 10.0396 - val_accuracy: 0.6038\n",
      "Epoch 523/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 23.3368 - accuracy: 0.6651 - val_loss: 27.5493 - val_accuracy: 0.6226\n",
      "Epoch 524/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 23.6978 - accuracy: 0.6431 - val_loss: 12.5268 - val_accuracy: 0.6478\n",
      "Epoch 525/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 22.9530 - accuracy: 0.6321 - val_loss: 19.2610 - val_accuracy: 0.5220\n",
      "Epoch 526/1000\n",
      "636/636 [==============================] - 0s 46us/step - loss: 24.1151 - accuracy: 0.6164 - val_loss: 15.9811 - val_accuracy: 0.6164\n",
      "Epoch 527/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 16.5638 - accuracy: 0.718 - 0s 47us/step - loss: 23.5154 - accuracy: 0.6415 - val_loss: 20.3947 - val_accuracy: 0.2642\n",
      "Epoch 528/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 21.8941 - accuracy: 0.5881 - val_loss: 32.2652 - val_accuracy: 0.5472\n",
      "Epoch 529/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 23.6299 - accuracy: 0.5503 - val_loss: 26.0236 - val_accuracy: 0.5723\n",
      "Epoch 530/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 24.1700 - accuracy: 0.5912 - val_loss: 29.4835 - val_accuracy: 0.6226\n",
      "Epoch 531/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 24.2073 - accuracy: 0.6226 - val_loss: 17.7844 - val_accuracy: 0.4340\n",
      "Epoch 532/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 23.5388 - accuracy: 0.6399 - val_loss: 28.7032 - val_accuracy: 0.5849\n",
      "Epoch 533/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 22.6184 - accuracy: 0.6478 - val_loss: 16.4242 - val_accuracy: 0.3648\n",
      "Epoch 534/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 23.1432 - accuracy: 0.5786 - val_loss: 22.6226 - val_accuracy: 0.5597\n",
      "Epoch 535/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 24.2995 - accuracy: 0.5975 - val_loss: 10.4254 - val_accuracy: 0.5472\n",
      "Epoch 536/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 22.3006 - accuracy: 0.6211 - val_loss: 21.6669 - val_accuracy: 0.4717\n",
      "Epoch 537/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 23.9122 - accuracy: 0.5928 - val_loss: 13.2907 - val_accuracy: 0.6667\n",
      "Epoch 538/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 22.4153 - accuracy: 0.6006 - val_loss: 27.6331 - val_accuracy: 0.5975\n",
      "Epoch 539/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 23.0687 - accuracy: 0.6053 - val_loss: 26.8026 - val_accuracy: 0.5597\n",
      "Epoch 540/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 23.5176 - accuracy: 0.6274 - val_loss: 25.7074 - val_accuracy: 0.5723\n",
      "Epoch 541/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 23.4999 - accuracy: 0.6431 - val_loss: 18.0668 - val_accuracy: 0.5472\n",
      "Epoch 542/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 21.8936 - accuracy: 0.6069 - val_loss: 38.3057 - val_accuracy: 0.5597\n",
      "Epoch 543/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 23.2933 - accuracy: 0.5865 - val_loss: 12.6739 - val_accuracy: 0.5346\n",
      "Epoch 544/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 22.6476 - accuracy: 0.6509 - val_loss: 18.1510 - val_accuracy: 0.6667\n",
      "Epoch 545/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 21.3222 - accuracy: 0.6085 - val_loss: 32.7932 - val_accuracy: 0.6101\n",
      "Epoch 546/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 22.7425 - accuracy: 0.6053 - val_loss: 25.0938 - val_accuracy: 0.4654\n",
      "Epoch 547/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 23.5657 - accuracy: 0.4890 - val_loss: 18.2935 - val_accuracy: 0.5660\n",
      "Epoch 548/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 17.2344 - accuracy: 0.468 - 0s 44us/step - loss: 23.0622 - accuracy: 0.5959 - val_loss: 14.4979 - val_accuracy: 0.5472\n",
      "Epoch 549/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 16.0811 - accuracy: 0.593 - 0s 42us/step - loss: 22.5002 - accuracy: 0.5550 - val_loss: 20.4759 - val_accuracy: 0.6855\n",
      "Epoch 550/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 23.1004 - accuracy: 0.6195 - val_loss: 25.5171 - val_accuracy: 0.6226\n",
      "Epoch 551/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 22.1427 - accuracy: 0.6038 - val_loss: 25.0497 - val_accuracy: 0.4528\n",
      "Epoch 552/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 26.1075 - accuracy: 0.343 - 0s 46us/step - loss: 22.5692 - accuracy: 0.5629 - val_loss: 16.5348 - val_accuracy: 0.5472\n",
      "Epoch 553/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 21.8945 - accuracy: 0.5833 - val_loss: 14.6816 - val_accuracy: 0.5660\n",
      "Epoch 554/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 21.0270 - accuracy: 0.6274 - val_loss: 11.0182 - val_accuracy: 0.6667\n",
      "Epoch 555/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 22.0799 - accuracy: 0.5566 - val_loss: 19.4347 - val_accuracy: 0.5660\n",
      "Epoch 556/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 22.6789 - accuracy: 0.6085 - val_loss: 27.3068 - val_accuracy: 0.6226\n",
      "Epoch 557/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 22.9300 - accuracy: 0.5582 - val_loss: 19.6722 - val_accuracy: 0.4277\n",
      "Epoch 558/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 22.9453 - accuracy: 0.5881 - val_loss: 18.1265 - val_accuracy: 0.6730\n",
      "Epoch 559/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 21.8476 - accuracy: 0.6226 - val_loss: 25.0281 - val_accuracy: 0.6541\n",
      "Epoch 560/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 22.6144 - accuracy: 0.5818 - val_loss: 14.3099 - val_accuracy: 0.5220\n",
      "Epoch 561/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 20.5205 - accuracy: 0.5865 - val_loss: 22.5912 - val_accuracy: 0.4969\n",
      "Epoch 562/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 22.0819 - accuracy: 0.6274 - val_loss: 23.2418 - val_accuracy: 0.4717\n",
      "Epoch 563/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 21.5248 - accuracy: 0.5802 - val_loss: 18.4799 - val_accuracy: 0.5723\n",
      "Epoch 564/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 22.7951 - accuracy: 0.6164 - val_loss: 12.1910 - val_accuracy: 0.5535\n",
      "Epoch 565/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 21.0479 - accuracy: 0.6447 - val_loss: 19.2091 - val_accuracy: 0.4403\n",
      "Epoch 566/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 21.8847 - accuracy: 0.6053 - val_loss: 29.0617 - val_accuracy: 0.5597\n",
      "Epoch 567/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 20.4876 - accuracy: 0.5645 - val_loss: 30.7429 - val_accuracy: 0.6164\n",
      "Epoch 568/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 22.0109 - accuracy: 0.5896 - val_loss: 17.0167 - val_accuracy: 0.5597\n",
      "Epoch 569/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 22.4977 - accuracy: 0.6635 - val_loss: 20.5770 - val_accuracy: 0.6792\n",
      "Epoch 570/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 21.7487 - accuracy: 0.6305 - val_loss: 28.5781 - val_accuracy: 0.6289\n",
      "Epoch 571/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 22.6705 - accuracy: 0.6085 - val_loss: 15.5511 - val_accuracy: 0.5786\n",
      "Epoch 572/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 22.1084 - accuracy: 0.6368 - val_loss: 11.3289 - val_accuracy: 0.4906\n",
      "Epoch 573/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 21.0464 - accuracy: 0.6415 - val_loss: 34.4625 - val_accuracy: 0.5031\n",
      "Epoch 574/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 21.8317 - accuracy: 0.6211 - val_loss: 18.2264 - val_accuracy: 0.4088\n",
      "Epoch 575/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 22.3894 - accuracy: 0.5692 - val_loss: 12.4461 - val_accuracy: 0.5849\n",
      "Epoch 576/1000\n",
      "636/636 [==============================] - 0s 46us/step - loss: 21.4684 - accuracy: 0.5928 - val_loss: 18.9133 - val_accuracy: 0.6604\n",
      "Epoch 577/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 22.1098 - accuracy: 0.6258 - val_loss: 10.2396 - val_accuracy: 0.6667\n",
      "Epoch 578/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 20.9587 - accuracy: 0.5393 - val_loss: 14.7701 - val_accuracy: 0.6541\n",
      "Epoch 579/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 21.6943 - accuracy: 0.5708 - val_loss: 23.1225 - val_accuracy: 0.5786\n",
      "Epoch 580/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 21.3045 - accuracy: 0.5000 - val_loss: 11.3034 - val_accuracy: 0.5535\n",
      "Epoch 581/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 21.1434 - accuracy: 0.6289 - val_loss: 21.2926 - val_accuracy: 0.5660\n",
      "Epoch 582/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 22.5498 - accuracy: 0.6006 - val_loss: 25.5189 - val_accuracy: 0.5283\n",
      "Epoch 583/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 22.3300 - accuracy: 0.5991 - val_loss: 20.9561 - val_accuracy: 0.5723\n",
      "Epoch 584/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 21.6062 - accuracy: 0.781 - 0s 47us/step - loss: 21.7023 - accuracy: 0.5755 - val_loss: 25.1195 - val_accuracy: 0.4843\n",
      "Epoch 585/1000\n",
      "636/636 [==============================] - 0s 46us/step - loss: 20.9915 - accuracy: 0.5896 - val_loss: 26.7526 - val_accuracy: 0.5723\n",
      "Epoch 586/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 20.6948 - accuracy: 0.5692 - val_loss: 38.0011 - val_accuracy: 0.5597\n",
      "Epoch 587/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 21.2431 - accuracy: 0.6352 - val_loss: 36.8896 - val_accuracy: 0.5660\n",
      "Epoch 588/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 21.8945 - accuracy: 0.5519 - val_loss: 15.3248 - val_accuracy: 0.6604\n",
      "Epoch 589/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 21.6925 - accuracy: 0.5991 - val_loss: 16.1763 - val_accuracy: 0.3082\n",
      "Epoch 590/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 20.9994 - accuracy: 0.5283 - val_loss: 14.8315 - val_accuracy: 0.5786\n",
      "Epoch 591/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 21.2804 - accuracy: 0.6368 - val_loss: 12.9924 - val_accuracy: 0.5409\n",
      "Epoch 592/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 21.1968 - accuracy: 0.6478 - val_loss: 25.6617 - val_accuracy: 0.5472\n",
      "Epoch 593/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 21.3998 - accuracy: 0.5645 - val_loss: 15.0496 - val_accuracy: 0.5157\n",
      "Epoch 594/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 21.1717 - accuracy: 0.5472 - val_loss: 18.2530 - val_accuracy: 0.2704\n",
      "Epoch 595/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 20.7587 - accuracy: 0.4465 - val_loss: 20.4727 - val_accuracy: 0.3333\n",
      "Epoch 596/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 20.7196 - accuracy: 0.5503 - val_loss: 11.9134 - val_accuracy: 0.5597\n",
      "Epoch 597/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 20.2602 - accuracy: 0.5975 - val_loss: 23.5429 - val_accuracy: 0.4717\n",
      "Epoch 598/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 18.9487 - accuracy: 0.5912 - val_loss: 31.8791 - val_accuracy: 0.5535\n",
      "Epoch 599/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 20.4647 - accuracy: 0.6525 - val_loss: 19.7712 - val_accuracy: 0.5220\n",
      "Epoch 600/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 21.2677 - accuracy: 0.6069 - val_loss: 20.0987 - val_accuracy: 0.5346\n",
      "Epoch 601/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 21.1059 - accuracy: 0.6053 - val_loss: 13.9461 - val_accuracy: 0.5094\n",
      "Epoch 602/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 20.0888 - accuracy: 0.6085 - val_loss: 20.5252 - val_accuracy: 0.6226\n",
      "Epoch 603/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 20.8373 - accuracy: 0.6368 - val_loss: 27.3238 - val_accuracy: 0.5094\n",
      "Epoch 604/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 19.3519 - accuracy: 0.6478 - val_loss: 21.2648 - val_accuracy: 0.6415\n",
      "Epoch 605/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 21.0758 - accuracy: 0.6038 - val_loss: 18.6411 - val_accuracy: 0.5786\n",
      "Epoch 606/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 20.6051 - accuracy: 0.6242 - val_loss: 19.4323 - val_accuracy: 0.4969\n",
      "Epoch 607/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 19.9332 - accuracy: 0.5865 - val_loss: 24.5099 - val_accuracy: 0.5723\n",
      "Epoch 608/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 20.0809 - accuracy: 0.6242 - val_loss: 22.7180 - val_accuracy: 0.6101\n",
      "Epoch 609/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 21.5488 - accuracy: 0.6226 - val_loss: 24.4565 - val_accuracy: 0.5786\n",
      "Epoch 610/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 19.9079 - accuracy: 0.5991 - val_loss: 29.9426 - val_accuracy: 0.5535\n",
      "Epoch 611/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 22.2002 - accuracy: 0.6494 - val_loss: 16.1315 - val_accuracy: 0.4214\n",
      "Epoch 612/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 21.3829 - accuracy: 0.6069 - val_loss: 24.8465 - val_accuracy: 0.5597\n",
      "Epoch 613/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 21.0501 - accuracy: 0.5519 - val_loss: 32.1148 - val_accuracy: 0.4465\n",
      "Epoch 614/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 21.2492 - accuracy: 0.6006 - val_loss: 14.0932 - val_accuracy: 0.5409\n",
      "Epoch 615/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 20.2215 - accuracy: 0.6745 - val_loss: 28.2779 - val_accuracy: 0.5472\n",
      "Epoch 616/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 21.3040 - accuracy: 0.6258 - val_loss: 13.5183 - val_accuracy: 0.5660\n",
      "Epoch 617/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 18.3489 - accuracy: 0.5928 - val_loss: 27.6594 - val_accuracy: 0.6226\n",
      "Epoch 618/1000\n",
      "636/636 [==============================] - 0s 52us/step - loss: 17.7697 - accuracy: 0.6494 - val_loss: 27.2347 - val_accuracy: 0.5660\n",
      "Epoch 619/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 20.7427 - accuracy: 0.6289 - val_loss: 12.0733 - val_accuracy: 0.6352\n",
      "Epoch 620/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 20.6357 - accuracy: 0.6384 - val_loss: 17.7106 - val_accuracy: 0.5786\n",
      "Epoch 621/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 19.5374 - accuracy: 0.5912 - val_loss: 23.2417 - val_accuracy: 0.5597\n",
      "Epoch 622/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 21.1763 - accuracy: 0.6305 - val_loss: 13.1901 - val_accuracy: 0.5220\n",
      "Epoch 623/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 20.2959 - accuracy: 0.6588 - val_loss: 22.2802 - val_accuracy: 0.6792\n",
      "Epoch 624/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 20.8149 - accuracy: 0.5047 - val_loss: 28.4931 - val_accuracy: 0.5597\n",
      "Epoch 625/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 20.4899 - accuracy: 0.6305 - val_loss: 10.7432 - val_accuracy: 0.5031\n",
      "Epoch 626/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 19.4925 - accuracy: 0.6132 - val_loss: 25.8883 - val_accuracy: 0.6226\n",
      "Epoch 627/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 20.2579 - accuracy: 0.6038 - val_loss: 17.8023 - val_accuracy: 0.5283\n",
      "Epoch 628/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 20.9786 - accuracy: 0.5110 - val_loss: 27.2183 - val_accuracy: 0.5660\n",
      "Epoch 629/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 21.4388 - accuracy: 0.5849 - val_loss: 12.2529 - val_accuracy: 0.5220\n",
      "Epoch 630/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 20.4334 - accuracy: 0.5833 - val_loss: 20.0363 - val_accuracy: 0.5786\n",
      "Epoch 631/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 20.3115 - accuracy: 0.6132 - val_loss: 22.3966 - val_accuracy: 0.6415\n",
      "Epoch 632/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 25.0947 - accuracy: 0.593 - 0s 36us/step - loss: 20.0555 - accuracy: 0.6069 - val_loss: 22.9452 - val_accuracy: 0.5472\n",
      "Epoch 633/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 20.2608 - accuracy: 0.5786 - val_loss: 26.0384 - val_accuracy: 0.6289\n",
      "Epoch 634/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 21.0379 - accuracy: 0.6164 - val_loss: 23.3447 - val_accuracy: 0.6164\n",
      "Epoch 635/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 21.1366 - accuracy: 0.6258 - val_loss: 20.1915 - val_accuracy: 0.4591\n",
      "Epoch 636/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 17.3254 - accuracy: 0.625 - 0s 44us/step - loss: 18.2901 - accuracy: 0.6132 - val_loss: 23.1873 - val_accuracy: 0.5786\n",
      "Epoch 637/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 18.1650 - accuracy: 0.6619 - val_loss: 11.1652 - val_accuracy: 0.6604\n",
      "Epoch 638/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 20.4995 - accuracy: 0.6415 - val_loss: 24.0576 - val_accuracy: 0.5786\n",
      "Epoch 639/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 19.9059 - accuracy: 0.6368 - val_loss: 12.0170 - val_accuracy: 0.4906\n",
      "Epoch 640/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 19.3247 - accuracy: 0.6211 - val_loss: 24.7472 - val_accuracy: 0.5660\n",
      "Epoch 641/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 19.3499 - accuracy: 0.6494 - val_loss: 16.3359 - val_accuracy: 0.5535\n",
      "Epoch 642/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 20.7403 - accuracy: 0.6211 - val_loss: 23.7172 - val_accuracy: 0.5723\n",
      "Epoch 643/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 20.4329 - accuracy: 0.6509 - val_loss: 17.8517 - val_accuracy: 0.6541\n",
      "Epoch 644/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 19.2741 - accuracy: 0.6258 - val_loss: 27.0438 - val_accuracy: 0.5786\n",
      "Epoch 645/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 20.6513 - accuracy: 0.5676 - val_loss: 12.0980 - val_accuracy: 0.4906\n",
      "Epoch 646/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 18.8604 - accuracy: 0.6604 - val_loss: 16.9395 - val_accuracy: 0.5346\n",
      "Epoch 647/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 20.1189 - accuracy: 0.6667 - val_loss: 20.8709 - val_accuracy: 0.5975\n",
      "Epoch 648/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 19.4396 - accuracy: 0.6651 - val_loss: 32.2330 - val_accuracy: 0.5786\n",
      "Epoch 649/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 20.4345 - accuracy: 0.6321 - val_loss: 23.9176 - val_accuracy: 0.5409\n",
      "Epoch 650/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 19.8779 - accuracy: 0.6368 - val_loss: 20.6310 - val_accuracy: 0.7233\n",
      "Epoch 651/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 19.5768 - accuracy: 0.6509 - val_loss: 21.2311 - val_accuracy: 0.6918\n",
      "Epoch 652/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 20.2331 - accuracy: 0.6195 - val_loss: 24.5161 - val_accuracy: 0.6730\n",
      "Epoch 653/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 19.3269 - accuracy: 0.6698 - val_loss: 20.6347 - val_accuracy: 0.5660\n",
      "Epoch 654/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 20.0594 - accuracy: 0.6541 - val_loss: 9.8685 - val_accuracy: 0.6541\n",
      "Epoch 655/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 19.3608 - accuracy: 0.6226 - val_loss: 21.1817 - val_accuracy: 0.5849\n",
      "Epoch 656/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 19.0659 - accuracy: 0.6447 - val_loss: 9.8959 - val_accuracy: 0.7296\n",
      "Epoch 657/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 18.3227 - accuracy: 0.5629 - val_loss: 14.8414 - val_accuracy: 0.6478\n",
      "Epoch 658/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 19.4717 - accuracy: 0.6447 - val_loss: 13.3687 - val_accuracy: 0.6038\n",
      "Epoch 659/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 20.2470 - accuracy: 0.6336 - val_loss: 24.3195 - val_accuracy: 0.5786\n",
      "Epoch 660/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 20.7255 - accuracy: 0.6368 - val_loss: 22.5605 - val_accuracy: 0.5723\n",
      "Epoch 661/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 19.6439 - accuracy: 0.6305 - val_loss: 19.9219 - val_accuracy: 0.5660\n",
      "Epoch 662/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 19.8582 - accuracy: 0.5881 - val_loss: 21.7944 - val_accuracy: 0.4528\n",
      "Epoch 663/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 20.6610 - accuracy: 0.6682 - val_loss: 17.8408 - val_accuracy: 0.5597\n",
      "Epoch 664/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 19.7075 - accuracy: 0.6321 - val_loss: 20.0246 - val_accuracy: 0.5094\n",
      "Epoch 665/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 19.4698 - accuracy: 0.6478 - val_loss: 26.0426 - val_accuracy: 0.6415\n",
      "Epoch 666/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 19.7937 - accuracy: 0.6525 - val_loss: 18.7497 - val_accuracy: 0.5723\n",
      "Epoch 667/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 19.6188 - accuracy: 0.6415 - val_loss: 13.7736 - val_accuracy: 0.5786\n",
      "Epoch 668/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 19.0166 - accuracy: 0.6604 - val_loss: 29.0461 - val_accuracy: 0.5786\n",
      "Epoch 669/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 19.8711 - accuracy: 0.6557 - val_loss: 15.2897 - val_accuracy: 0.5597\n",
      "Epoch 670/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 19.3192 - accuracy: 0.6525 - val_loss: 19.6381 - val_accuracy: 0.5786\n",
      "Epoch 671/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 19.9567 - accuracy: 0.6604 - val_loss: 22.7945 - val_accuracy: 0.7170\n",
      "Epoch 672/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 19.4285 - accuracy: 0.6840 - val_loss: 24.2526 - val_accuracy: 0.3522\n",
      "Epoch 673/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 19.8961 - accuracy: 0.6399 - val_loss: 18.0782 - val_accuracy: 0.5786\n",
      "Epoch 674/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 20.0295 - accuracy: 0.6557 - val_loss: 10.6620 - val_accuracy: 0.5723\n",
      "Epoch 675/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 19.2305 - accuracy: 0.6714 - val_loss: 14.7596 - val_accuracy: 0.6038\n",
      "Epoch 676/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 19.0382 - accuracy: 0.6698 - val_loss: 17.3342 - val_accuracy: 0.5723\n",
      "Epoch 677/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 18.8746 - accuracy: 0.6462 - val_loss: 24.8490 - val_accuracy: 0.6730\n",
      "Epoch 678/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 19.9068 - accuracy: 0.6478 - val_loss: 15.2119 - val_accuracy: 0.5912\n",
      "Epoch 679/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 16.4372 - accuracy: 0.531 - 0s 44us/step - loss: 18.6246 - accuracy: 0.6384 - val_loss: 23.8528 - val_accuracy: 0.6604\n",
      "Epoch 680/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 19.2403 - accuracy: 0.6494 - val_loss: 10.5253 - val_accuracy: 0.5660\n",
      "Epoch 681/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 10.0990 - accuracy: 0.656 - 0s 44us/step - loss: 18.6960 - accuracy: 0.6541 - val_loss: 23.4890 - val_accuracy: 0.4969\n",
      "Epoch 682/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 18.4125 - accuracy: 0.6682 - val_loss: 23.6042 - val_accuracy: 0.6164\n",
      "Epoch 683/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 19.1960 - accuracy: 0.6352 - val_loss: 19.6956 - val_accuracy: 0.5849\n",
      "Epoch 684/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 19.2377 - accuracy: 0.6525 - val_loss: 23.1181 - val_accuracy: 0.5975\n",
      "Epoch 685/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.8299 - accuracy: 0.6840 - val_loss: 18.5346 - val_accuracy: 0.5912\n",
      "Epoch 686/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 19.2992 - accuracy: 0.6651 - val_loss: 24.9530 - val_accuracy: 0.6289\n",
      "Epoch 687/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 19.6666 - accuracy: 0.6179 - val_loss: 19.1100 - val_accuracy: 0.5723\n",
      "Epoch 688/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 19.1499 - accuracy: 0.6824 - val_loss: 26.3282 - val_accuracy: 0.6730\n",
      "Epoch 689/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 19.3574 - accuracy: 0.6588 - val_loss: 11.6079 - val_accuracy: 0.5849\n",
      "Epoch 690/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.9021 - accuracy: 0.6950 - val_loss: 16.0681 - val_accuracy: 0.6164\n",
      "Epoch 691/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 18.1237 - accuracy: 0.6855 - val_loss: 21.6810 - val_accuracy: 0.6415\n",
      "Epoch 692/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 19.3080 - accuracy: 0.6777 - val_loss: 26.8235 - val_accuracy: 0.6352\n",
      "Epoch 693/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 19.2274 - accuracy: 0.6604 - val_loss: 20.1752 - val_accuracy: 0.6541\n",
      "Epoch 694/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 18.9011 - accuracy: 0.6588 - val_loss: 13.4363 - val_accuracy: 0.6792\n",
      "Epoch 695/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.6274 - accuracy: 0.6730 - val_loss: 9.5618 - val_accuracy: 0.5597\n",
      "Epoch 696/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 17.3118 - accuracy: 0.6289 - val_loss: 23.2805 - val_accuracy: 0.4654\n",
      "Epoch 697/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 19.2687 - accuracy: 0.5975 - val_loss: 13.2352 - val_accuracy: 0.5535\n",
      "Epoch 698/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.9679 - accuracy: 0.6557 - val_loss: 9.9177 - val_accuracy: 0.6415\n",
      "Epoch 699/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.7874 - accuracy: 0.6179 - val_loss: 12.7456 - val_accuracy: 0.5723\n",
      "Epoch 700/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.6311 - accuracy: 0.6148 - val_loss: 19.8056 - val_accuracy: 0.6604\n",
      "Epoch 701/1000\n",
      "636/636 [==============================] - 0s 31us/step - loss: 17.8497 - accuracy: 0.6368 - val_loss: 23.6887 - val_accuracy: 0.6415\n",
      "Epoch 702/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 19.7931 - accuracy: 0.6478 - val_loss: 12.2090 - val_accuracy: 0.6855\n",
      "Epoch 703/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 18.7149 - accuracy: 0.6305 - val_loss: 12.9261 - val_accuracy: 0.6478\n",
      "Epoch 704/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 19.1638 - accuracy: 0.7107 - val_loss: 12.0493 - val_accuracy: 0.6101\n",
      "Epoch 705/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 17.7008 - accuracy: 0.6604 - val_loss: 22.7459 - val_accuracy: 0.5723\n",
      "Epoch 706/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 19.5328 - accuracy: 0.6494 - val_loss: 15.2652 - val_accuracy: 0.5786\n",
      "Epoch 707/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 18.9408 - accuracy: 0.6447 - val_loss: 18.2709 - val_accuracy: 0.6289\n",
      "Epoch 708/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 19.7301 - accuracy: 0.6965 - val_loss: 15.3288 - val_accuracy: 0.6352\n",
      "Epoch 709/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 18.9474 - accuracy: 0.6840 - val_loss: 25.9050 - val_accuracy: 0.6352\n",
      "Epoch 710/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 19.3525 - accuracy: 0.6903 - val_loss: 21.4886 - val_accuracy: 0.6730\n",
      "Epoch 711/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 18.8215 - accuracy: 0.6525 - val_loss: 18.3682 - val_accuracy: 0.6352\n",
      "Epoch 712/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 19.0149 - accuracy: 0.6871 - val_loss: 15.2967 - val_accuracy: 0.6855\n",
      "Epoch 713/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 17.8494 - accuracy: 0.6761 - val_loss: 9.9247 - val_accuracy: 0.6730\n",
      "Epoch 714/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 17.7411 - accuracy: 0.6619 - val_loss: 25.0153 - val_accuracy: 0.6792\n",
      "Epoch 715/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 18.3831 - accuracy: 0.6682 - val_loss: 22.6618 - val_accuracy: 0.6541\n",
      "Epoch 716/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 18.7626 - accuracy: 0.6509 - val_loss: 16.0574 - val_accuracy: 0.6478\n",
      "Epoch 717/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.9887 - accuracy: 0.6777 - val_loss: 24.7637 - val_accuracy: 0.6541\n",
      "Epoch 718/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 19.4095 - accuracy: 0.6981 - val_loss: 17.7053 - val_accuracy: 0.6541\n",
      "Epoch 719/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 18.6823 - accuracy: 0.6305 - val_loss: 25.5040 - val_accuracy: 0.6101\n",
      "Epoch 720/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 18.9354 - accuracy: 0.6792 - val_loss: 21.0510 - val_accuracy: 0.6164\n",
      "Epoch 721/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 18.2535 - accuracy: 0.6808 - val_loss: 21.3127 - val_accuracy: 0.6730\n",
      "Epoch 722/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 18.8790 - accuracy: 0.7013 - val_loss: 15.2298 - val_accuracy: 0.6792\n",
      "Epoch 723/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.5148 - accuracy: 0.6918 - val_loss: 20.0584 - val_accuracy: 0.5786\n",
      "Epoch 724/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.3459 - accuracy: 0.6415 - val_loss: 20.6436 - val_accuracy: 0.6352\n",
      "Epoch 725/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 19.1387 - accuracy: 0.6792 - val_loss: 14.6931 - val_accuracy: 0.6478\n",
      "Epoch 726/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 17.2838 - accuracy: 0.6619 - val_loss: 20.2551 - val_accuracy: 0.6918\n",
      "Epoch 727/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 16.3033 - accuracy: 0.7075 - val_loss: 21.6043 - val_accuracy: 0.5660\n",
      "Epoch 728/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 21.0842 - accuracy: 0.718 - 0s 41us/step - loss: 19.0073 - accuracy: 0.6934 - val_loss: 15.6137 - val_accuracy: 0.6289\n",
      "Epoch 729/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 18.0455 - accuracy: 0.6777 - val_loss: 29.8997 - val_accuracy: 0.6792\n",
      "Epoch 730/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 18.3264 - accuracy: 0.7013 - val_loss: 20.0854 - val_accuracy: 0.6038\n",
      "Epoch 731/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 18.5841 - accuracy: 0.6965 - val_loss: 21.2282 - val_accuracy: 0.6604\n",
      "Epoch 732/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 18.2317 - accuracy: 0.6730 - val_loss: 28.2896 - val_accuracy: 0.6352\n",
      "Epoch 733/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 16.5452 - accuracy: 0.6918 - val_loss: 15.9294 - val_accuracy: 0.6792\n",
      "Epoch 734/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 18.4414 - accuracy: 0.6808 - val_loss: 19.2077 - val_accuracy: 0.6855\n",
      "Epoch 735/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.1721 - accuracy: 0.6179 - val_loss: 11.7577 - val_accuracy: 0.5220\n",
      "Epoch 736/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 17.8223 - accuracy: 0.6777 - val_loss: 9.9311 - val_accuracy: 0.6415\n",
      "Epoch 737/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 17.0458 - accuracy: 0.6965 - val_loss: 25.7775 - val_accuracy: 0.5723\n",
      "Epoch 738/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.8525 - accuracy: 0.7075 - val_loss: 16.0429 - val_accuracy: 0.7107\n",
      "Epoch 739/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 18.3818 - accuracy: 0.7028 - val_loss: 25.0774 - val_accuracy: 0.6855\n",
      "Epoch 740/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.2478 - accuracy: 0.6840 - val_loss: 18.2713 - val_accuracy: 0.6352\n",
      "Epoch 741/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 18.4477 - accuracy: 0.7091 - val_loss: 9.4023 - val_accuracy: 0.6981\n",
      "Epoch 742/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.3258 - accuracy: 0.6698 - val_loss: 14.1408 - val_accuracy: 0.5283\n",
      "Epoch 743/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 18.4071 - accuracy: 0.7028 - val_loss: 9.6209 - val_accuracy: 0.7170\n",
      "Epoch 744/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 17.1779 - accuracy: 0.6619 - val_loss: 17.0182 - val_accuracy: 0.6918\n",
      "Epoch 745/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 17.8492 - accuracy: 0.6965 - val_loss: 16.3863 - val_accuracy: 0.6792\n",
      "Epoch 746/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 17.9453 - accuracy: 0.6745 - val_loss: 17.0676 - val_accuracy: 0.6730\n",
      "Epoch 747/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.3937 - accuracy: 0.6667 - val_loss: 26.0097 - val_accuracy: 0.5849\n",
      "Epoch 748/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 18.6253 - accuracy: 0.7075 - val_loss: 11.7644 - val_accuracy: 0.6541\n",
      "Epoch 749/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.4365 - accuracy: 0.6478 - val_loss: 17.0983 - val_accuracy: 0.5786\n",
      "Epoch 750/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 18.6158 - accuracy: 0.6824 - val_loss: 15.5107 - val_accuracy: 0.6289\n",
      "Epoch 751/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 17.5695 - accuracy: 0.6934 - val_loss: 23.5116 - val_accuracy: 0.7044\n",
      "Epoch 752/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.5150 - accuracy: 0.6918 - val_loss: 11.9050 - val_accuracy: 0.5094\n",
      "Epoch 753/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 18.0726 - accuracy: 0.6588 - val_loss: 12.2489 - val_accuracy: 0.4654\n",
      "Epoch 754/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 16.6379 - accuracy: 0.6038 - val_loss: 21.0131 - val_accuracy: 0.7358\n",
      "Epoch 755/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 18.9795 - accuracy: 0.7358 - val_loss: 11.4230 - val_accuracy: 0.6415\n",
      "Epoch 756/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.4162 - accuracy: 0.7201 - val_loss: 19.0704 - val_accuracy: 0.6541\n",
      "Epoch 757/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 18.1913 - accuracy: 0.6887 - val_loss: 22.1900 - val_accuracy: 0.6289\n",
      "Epoch 758/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 18.5088 - accuracy: 0.6478 - val_loss: 21.3929 - val_accuracy: 0.5849\n",
      "Epoch 759/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 18.2580 - accuracy: 0.6336 - val_loss: 19.6882 - val_accuracy: 0.5723\n",
      "Epoch 760/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 21.0208 - accuracy: 0.406 - 0s 35us/step - loss: 17.7304 - accuracy: 0.6698 - val_loss: 20.4589 - val_accuracy: 0.6478\n",
      "Epoch 761/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.4708 - accuracy: 0.6997 - val_loss: 14.9989 - val_accuracy: 0.6604\n",
      "Epoch 762/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.5806 - accuracy: 0.6918 - val_loss: 18.4537 - val_accuracy: 0.4717\n",
      "Epoch 763/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 17.1970 - accuracy: 0.7311 - val_loss: 22.2964 - val_accuracy: 0.6855\n",
      "Epoch 764/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 18.7070 - accuracy: 0.6997 - val_loss: 20.6640 - val_accuracy: 0.5597\n",
      "Epoch 765/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.6311 - accuracy: 0.7091 - val_loss: 11.1509 - val_accuracy: 0.6226\n",
      "Epoch 766/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 17.0018 - accuracy: 0.6792 - val_loss: 11.0623 - val_accuracy: 0.6855\n",
      "Epoch 767/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 17.3923 - accuracy: 0.6918 - val_loss: 21.3486 - val_accuracy: 0.6541\n",
      "Epoch 768/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.1433 - accuracy: 0.6368 - val_loss: 13.3902 - val_accuracy: 0.6541\n",
      "Epoch 769/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 17.3407 - accuracy: 0.6604 - val_loss: 19.9403 - val_accuracy: 0.5157\n",
      "Epoch 770/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 17.4305 - accuracy: 0.6541 - val_loss: 17.5136 - val_accuracy: 0.5912\n",
      "Epoch 771/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 16.7754 - accuracy: 0.6730 - val_loss: 18.2161 - val_accuracy: 0.6792\n",
      "Epoch 772/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 17.4056 - accuracy: 0.7044 - val_loss: 16.5999 - val_accuracy: 0.6981\n",
      "Epoch 773/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 15.4051 - accuracy: 0.7217 - val_loss: 9.8892 - val_accuracy: 0.7170\n",
      "Epoch 774/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.8529 - accuracy: 0.7044 - val_loss: 15.4493 - val_accuracy: 0.6667\n",
      "Epoch 775/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.0047 - accuracy: 0.6871 - val_loss: 10.4198 - val_accuracy: 0.5723\n",
      "Epoch 776/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 16.5993 - accuracy: 0.6808 - val_loss: 27.2328 - val_accuracy: 0.6101\n",
      "Epoch 777/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 18.0239 - accuracy: 0.6950 - val_loss: 13.5974 - val_accuracy: 0.6667\n",
      "Epoch 778/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 17.9243 - accuracy: 0.6950 - val_loss: 17.7108 - val_accuracy: 0.6667\n",
      "Epoch 779/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 18.6433 - accuracy: 0.7060 - val_loss: 15.5845 - val_accuracy: 0.6415\n",
      "Epoch 780/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 16.5500 - accuracy: 0.6950 - val_loss: 30.9218 - val_accuracy: 0.6415\n",
      "Epoch 781/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.3311 - accuracy: 0.7013 - val_loss: 19.7689 - val_accuracy: 0.6604\n",
      "Epoch 782/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 17.3826 - accuracy: 0.6667 - val_loss: 30.9755 - val_accuracy: 0.5975\n",
      "Epoch 783/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.9968 - accuracy: 0.6667 - val_loss: 11.5391 - val_accuracy: 0.5975\n",
      "Epoch 784/1000\n",
      "636/636 [==============================] - 0s 31us/step - loss: 18.0141 - accuracy: 0.6619 - val_loss: 24.1800 - val_accuracy: 0.5786\n",
      "Epoch 785/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.5523 - accuracy: 0.6635 - val_loss: 18.7759 - val_accuracy: 0.6730\n",
      "Epoch 786/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 17.2033 - accuracy: 0.6808 - val_loss: 12.1758 - val_accuracy: 0.6792\n",
      "Epoch 787/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.1353 - accuracy: 0.7107 - val_loss: 23.3051 - val_accuracy: 0.6415\n",
      "Epoch 788/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.9817 - accuracy: 0.6777 - val_loss: 13.1217 - val_accuracy: 0.6352\n",
      "Epoch 789/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 17.6799 - accuracy: 0.6792 - val_loss: 24.0131 - val_accuracy: 0.6478\n",
      "Epoch 790/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.9794 - accuracy: 0.7060 - val_loss: 10.0289 - val_accuracy: 0.6478\n",
      "Epoch 791/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 15.1403 - accuracy: 0.6572 - val_loss: 10.2640 - val_accuracy: 0.6730\n",
      "Epoch 792/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.7682 - accuracy: 0.6651 - val_loss: 11.6229 - val_accuracy: 0.7170\n",
      "Epoch 793/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.6006 - accuracy: 0.6462 - val_loss: 13.2143 - val_accuracy: 0.6541\n",
      "Epoch 794/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 17.6496 - accuracy: 0.5991 - val_loss: 16.2904 - val_accuracy: 0.5597\n",
      "Epoch 795/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 16.4775 - accuracy: 0.6478 - val_loss: 21.6281 - val_accuracy: 0.5660\n",
      "Epoch 796/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.3291 - accuracy: 0.6840 - val_loss: 14.6269 - val_accuracy: 0.4906\n",
      "Epoch 797/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 15.8919 - accuracy: 0.6698 - val_loss: 14.5062 - val_accuracy: 0.6164\n",
      "Epoch 798/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 16.8729 - accuracy: 0.6336 - val_loss: 18.3451 - val_accuracy: 0.6667\n",
      "Epoch 799/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 17.7417 - accuracy: 0.6730 - val_loss: 21.0858 - val_accuracy: 0.6541\n",
      "Epoch 800/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 17.8050 - accuracy: 0.6462 - val_loss: 22.9495 - val_accuracy: 0.6541\n",
      "Epoch 801/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 17.0828 - accuracy: 0.6887 - val_loss: 17.0808 - val_accuracy: 0.6352\n",
      "Epoch 802/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 17.7323 - accuracy: 0.6651 - val_loss: 12.0200 - val_accuracy: 0.6667\n",
      "Epoch 803/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 18.0375 - accuracy: 0.7091 - val_loss: 9.9166 - val_accuracy: 0.5723\n",
      "Epoch 804/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 16.4564 - accuracy: 0.6808 - val_loss: 21.7850 - val_accuracy: 0.6604\n",
      "Epoch 805/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.2214 - accuracy: 0.6808 - val_loss: 11.2368 - val_accuracy: 0.5346\n",
      "Epoch 806/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 16.7130 - accuracy: 0.7296 - val_loss: 20.3233 - val_accuracy: 0.6541\n",
      "Epoch 807/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.8851 - accuracy: 0.6572 - val_loss: 20.4468 - val_accuracy: 0.6164\n",
      "Epoch 808/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 17.2829 - accuracy: 0.6918 - val_loss: 19.1701 - val_accuracy: 0.6792\n",
      "Epoch 809/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 17.5742 - accuracy: 0.6997 - val_loss: 10.6200 - val_accuracy: 0.6541\n",
      "Epoch 810/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 17.0578 - accuracy: 0.6808 - val_loss: 22.7492 - val_accuracy: 0.6164\n",
      "Epoch 811/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 15.1780 - accuracy: 0.6840 - val_loss: 10.2165 - val_accuracy: 0.5849\n",
      "Epoch 812/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.8414 - accuracy: 0.6871 - val_loss: 14.2317 - val_accuracy: 0.6918\n",
      "Epoch 813/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 16.6526 - accuracy: 0.6777 - val_loss: 13.5470 - val_accuracy: 0.6667\n",
      "Epoch 814/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 16.9091 - accuracy: 0.7123 - val_loss: 13.7388 - val_accuracy: 0.6604\n",
      "Epoch 815/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 16.9512 - accuracy: 0.6934 - val_loss: 21.9335 - val_accuracy: 0.6604\n",
      "Epoch 816/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 17.6095 - accuracy: 0.6808 - val_loss: 8.4420 - val_accuracy: 0.5723\n",
      "Epoch 817/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 16.2964 - accuracy: 0.6431 - val_loss: 17.7407 - val_accuracy: 0.5723\n",
      "Epoch 818/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 17.3929 - accuracy: 0.7013 - val_loss: 22.0589 - val_accuracy: 0.6289\n",
      "Epoch 819/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.2356 - accuracy: 0.6714 - val_loss: 11.7850 - val_accuracy: 0.5346\n",
      "Epoch 820/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.4191 - accuracy: 0.7138 - val_loss: 19.5517 - val_accuracy: 0.5723\n",
      "Epoch 821/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.8129 - accuracy: 0.6887 - val_loss: 13.7796 - val_accuracy: 0.6541\n",
      "Epoch 822/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.6237 - accuracy: 0.6934 - val_loss: 23.8495 - val_accuracy: 0.6352\n",
      "Epoch 823/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 17.1594 - accuracy: 0.7358 - val_loss: 14.6229 - val_accuracy: 0.6164\n",
      "Epoch 824/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 17.6383 - accuracy: 0.6792 - val_loss: 10.7310 - val_accuracy: 0.5723\n",
      "Epoch 825/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.5875 - accuracy: 0.6903 - val_loss: 20.2273 - val_accuracy: 0.6164\n",
      "Epoch 826/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 17.7401 - accuracy: 0.6777 - val_loss: 11.5700 - val_accuracy: 0.5723\n",
      "Epoch 827/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 15.6489 - accuracy: 0.7264 - val_loss: 25.5781 - val_accuracy: 0.6226\n",
      "Epoch 828/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 17.0322 - accuracy: 0.7280 - val_loss: 21.3509 - val_accuracy: 0.7044\n",
      "Epoch 829/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.0630 - accuracy: 0.7060 - val_loss: 21.6407 - val_accuracy: 0.6289\n",
      "Epoch 830/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.0764 - accuracy: 0.7107 - val_loss: 11.1303 - val_accuracy: 0.6730\n",
      "Epoch 831/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 16.1154 - accuracy: 0.6997 - val_loss: 17.8902 - val_accuracy: 0.6730\n",
      "Epoch 832/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.4659 - accuracy: 0.6997 - val_loss: 14.5189 - val_accuracy: 0.6667\n",
      "Epoch 833/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.5743 - accuracy: 0.6777 - val_loss: 15.1090 - val_accuracy: 0.6289\n",
      "Epoch 834/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 14.9010 - accuracy: 0.750 - 0s 36us/step - loss: 16.4285 - accuracy: 0.7123 - val_loss: 23.2294 - val_accuracy: 0.6730\n",
      "Epoch 835/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 15.6070 - accuracy: 0.7013 - val_loss: 10.8778 - val_accuracy: 0.7044\n",
      "Epoch 836/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 16.5654 - accuracy: 0.6934 - val_loss: 12.0832 - val_accuracy: 0.5597\n",
      "Epoch 837/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.5906 - accuracy: 0.6855 - val_loss: 18.3687 - val_accuracy: 0.6415\n",
      "Epoch 838/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.5483 - accuracy: 0.6965 - val_loss: 15.6408 - val_accuracy: 0.7170\n",
      "Epoch 839/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 17.4349 - accuracy: 0.7327 - val_loss: 15.7685 - val_accuracy: 0.6541\n",
      "Epoch 840/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 16.5040 - accuracy: 0.7013 - val_loss: 13.5619 - val_accuracy: 0.6164\n",
      "Epoch 841/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 16.6028 - accuracy: 0.6840 - val_loss: 22.3817 - val_accuracy: 0.4969\n",
      "Epoch 842/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 17.3949 - accuracy: 0.6698 - val_loss: 20.4555 - val_accuracy: 0.5786\n",
      "Epoch 843/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 17.1627 - accuracy: 0.7170 - val_loss: 18.0279 - val_accuracy: 0.6164\n",
      "Epoch 844/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.5756 - accuracy: 0.7123 - val_loss: 20.7698 - val_accuracy: 0.6855\n",
      "Epoch 845/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 16.1561 - accuracy: 0.7170 - val_loss: 25.8281 - val_accuracy: 0.5660\n",
      "Epoch 846/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 16.5273 - accuracy: 0.7186 - val_loss: 16.9937 - val_accuracy: 0.6415\n",
      "Epoch 847/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 16.0322 - accuracy: 0.6981 - val_loss: 16.9130 - val_accuracy: 0.3082\n",
      "Epoch 848/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 16.7735 - accuracy: 0.5723 - val_loss: 17.9951 - val_accuracy: 0.6918\n",
      "Epoch 849/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 17.2157 - accuracy: 0.6981 - val_loss: 18.7965 - val_accuracy: 0.6792\n",
      "Epoch 850/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 15.0612 - accuracy: 0.7107 - val_loss: 8.1913 - val_accuracy: 0.6792\n",
      "Epoch 851/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 15.8863 - accuracy: 0.7186 - val_loss: 12.6991 - val_accuracy: 0.6038\n",
      "Epoch 852/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 15.8359 - accuracy: 0.7107 - val_loss: 20.0941 - val_accuracy: 0.6792\n",
      "Epoch 853/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.6970 - accuracy: 0.6682 - val_loss: 22.8984 - val_accuracy: 0.6730\n",
      "Epoch 854/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 17.3312 - accuracy: 0.6305 - val_loss: 16.4218 - val_accuracy: 0.5472\n",
      "Epoch 855/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 16.4002 - accuracy: 0.6855 - val_loss: 12.5949 - val_accuracy: 0.6541\n",
      "Epoch 856/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 16.8928 - accuracy: 0.6761 - val_loss: 28.4454 - val_accuracy: 0.5409\n",
      "Epoch 857/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 17.0601 - accuracy: 0.7060 - val_loss: 21.5114 - val_accuracy: 0.6352\n",
      "Epoch 858/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 16.4205 - accuracy: 0.7123 - val_loss: 20.2596 - val_accuracy: 0.5975\n",
      "Epoch 859/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 16.3549 - accuracy: 0.6903 - val_loss: 8.5440 - val_accuracy: 0.6289\n",
      "Epoch 860/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 16.3461 - accuracy: 0.6321 - val_loss: 14.1350 - val_accuracy: 0.6730\n",
      "Epoch 861/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 16.5372 - accuracy: 0.7390 - val_loss: 9.6452 - val_accuracy: 0.5723\n",
      "Epoch 862/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.2534 - accuracy: 0.6682 - val_loss: 14.6543 - val_accuracy: 0.5660\n",
      "Epoch 863/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 14.8170 - accuracy: 0.6667 - val_loss: 22.2916 - val_accuracy: 0.6289\n",
      "Epoch 864/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.5447 - accuracy: 0.6541 - val_loss: 13.4631 - val_accuracy: 0.6604\n",
      "Epoch 865/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.6422 - accuracy: 0.6871 - val_loss: 19.2978 - val_accuracy: 0.6604\n",
      "Epoch 866/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 16.4555 - accuracy: 0.6792 - val_loss: 13.9576 - val_accuracy: 0.6604\n",
      "Epoch 867/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 17.1320 - accuracy: 0.7170 - val_loss: 10.8927 - val_accuracy: 0.5786\n",
      "Epoch 868/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 15.7087 - accuracy: 0.6509 - val_loss: 16.0157 - val_accuracy: 0.6604\n",
      "Epoch 869/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.8798 - accuracy: 0.6714 - val_loss: 12.0396 - val_accuracy: 0.6730\n",
      "Epoch 870/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.7367 - accuracy: 0.6855 - val_loss: 20.7969 - val_accuracy: 0.6918\n",
      "Epoch 871/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 16.3341 - accuracy: 0.6777 - val_loss: 20.1659 - val_accuracy: 0.6792\n",
      "Epoch 872/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 15.6138 - accuracy: 0.7296 - val_loss: 18.9719 - val_accuracy: 0.6792\n",
      "Epoch 873/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 16.7604 - accuracy: 0.7123 - val_loss: 14.0118 - val_accuracy: 0.6415\n",
      "Epoch 874/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.2423 - accuracy: 0.7217 - val_loss: 9.9814 - val_accuracy: 0.6541\n",
      "Epoch 875/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 16.3998 - accuracy: 0.6840 - val_loss: 11.6034 - val_accuracy: 0.4969\n",
      "Epoch 876/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 14.9086 - accuracy: 0.6588 - val_loss: 19.4011 - val_accuracy: 0.5597\n",
      "Epoch 877/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.7322 - accuracy: 0.6714 - val_loss: 11.8357 - val_accuracy: 0.5723\n",
      "Epoch 878/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.2593 - accuracy: 0.6714 - val_loss: 11.9201 - val_accuracy: 0.5975\n",
      "Epoch 879/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 16.5362 - accuracy: 0.6918 - val_loss: 11.1347 - val_accuracy: 0.6541\n",
      "Epoch 880/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 15.8503 - accuracy: 0.6997 - val_loss: 15.5825 - val_accuracy: 0.6792\n",
      "Epoch 881/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 14.3172 - accuracy: 0.843 - 0s 38us/step - loss: 15.5042 - accuracy: 0.6903 - val_loss: 26.8240 - val_accuracy: 0.6289\n",
      "Epoch 882/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 16.8323 - accuracy: 0.7060 - val_loss: 18.2304 - val_accuracy: 0.6730\n",
      "Epoch 883/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 16.0650 - accuracy: 0.6918 - val_loss: 25.3978 - val_accuracy: 0.5912\n",
      "Epoch 884/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 14.7282 - accuracy: 0.6918 - val_loss: 8.6882 - val_accuracy: 0.6918\n",
      "Epoch 885/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 15.8939 - accuracy: 0.7170 - val_loss: 15.8989 - val_accuracy: 0.6855\n",
      "Epoch 886/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 17.2417 - accuracy: 0.7248 - val_loss: 11.9786 - val_accuracy: 0.6415\n",
      "Epoch 887/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 16.6954 - accuracy: 0.6824 - val_loss: 20.8548 - val_accuracy: 0.6038\n",
      "Epoch 888/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 17.0803 - accuracy: 0.6698 - val_loss: 11.5317 - val_accuracy: 0.6981\n",
      "Epoch 889/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.2237 - accuracy: 0.7453 - val_loss: 23.6585 - val_accuracy: 0.5786\n",
      "Epoch 890/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.9433 - accuracy: 0.7138 - val_loss: 18.6308 - val_accuracy: 0.6730\n",
      "Epoch 891/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 16.6611 - accuracy: 0.7060 - val_loss: 15.8716 - val_accuracy: 0.6164\n",
      "Epoch 892/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 14.8775 - accuracy: 0.6431 - val_loss: 19.6603 - val_accuracy: 0.6792\n",
      "Epoch 893/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 15.6528 - accuracy: 0.7170 - val_loss: 17.9401 - val_accuracy: 0.6415\n",
      "Epoch 894/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 16.0023 - accuracy: 0.6336 - val_loss: 12.2023 - val_accuracy: 0.6226\n",
      "Epoch 895/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.2169 - accuracy: 0.6572 - val_loss: 12.5502 - val_accuracy: 0.6792\n",
      "Epoch 896/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 15.8276 - accuracy: 0.7028 - val_loss: 20.6350 - val_accuracy: 0.6226\n",
      "Epoch 897/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.3581 - accuracy: 0.6840 - val_loss: 18.8436 - val_accuracy: 0.6792\n",
      "Epoch 898/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.3230 - accuracy: 0.6824 - val_loss: 14.8629 - val_accuracy: 0.7044\n",
      "Epoch 899/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 15.5773 - accuracy: 0.6871 - val_loss: 9.2871 - val_accuracy: 0.6604\n",
      "Epoch 900/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 13.9994 - accuracy: 0.7233 - val_loss: 8.7426 - val_accuracy: 0.6352\n",
      "Epoch 901/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 16.2877 - accuracy: 0.7060 - val_loss: 16.8321 - val_accuracy: 0.5723\n",
      "Epoch 902/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 15.9086 - accuracy: 0.6698 - val_loss: 10.6103 - val_accuracy: 0.6541\n",
      "Epoch 903/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 15.4325 - accuracy: 0.6855 - val_loss: 20.5867 - val_accuracy: 0.5723\n",
      "Epoch 904/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.2328 - accuracy: 0.6887 - val_loss: 24.3521 - val_accuracy: 0.5723\n",
      "Epoch 905/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 16.5656 - accuracy: 0.6525 - val_loss: 15.8257 - val_accuracy: 0.6541\n",
      "Epoch 906/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 16.3557 - accuracy: 0.6840 - val_loss: 17.0338 - val_accuracy: 0.6038\n",
      "Epoch 907/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 16.1327 - accuracy: 0.7201 - val_loss: 14.9325 - val_accuracy: 0.5723\n",
      "Epoch 908/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.5653 - accuracy: 0.6572 - val_loss: 8.0592 - val_accuracy: 0.5786\n",
      "Epoch 909/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.4339 - accuracy: 0.7091 - val_loss: 21.9179 - val_accuracy: 0.6855\n",
      "Epoch 910/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.0650 - accuracy: 0.6918 - val_loss: 12.6909 - val_accuracy: 0.6352\n",
      "Epoch 911/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 15.9887 - accuracy: 0.6494 - val_loss: 18.2112 - val_accuracy: 0.6415\n",
      "Epoch 912/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 15.9212 - accuracy: 0.6352 - val_loss: 14.6845 - val_accuracy: 0.6918\n",
      "Epoch 913/1000\n",
      "636/636 [==============================] - 0s 34us/step - loss: 15.9639 - accuracy: 0.6965 - val_loss: 20.4463 - val_accuracy: 0.5472\n",
      "Epoch 914/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.4691 - accuracy: 0.6651 - val_loss: 12.7228 - val_accuracy: 0.5472\n",
      "Epoch 915/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 15.2805 - accuracy: 0.6730 - val_loss: 16.2874 - val_accuracy: 0.6352\n",
      "Epoch 916/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 16.5486 - accuracy: 0.7138 - val_loss: 7.9359 - val_accuracy: 0.5472\n",
      "Epoch 917/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 14.5987 - accuracy: 0.7013 - val_loss: 18.1972 - val_accuracy: 0.6038\n",
      "Epoch 918/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.8104 - accuracy: 0.6588 - val_loss: 15.6741 - val_accuracy: 0.6792\n",
      "Epoch 919/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.2980 - accuracy: 0.6871 - val_loss: 13.1392 - val_accuracy: 0.6792\n",
      "Epoch 920/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 15.7745 - accuracy: 0.6714 - val_loss: 14.8984 - val_accuracy: 0.5660\n",
      "Epoch 921/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.2208 - accuracy: 0.6950 - val_loss: 20.9229 - val_accuracy: 0.6289\n",
      "Epoch 922/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 16.0786 - accuracy: 0.6965 - val_loss: 17.1218 - val_accuracy: 0.6352\n",
      "Epoch 923/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 15.8426 - accuracy: 0.7123 - val_loss: 14.6245 - val_accuracy: 0.5157\n",
      "Epoch 924/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.1408 - accuracy: 0.6871 - val_loss: 22.3102 - val_accuracy: 0.6730\n",
      "Epoch 925/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 15.0016 - accuracy: 0.6934 - val_loss: 12.0512 - val_accuracy: 0.6918\n",
      "Epoch 926/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.2534 - accuracy: 0.6855 - val_loss: 19.9673 - val_accuracy: 0.6038\n",
      "Epoch 927/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 16.0691 - accuracy: 0.6698 - val_loss: 19.5291 - val_accuracy: 0.6164\n",
      "Epoch 928/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 15.0036 - accuracy: 0.7154 - val_loss: 22.0909 - val_accuracy: 0.6038\n",
      "Epoch 929/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 15.7180 - accuracy: 0.6352 - val_loss: 18.7811 - val_accuracy: 0.6415\n",
      "Epoch 930/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.9217 - accuracy: 0.7091 - val_loss: 17.7939 - val_accuracy: 0.6415\n",
      "Epoch 931/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 16.0312 - accuracy: 0.7217 - val_loss: 14.0825 - val_accuracy: 0.5975\n",
      "Epoch 932/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 14.2747 - accuracy: 0.7154 - val_loss: 20.2230 - val_accuracy: 0.6855\n",
      "Epoch 933/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.8339 - accuracy: 0.7233 - val_loss: 24.1332 - val_accuracy: 0.6981\n",
      "Epoch 934/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.9769 - accuracy: 0.6698 - val_loss: 23.3027 - val_accuracy: 0.6226\n",
      "Epoch 935/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 12.6791 - accuracy: 0.6997 - val_loss: 11.7308 - val_accuracy: 0.6038\n",
      "Epoch 936/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 16.3467 - accuracy: 0.7123 - val_loss: 9.2610 - val_accuracy: 0.6541\n",
      "Epoch 937/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 15.4491 - accuracy: 0.6792 - val_loss: 13.3774 - val_accuracy: 0.6667\n",
      "Epoch 938/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 15.4320 - accuracy: 0.6792 - val_loss: 20.6896 - val_accuracy: 0.6226\n",
      "Epoch 939/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.3982 - accuracy: 0.6305 - val_loss: 13.6096 - val_accuracy: 0.6667\n",
      "Epoch 940/1000\n",
      "636/636 [==============================] - 0s 46us/step - loss: 15.6011 - accuracy: 0.7013 - val_loss: 15.6608 - val_accuracy: 0.6478\n",
      "Epoch 941/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 15.6899 - accuracy: 0.7374 - val_loss: 14.9590 - val_accuracy: 0.5975\n",
      "Epoch 942/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 16.0847 - accuracy: 0.6761 - val_loss: 24.1856 - val_accuracy: 0.6415\n",
      "Epoch 943/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 16.1763 - accuracy: 0.6981 - val_loss: 10.7833 - val_accuracy: 0.6478\n",
      "Epoch 944/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 15.2648 - accuracy: 0.7013 - val_loss: 8.7077 - val_accuracy: 0.5409\n",
      "Epoch 945/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 15.0995 - accuracy: 0.6950 - val_loss: 21.7217 - val_accuracy: 0.6667\n",
      "Epoch 946/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 14.9771 - accuracy: 0.7138 - val_loss: 19.7179 - val_accuracy: 0.6164\n",
      "Epoch 947/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 16.1324 - accuracy: 0.7186 - val_loss: 13.6237 - val_accuracy: 0.6415\n",
      "Epoch 948/1000\n",
      "636/636 [==============================] - ETA: 0s - loss: 13.5208 - accuracy: 0.718 - 0s 42us/step - loss: 15.4373 - accuracy: 0.6667 - val_loss: 14.3635 - val_accuracy: 0.6415\n",
      "Epoch 949/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 14.9552 - accuracy: 0.6777 - val_loss: 21.1779 - val_accuracy: 0.6541\n",
      "Epoch 950/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.2095 - accuracy: 0.6730 - val_loss: 16.2933 - val_accuracy: 0.5660\n",
      "Epoch 951/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 15.7012 - accuracy: 0.6478 - val_loss: 17.7576 - val_accuracy: 0.5346\n",
      "Epoch 952/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 16.1631 - accuracy: 0.6415 - val_loss: 8.8266 - val_accuracy: 0.5472\n",
      "Epoch 953/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.2223 - accuracy: 0.6997 - val_loss: 24.9976 - val_accuracy: 0.5409\n",
      "Epoch 954/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.9267 - accuracy: 0.7248 - val_loss: 10.2212 - val_accuracy: 0.6289\n",
      "Epoch 955/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.5377 - accuracy: 0.7500 - val_loss: 13.2560 - val_accuracy: 0.6352\n",
      "Epoch 956/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.6062 - accuracy: 0.6918 - val_loss: 19.3142 - val_accuracy: 0.6415\n",
      "Epoch 957/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 16.2130 - accuracy: 0.7264 - val_loss: 8.3494 - val_accuracy: 0.6541\n",
      "Epoch 958/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 14.8540 - accuracy: 0.6918 - val_loss: 24.6469 - val_accuracy: 0.7170\n",
      "Epoch 959/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 16.7012 - accuracy: 0.6997 - val_loss: 14.9794 - val_accuracy: 0.6981\n",
      "Epoch 960/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.2800 - accuracy: 0.7406 - val_loss: 15.9451 - val_accuracy: 0.6604\n",
      "Epoch 961/1000\n",
      "636/636 [==============================] - 0s 35us/step - loss: 14.9302 - accuracy: 0.6887 - val_loss: 20.5541 - val_accuracy: 0.7044\n",
      "Epoch 962/1000\n",
      "636/636 [==============================] - 0s 33us/step - loss: 15.8320 - accuracy: 0.6903 - val_loss: 23.7696 - val_accuracy: 0.6415\n",
      "Epoch 963/1000\n",
      "636/636 [==============================] - 0s 36us/step - loss: 15.7804 - accuracy: 0.6651 - val_loss: 14.5741 - val_accuracy: 0.6792\n",
      "Epoch 964/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 14.9460 - accuracy: 0.6981 - val_loss: 18.5792 - val_accuracy: 0.6038\n",
      "Epoch 965/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 15.2040 - accuracy: 0.7028 - val_loss: 19.2163 - val_accuracy: 0.5597\n",
      "Epoch 966/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.4193 - accuracy: 0.6997 - val_loss: 10.9856 - val_accuracy: 0.5597\n",
      "Epoch 967/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 15.1718 - accuracy: 0.7186 - val_loss: 20.9551 - val_accuracy: 0.5597\n",
      "Epoch 968/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 16.0586 - accuracy: 0.7138 - val_loss: 22.7450 - val_accuracy: 0.6352\n",
      "Epoch 969/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.9952 - accuracy: 0.6965 - val_loss: 21.6813 - val_accuracy: 0.6667\n",
      "Epoch 970/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 14.7681 - accuracy: 0.7500 - val_loss: 22.2803 - val_accuracy: 0.6981\n",
      "Epoch 971/1000\n",
      "636/636 [==============================] - 0s 49us/step - loss: 15.1110 - accuracy: 0.6918 - val_loss: 14.5141 - val_accuracy: 0.5912\n",
      "Epoch 972/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 14.9956 - accuracy: 0.6840 - val_loss: 12.6474 - val_accuracy: 0.6792\n",
      "Epoch 973/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.5068 - accuracy: 0.6918 - val_loss: 12.0503 - val_accuracy: 0.6478\n",
      "Epoch 974/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 14.0907 - accuracy: 0.7233 - val_loss: 14.4609 - val_accuracy: 0.6855\n",
      "Epoch 975/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.0437 - accuracy: 0.6997 - val_loss: 10.0142 - val_accuracy: 0.6415\n",
      "Epoch 976/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.5491 - accuracy: 0.7280 - val_loss: 18.5325 - val_accuracy: 0.6289\n",
      "Epoch 977/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 14.0033 - accuracy: 0.6761 - val_loss: 16.9036 - val_accuracy: 0.6289\n",
      "Epoch 978/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 14.8095 - accuracy: 0.7248 - val_loss: 23.4605 - val_accuracy: 0.6855\n",
      "Epoch 979/1000\n",
      "636/636 [==============================] - 0s 38us/step - loss: 16.4040 - accuracy: 0.7531 - val_loss: 8.4100 - val_accuracy: 0.7044\n",
      "Epoch 980/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.1143 - accuracy: 0.6619 - val_loss: 12.8386 - val_accuracy: 0.6478\n",
      "Epoch 981/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 15.2901 - accuracy: 0.7296 - val_loss: 16.8904 - val_accuracy: 0.5912\n",
      "Epoch 982/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.5724 - accuracy: 0.7358 - val_loss: 9.5194 - val_accuracy: 0.6352\n",
      "Epoch 983/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 16.2333 - accuracy: 0.7060 - val_loss: 10.9887 - val_accuracy: 0.7233\n",
      "Epoch 984/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.2951 - accuracy: 0.7311 - val_loss: 16.8793 - val_accuracy: 0.6415\n",
      "Epoch 985/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.9637 - accuracy: 0.7296 - val_loss: 9.8221 - val_accuracy: 0.6226\n",
      "Epoch 986/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 14.8946 - accuracy: 0.7280 - val_loss: 18.5492 - val_accuracy: 0.6038\n",
      "Epoch 987/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 16.0051 - accuracy: 0.7107 - val_loss: 10.5589 - val_accuracy: 0.6289\n",
      "Epoch 988/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 14.2357 - accuracy: 0.6934 - val_loss: 8.2607 - val_accuracy: 0.6226\n",
      "Epoch 989/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 14.3496 - accuracy: 0.7154 - val_loss: 14.5745 - val_accuracy: 0.6667\n",
      "Epoch 990/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.7899 - accuracy: 0.7296 - val_loss: 17.3101 - val_accuracy: 0.6352\n",
      "Epoch 991/1000\n",
      "636/636 [==============================] - 0s 39us/step - loss: 14.8882 - accuracy: 0.6871 - val_loss: 20.3593 - val_accuracy: 0.7044\n",
      "Epoch 992/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 15.3356 - accuracy: 0.7170 - val_loss: 22.9437 - val_accuracy: 0.7107\n",
      "Epoch 993/1000\n",
      "636/636 [==============================] - 0s 41us/step - loss: 14.5886 - accuracy: 0.7138 - val_loss: 14.5167 - val_accuracy: 0.7358\n",
      "Epoch 994/1000\n",
      "636/636 [==============================] - 0s 44us/step - loss: 15.8934 - accuracy: 0.7484 - val_loss: 13.5957 - val_accuracy: 0.6101\n",
      "Epoch 995/1000\n",
      "636/636 [==============================] - 0s 42us/step - loss: 15.2467 - accuracy: 0.6777 - val_loss: 12.3877 - val_accuracy: 0.7107\n",
      "Epoch 996/1000\n",
      "636/636 [==============================] - 0s 45us/step - loss: 14.4303 - accuracy: 0.6934 - val_loss: 14.7736 - val_accuracy: 0.6415\n",
      "Epoch 997/1000\n",
      "636/636 [==============================] - 0s 47us/step - loss: 15.8189 - accuracy: 0.7531 - val_loss: 9.5166 - val_accuracy: 0.6226\n",
      "Epoch 998/1000\n",
      "636/636 [==============================] - 0s 50us/step - loss: 14.9947 - accuracy: 0.6997 - val_loss: 12.6164 - val_accuracy: 0.6981\n",
      "Epoch 999/1000\n",
      "636/636 [==============================] - 0s 58us/step - loss: 15.4766 - accuracy: 0.7217 - val_loss: 13.8981 - val_accuracy: 0.7044\n",
      "Epoch 1000/1000\n",
      "636/636 [==============================] - 0s 55us/step - loss: 15.0761 - accuracy: 0.7233 - val_loss: 14.6907 - val_accuracy: 0.6981\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_test,y_test), epochs=training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3938.39828594   91.7991629  3823.63320293   70.77795305 3711.30403945\n",
      "   78.46771143 3864.70386455   89.46871853 3968.49103253   61.90130712\n",
      " 3705.49172961  184.79185391 3814.45072736  116.1817613  3775.53604743\n",
      "  128.80332016 3993.09488747   91.0887052  3785.55524457  177.51812608\n",
      " 4023.09569408   39.62329453 3894.38311279   94.74970244 3834.750479\n",
      "  158.6257185  3630.57702847  152.64446643 3736.80278395  169.00163762\n",
      " 3959.79927654   88.38690687 3833.36343175   73.86084999 3720.26202803\n",
      "   80.89942707 3875.73527218   89.76332243 3984.65855778   61.87102125\n",
      " 3719.09628146  182.6610984  3820.20357804  120.41394201 3784.76640989\n",
      "  129.26342628 4008.75510365   97.71687554 3794.63708076  176.50269565\n",
      " 4028.530833     41.54125543 3908.45807054   92.69808456 3837.47288001\n",
      "  158.77981372 3639.46159761  148.1763349  3754.47286213  171.42434555\n",
      " 3969.41995256   91.99066113 3848.50261704   76.59341378 3730.22105916\n",
      "   78.53563804 3886.60946468   86.18470208 3984.65855778   61.87102125\n",
      " 3722.52274485  186.69232933 3830.26989855  118.90978416 3803.44184541\n",
      "  131.02679077 4015.53905015   96.82782999 3809.38507215  176.37129624\n",
      " 4044.70568882   44.01962075 3920.69563913   90.90402495 3845.54322148\n",
      "  157.30731544 3651.3314282   149.80779604 3765.70171849  170.39934534\n",
      " 3980.69572463   89.48484072 3855.3103072    80.61440322 3743.45430088\n",
      "   74.76870796 3897.45997993   89.4933509  4009.58342019   62.59780418\n",
      " 3734.64500964  181.49403327 3842.08013509  113.35162726 3808.53776626\n",
      "  130.16187346 4028.61761348   93.61717858 3820.49477232  181.5872197\n",
      " 4051.55683933   40.26039082 3927.26528585   90.45551819 3856.13136414\n",
      "  162.50372893 3668.58161323  148.54238032 3773.19941558  171.40831169\n",
      " 3989.61388184   87.72153692 3866.18887903   77.63306714 3761.68466001\n",
      "   74.25813729 3904.21276076   85.21699804 4024.46298641   67.21638268\n",
      " 3746.06521007  179.40384183 3857.24013973  118.65066486 3815.22486391\n",
      "  135.68940122 4039.09977568   86.44315837 3832.33209991  180.71308617\n",
      " 4067.4245207    47.13589496 3938.65338889   87.84083333 3867.52215816\n",
      "  159.15936666 3674.81216444  152.41563123 3787.42095461  167.35163471]\n",
      "[3967.8213     97.975945 3846.9502     80.87805  3745.78       74.647675\n",
      " 3898.3486     76.56988  3988.1958     58.912273 3708.3057    180.11409\n",
      " 3821.83      129.02773  3791.3047    139.2328   4031.9128    105.69415\n",
      " 3805.391     183.75957  4033.887      41.50951  3899.1768     97.91133\n",
      " 3841.3516    169.55711  3650.054     158.89568  3763.5847    177.39133\n",
      " 3981.4294     90.94883  3862.739      75.10785  3751.2053     78.71534\n",
      " 3904.3206     95.64975  3999.0833     51.392574 3717.5088    174.89062\n",
      " 3832.792     127.85217  3804.389     137.81192  4045.8965    105.223495\n",
      " 3819.6833    186.91513  4044.569      46.774048 3916.6042     97.163765\n",
      " 3861.6765    166.37488  3663.4688    168.68489  3762.2214    176.11937\n",
      " 3989.8237    103.09348  3872.1824     80.32512  3774.8228     91.83777\n",
      " 3921.1646     82.545006 4009.21       58.497272 3725.096     179.85413\n",
      " 3844.8794    132.19394  3811.4421    139.0359   4051.0835    105.70052\n",
      " 3832.0771    182.367    4057.0586     49.027714 3932.8708     96.84124\n",
      " 3875.4778    168.42506  3672.2834    165.02954  3777.625     178.18301\n",
      " 4005.9863    100.215    3888.1694     79.12462  3774.8484     91.61161\n",
      " 3921.7031     82.238    4021.6484     58.893917 3740.18      180.45917\n",
      " 3856.3157    129.95651  3830.2085    140.42683  4062.2727    104.62079\n",
      " 3844.182     185.07419  4064.8716     48.50663  3943.6504     87.576675\n",
      " 3880.7173    170.90811  3675.299     165.71423  3785.107     177.26523\n",
      " 4016.6116     90.69106  3890.9407     76.88997  3798.3132     78.38446\n",
      " 3940.0935     91.23563  4034.0037     59.910553 3750.9736    179.2218\n",
      " 3863.548     132.40474  3835.6284    141.71152  4078.0046     99.267395\n",
      " 3849.8604    183.68858  4084.2434     47.54083  3961.7305     89.61441\n",
      " 3894.2546    171.6985   3701.9907    172.60347  3792.615     181.82199 ]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "print(y_test[0])\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are the results affected if I try to predict on using my prediction as input data? For how many rounds can this be accurate enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "      <th>y_4</th>\n",
       "      <th>y_5</th>\n",
       "      <th>y_6</th>\n",
       "      <th>y_7</th>\n",
       "      <th>y_8</th>\n",
       "      <th>y_9</th>\n",
       "      <th>...</th>\n",
       "      <th>y_41</th>\n",
       "      <th>y_42</th>\n",
       "      <th>y_43</th>\n",
       "      <th>y_44</th>\n",
       "      <th>y_45</th>\n",
       "      <th>y_46</th>\n",
       "      <th>y_47</th>\n",
       "      <th>y_48</th>\n",
       "      <th>y_49</th>\n",
       "      <th>y_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[61.80999150562753, 24.420046719048628, 109.00...</td>\n",
       "      <td>[70.80009877085162, 22.786106233538195, 115.03...</td>\n",
       "      <td>[78.53846153846153, 23.79722075869336, 130.692...</td>\n",
       "      <td>[92.43681939593179, 22.50037668652832, 141.306...</td>\n",
       "      <td>[104.5978211529732, 22.04513325984048, 153.672...</td>\n",
       "      <td>[114.654700661428, 17.817739838317607, 167.487...</td>\n",
       "      <td>[130.2578821609651, 15.584474619733086, 177.42...</td>\n",
       "      <td>[137.88253604193972, 15.713794233289649, 183.3...</td>\n",
       "      <td>[151.1877135081929, 20.428405998494586, 191.79...</td>\n",
       "      <td>[165.68593072270755, 20.306249618180708, 203.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>[548.3095747526077, 55.4499866274405, 542.7064...</td>\n",
       "      <td>[559.3591679425583, 54.50747056649596, 551.733...</td>\n",
       "      <td>[571.6816533108394, 56.70940531421341, 561.582...</td>\n",
       "      <td>[579.4599029964786, 54.63856222177928, 574.765...</td>\n",
       "      <td>[595.5333143912487, 52.484301747407294, 584.49...</td>\n",
       "      <td>[608.6178598298512, 54.07367324497946, 596.594...</td>\n",
       "      <td>[616.5485332302082, 54.14087619468538, 610.544...</td>\n",
       "      <td>[630.1560680698611, 48.47305567995223, 624.534...</td>\n",
       "      <td>[645.4160783268949, 54.60836552550953, 632.508...</td>\n",
       "      <td>[655.4035507361932, 59.76984000425239, 648.778...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[70.80009877085162, 22.786106233538195, 115.03...</td>\n",
       "      <td>[78.53846153846153, 23.79722075869336, 130.692...</td>\n",
       "      <td>[92.43681939593179, 22.50037668652832, 141.306...</td>\n",
       "      <td>[104.5978211529732, 22.04513325984048, 153.672...</td>\n",
       "      <td>[114.654700661428, 17.817739838317607, 167.487...</td>\n",
       "      <td>[130.2578821609651, 15.584474619733086, 177.42...</td>\n",
       "      <td>[137.88253604193972, 15.713794233289649, 183.3...</td>\n",
       "      <td>[151.1877135081929, 20.428405998494586, 191.79...</td>\n",
       "      <td>[165.68593072270755, 20.306249618180708, 203.6...</td>\n",
       "      <td>[175.51762512381282, 22.85107498689041, 214.47...</td>\n",
       "      <td>...</td>\n",
       "      <td>[559.3591679425583, 54.50747056649596, 551.733...</td>\n",
       "      <td>[571.6816533108394, 56.70940531421341, 561.582...</td>\n",
       "      <td>[579.4599029964786, 54.63856222177928, 574.765...</td>\n",
       "      <td>[595.5333143912487, 52.484301747407294, 584.49...</td>\n",
       "      <td>[608.6178598298512, 54.07367324497946, 596.594...</td>\n",
       "      <td>[616.5485332302082, 54.14087619468538, 610.544...</td>\n",
       "      <td>[630.1560680698611, 48.47305567995223, 624.534...</td>\n",
       "      <td>[645.4160783268949, 54.60836552550953, 632.508...</td>\n",
       "      <td>[655.4035507361932, 59.76984000425239, 648.778...</td>\n",
       "      <td>[664.2836707746479, 64.61612382629107, 654.376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[78.53846153846153, 23.79722075869336, 130.692...</td>\n",
       "      <td>[92.43681939593179, 22.50037668652832, 141.306...</td>\n",
       "      <td>[104.5978211529732, 22.04513325984048, 153.672...</td>\n",
       "      <td>[114.654700661428, 17.817739838317607, 167.487...</td>\n",
       "      <td>[130.2578821609651, 15.584474619733086, 177.42...</td>\n",
       "      <td>[137.88253604193972, 15.713794233289649, 183.3...</td>\n",
       "      <td>[151.1877135081929, 20.428405998494586, 191.79...</td>\n",
       "      <td>[165.68593072270755, 20.306249618180708, 203.6...</td>\n",
       "      <td>[175.51762512381282, 22.85107498689041, 214.47...</td>\n",
       "      <td>[187.1709068399876, 20.589291241101826, 222.27...</td>\n",
       "      <td>...</td>\n",
       "      <td>[571.6816533108394, 56.70940531421341, 561.582...</td>\n",
       "      <td>[579.4599029964786, 54.63856222177928, 574.765...</td>\n",
       "      <td>[595.5333143912487, 52.484301747407294, 584.49...</td>\n",
       "      <td>[608.6178598298512, 54.07367324497946, 596.594...</td>\n",
       "      <td>[616.5485332302082, 54.14087619468538, 610.544...</td>\n",
       "      <td>[630.1560680698611, 48.47305567995223, 624.534...</td>\n",
       "      <td>[645.4160783268949, 54.60836552550953, 632.508...</td>\n",
       "      <td>[655.4035507361932, 59.76984000425239, 648.778...</td>\n",
       "      <td>[664.2836707746479, 64.61612382629107, 654.376...</td>\n",
       "      <td>[681.5213026017112, 66.98795180722891, 667.341...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[92.43681939593179, 22.50037668652832, 141.306...</td>\n",
       "      <td>[104.5978211529732, 22.04513325984048, 153.672...</td>\n",
       "      <td>[114.654700661428, 17.817739838317607, 167.487...</td>\n",
       "      <td>[130.2578821609651, 15.584474619733086, 177.42...</td>\n",
       "      <td>[137.88253604193972, 15.713794233289649, 183.3...</td>\n",
       "      <td>[151.1877135081929, 20.428405998494586, 191.79...</td>\n",
       "      <td>[165.68593072270755, 20.306249618180708, 203.6...</td>\n",
       "      <td>[175.51762512381282, 22.85107498689041, 214.47...</td>\n",
       "      <td>[187.1709068399876, 20.589291241101826, 222.27...</td>\n",
       "      <td>[204.5807988024767, 16.58671837790025, 232.493...</td>\n",
       "      <td>...</td>\n",
       "      <td>[579.4599029964786, 54.63856222177928, 574.765...</td>\n",
       "      <td>[595.5333143912487, 52.484301747407294, 584.49...</td>\n",
       "      <td>[608.6178598298512, 54.07367324497946, 596.594...</td>\n",
       "      <td>[616.5485332302082, 54.14087619468538, 610.544...</td>\n",
       "      <td>[630.1560680698611, 48.47305567995223, 624.534...</td>\n",
       "      <td>[645.4160783268949, 54.60836552550953, 632.508...</td>\n",
       "      <td>[655.4035507361932, 59.76984000425239, 648.778...</td>\n",
       "      <td>[664.2836707746479, 64.61612382629107, 654.376...</td>\n",
       "      <td>[681.5213026017112, 66.98795180722891, 667.341...</td>\n",
       "      <td>[688.0223759593558, 65.73148848773107, 682.724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[104.5978211529732, 22.04513325984048, 153.672...</td>\n",
       "      <td>[114.654700661428, 17.817739838317607, 167.487...</td>\n",
       "      <td>[130.2578821609651, 15.584474619733086, 177.42...</td>\n",
       "      <td>[137.88253604193972, 15.713794233289649, 183.3...</td>\n",
       "      <td>[151.1877135081929, 20.428405998494586, 191.79...</td>\n",
       "      <td>[165.68593072270755, 20.306249618180708, 203.6...</td>\n",
       "      <td>[175.51762512381282, 22.85107498689041, 214.47...</td>\n",
       "      <td>[187.1709068399876, 20.589291241101826, 222.27...</td>\n",
       "      <td>[204.5807988024767, 16.58671837790025, 232.493...</td>\n",
       "      <td>[209.63801231212918, 22.24546614164495, 243.42...</td>\n",
       "      <td>...</td>\n",
       "      <td>[595.5333143912487, 52.484301747407294, 584.49...</td>\n",
       "      <td>[608.6178598298512, 54.07367324497946, 596.594...</td>\n",
       "      <td>[616.5485332302082, 54.14087619468538, 610.544...</td>\n",
       "      <td>[630.1560680698611, 48.47305567995223, 624.534...</td>\n",
       "      <td>[645.4160783268949, 54.60836552550953, 632.508...</td>\n",
       "      <td>[655.4035507361932, 59.76984000425239, 648.778...</td>\n",
       "      <td>[664.2836707746479, 64.61612382629107, 654.376...</td>\n",
       "      <td>[681.5213026017112, 66.98795180722891, 667.341...</td>\n",
       "      <td>[688.0223759593558, 65.73148848773107, 682.724...</td>\n",
       "      <td>[700.2392071106094, 66.80523419864559, 692.610...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>[8437.670223857713, 135.98620055197793, 8354.1...</td>\n",
       "      <td>[8446.438829999202, 139.95640392125608, 8373.5...</td>\n",
       "      <td>[8466.83463489252, 134.7544481120683, 8386.770...</td>\n",
       "      <td>[8469.190992493744, 138.5072679613964, 8395.26...</td>\n",
       "      <td>[8482.545345967035, 138.31156939246756, 8406.1...</td>\n",
       "      <td>[8492.787600980304, 137.14967777071794, 8418.2...</td>\n",
       "      <td>[8506.449072044425, 145.54815139558673, 8428.5...</td>\n",
       "      <td>[8510.666243413116, 139.60396165322837, 8440.1...</td>\n",
       "      <td>[8522.268657609671, 140.52361003052246, 8452.9...</td>\n",
       "      <td>[8535.609989238463, 139.60087358359183, 8459.2...</td>\n",
       "      <td>...</td>\n",
       "      <td>[8883.88434368326, 159.93483379880618, 8802.23...</td>\n",
       "      <td>[8897.635639854818, 161.3784685634001, 8813.47...</td>\n",
       "      <td>[8910.625345877144, 159.51474424855718, 8821.4...</td>\n",
       "      <td>[8923.702892001807, 159.2881834613647, 8836.41...</td>\n",
       "      <td>[8935.34010033835, 157.95111422237778, 8847.53...</td>\n",
       "      <td>[8940.46656641604, 155.12932330827067, 8861.91...</td>\n",
       "      <td>[8954.623459439574, 159.4531853577779, 8867.34...</td>\n",
       "      <td>[8963.531319216798, 166.59466643040668, 8880.4...</td>\n",
       "      <td>[8971.234802590348, 161.42225471763803, 8889.3...</td>\n",
       "      <td>[8981.526291116494, 162.7594343308071, 8905.46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>[8446.438829999202, 139.95640392125608, 8373.5...</td>\n",
       "      <td>[8466.83463489252, 134.7544481120683, 8386.770...</td>\n",
       "      <td>[8469.190992493744, 138.5072679613964, 8395.26...</td>\n",
       "      <td>[8482.545345967035, 138.31156939246756, 8406.1...</td>\n",
       "      <td>[8492.787600980304, 137.14967777071794, 8418.2...</td>\n",
       "      <td>[8506.449072044425, 145.54815139558673, 8428.5...</td>\n",
       "      <td>[8510.666243413116, 139.60396165322837, 8440.1...</td>\n",
       "      <td>[8522.268657609671, 140.52361003052246, 8452.9...</td>\n",
       "      <td>[8535.609989238463, 139.60087358359183, 8459.2...</td>\n",
       "      <td>[8543.693538597363, 136.3635159741765, 8465.12...</td>\n",
       "      <td>...</td>\n",
       "      <td>[8897.635639854818, 161.3784685634001, 8813.47...</td>\n",
       "      <td>[8910.625345877144, 159.51474424855718, 8821.4...</td>\n",
       "      <td>[8923.702892001807, 159.2881834613647, 8836.41...</td>\n",
       "      <td>[8935.34010033835, 157.95111422237778, 8847.53...</td>\n",
       "      <td>[8940.46656641604, 155.12932330827067, 8861.91...</td>\n",
       "      <td>[8954.623459439574, 159.4531853577779, 8867.34...</td>\n",
       "      <td>[8963.531319216798, 166.59466643040668, 8880.4...</td>\n",
       "      <td>[8971.234802590348, 161.42225471763803, 8889.3...</td>\n",
       "      <td>[8981.526291116494, 162.7594343308071, 8905.46...</td>\n",
       "      <td>[8990.157367074604, 162.0370600843532, 8920.93...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>[8466.83463489252, 134.7544481120683, 8386.770...</td>\n",
       "      <td>[8469.190992493744, 138.5072679613964, 8395.26...</td>\n",
       "      <td>[8482.545345967035, 138.31156939246756, 8406.1...</td>\n",
       "      <td>[8492.787600980304, 137.14967777071794, 8418.2...</td>\n",
       "      <td>[8506.449072044425, 145.54815139558673, 8428.5...</td>\n",
       "      <td>[8510.666243413116, 139.60396165322837, 8440.1...</td>\n",
       "      <td>[8522.268657609671, 140.52361003052246, 8452.9...</td>\n",
       "      <td>[8535.609989238463, 139.60087358359183, 8459.2...</td>\n",
       "      <td>[8543.693538597363, 136.3635159741765, 8465.12...</td>\n",
       "      <td>[8555.416489178977, 138.29708004122296, 8474.3...</td>\n",
       "      <td>...</td>\n",
       "      <td>[8910.625345877144, 159.51474424855718, 8821.4...</td>\n",
       "      <td>[8923.702892001807, 159.2881834613647, 8836.41...</td>\n",
       "      <td>[8935.34010033835, 157.95111422237778, 8847.53...</td>\n",
       "      <td>[8940.46656641604, 155.12932330827067, 8861.91...</td>\n",
       "      <td>[8954.623459439574, 159.4531853577779, 8867.34...</td>\n",
       "      <td>[8963.531319216798, 166.59466643040668, 8880.4...</td>\n",
       "      <td>[8971.234802590348, 161.42225471763803, 8889.3...</td>\n",
       "      <td>[8981.526291116494, 162.7594343308071, 8905.46...</td>\n",
       "      <td>[8990.157367074604, 162.0370600843532, 8920.93...</td>\n",
       "      <td>[9007.540353356892, 162.55978798586568, 8926.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>[8469.190992493744, 138.5072679613964, 8395.26...</td>\n",
       "      <td>[8482.545345967035, 138.31156939246756, 8406.1...</td>\n",
       "      <td>[8492.787600980304, 137.14967777071794, 8418.2...</td>\n",
       "      <td>[8506.449072044425, 145.54815139558673, 8428.5...</td>\n",
       "      <td>[8510.666243413116, 139.60396165322837, 8440.1...</td>\n",
       "      <td>[8522.268657609671, 140.52361003052246, 8452.9...</td>\n",
       "      <td>[8535.609989238463, 139.60087358359183, 8459.2...</td>\n",
       "      <td>[8543.693538597363, 136.3635159741765, 8465.12...</td>\n",
       "      <td>[8555.416489178977, 138.29708004122296, 8474.3...</td>\n",
       "      <td>[8569.926518992972, 139.0579129970643, 8482.14...</td>\n",
       "      <td>...</td>\n",
       "      <td>[8923.702892001807, 159.2881834613647, 8836.41...</td>\n",
       "      <td>[8935.34010033835, 157.95111422237778, 8847.53...</td>\n",
       "      <td>[8940.46656641604, 155.12932330827067, 8861.91...</td>\n",
       "      <td>[8954.623459439574, 159.4531853577779, 8867.34...</td>\n",
       "      <td>[8963.531319216798, 166.59466643040668, 8880.4...</td>\n",
       "      <td>[8971.234802590348, 161.42225471763803, 8889.3...</td>\n",
       "      <td>[8981.526291116494, 162.7594343308071, 8905.46...</td>\n",
       "      <td>[8990.157367074604, 162.0370600843532, 8920.93...</td>\n",
       "      <td>[9007.540353356892, 162.55978798586568, 8926.0...</td>\n",
       "      <td>[9017.586395147311, 165.976863084922, 8941.781...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>[8482.545345967035, 138.31156939246756, 8406.1...</td>\n",
       "      <td>[8492.787600980304, 137.14967777071794, 8418.2...</td>\n",
       "      <td>[8506.449072044425, 145.54815139558673, 8428.5...</td>\n",
       "      <td>[8510.666243413116, 139.60396165322837, 8440.1...</td>\n",
       "      <td>[8522.268657609671, 140.52361003052246, 8452.9...</td>\n",
       "      <td>[8535.609989238463, 139.60087358359183, 8459.2...</td>\n",
       "      <td>[8543.693538597363, 136.3635159741765, 8465.12...</td>\n",
       "      <td>[8555.416489178977, 138.29708004122296, 8474.3...</td>\n",
       "      <td>[8569.926518992972, 139.0579129970643, 8482.14...</td>\n",
       "      <td>[8576.6442358155, 137.6659863548895, 8495.5077...</td>\n",
       "      <td>...</td>\n",
       "      <td>[8935.34010033835, 157.95111422237778, 8847.53...</td>\n",
       "      <td>[8940.46656641604, 155.12932330827067, 8861.91...</td>\n",
       "      <td>[8954.623459439574, 159.4531853577779, 8867.34...</td>\n",
       "      <td>[8963.531319216798, 166.59466643040668, 8880.4...</td>\n",
       "      <td>[8971.234802590348, 161.42225471763803, 8889.3...</td>\n",
       "      <td>[8981.526291116494, 162.7594343308071, 8905.46...</td>\n",
       "      <td>[8990.157367074604, 162.0370600843532, 8920.93...</td>\n",
       "      <td>[9007.540353356892, 162.55978798586568, 8926.0...</td>\n",
       "      <td>[9017.586395147311, 165.976863084922, 8941.781...</td>\n",
       "      <td>[9030.630412102242, 161.97515649452268, 8950.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     X  \\\n",
       "0    [61.80999150562753, 24.420046719048628, 109.00...   \n",
       "1    [70.80009877085162, 22.786106233538195, 115.03...   \n",
       "2    [78.53846153846153, 23.79722075869336, 130.692...   \n",
       "3    [92.43681939593179, 22.50037668652832, 141.306...   \n",
       "4    [104.5978211529732, 22.04513325984048, 153.672...   \n",
       "..                                                 ...   \n",
       "745  [8437.670223857713, 135.98620055197793, 8354.1...   \n",
       "746  [8446.438829999202, 139.95640392125608, 8373.5...   \n",
       "747  [8466.83463489252, 134.7544481120683, 8386.770...   \n",
       "748  [8469.190992493744, 138.5072679613964, 8395.26...   \n",
       "749  [8482.545345967035, 138.31156939246756, 8406.1...   \n",
       "\n",
       "                                                   y_1  \\\n",
       "0    [70.80009877085162, 22.786106233538195, 115.03...   \n",
       "1    [78.53846153846153, 23.79722075869336, 130.692...   \n",
       "2    [92.43681939593179, 22.50037668652832, 141.306...   \n",
       "3    [104.5978211529732, 22.04513325984048, 153.672...   \n",
       "4    [114.654700661428, 17.817739838317607, 167.487...   \n",
       "..                                                 ...   \n",
       "745  [8446.438829999202, 139.95640392125608, 8373.5...   \n",
       "746  [8466.83463489252, 134.7544481120683, 8386.770...   \n",
       "747  [8469.190992493744, 138.5072679613964, 8395.26...   \n",
       "748  [8482.545345967035, 138.31156939246756, 8406.1...   \n",
       "749  [8492.787600980304, 137.14967777071794, 8418.2...   \n",
       "\n",
       "                                                   y_2  \\\n",
       "0    [78.53846153846153, 23.79722075869336, 130.692...   \n",
       "1    [92.43681939593179, 22.50037668652832, 141.306...   \n",
       "2    [104.5978211529732, 22.04513325984048, 153.672...   \n",
       "3    [114.654700661428, 17.817739838317607, 167.487...   \n",
       "4    [130.2578821609651, 15.584474619733086, 177.42...   \n",
       "..                                                 ...   \n",
       "745  [8466.83463489252, 134.7544481120683, 8386.770...   \n",
       "746  [8469.190992493744, 138.5072679613964, 8395.26...   \n",
       "747  [8482.545345967035, 138.31156939246756, 8406.1...   \n",
       "748  [8492.787600980304, 137.14967777071794, 8418.2...   \n",
       "749  [8506.449072044425, 145.54815139558673, 8428.5...   \n",
       "\n",
       "                                                   y_3  \\\n",
       "0    [92.43681939593179, 22.50037668652832, 141.306...   \n",
       "1    [104.5978211529732, 22.04513325984048, 153.672...   \n",
       "2    [114.654700661428, 17.817739838317607, 167.487...   \n",
       "3    [130.2578821609651, 15.584474619733086, 177.42...   \n",
       "4    [137.88253604193972, 15.713794233289649, 183.3...   \n",
       "..                                                 ...   \n",
       "745  [8469.190992493744, 138.5072679613964, 8395.26...   \n",
       "746  [8482.545345967035, 138.31156939246756, 8406.1...   \n",
       "747  [8492.787600980304, 137.14967777071794, 8418.2...   \n",
       "748  [8506.449072044425, 145.54815139558673, 8428.5...   \n",
       "749  [8510.666243413116, 139.60396165322837, 8440.1...   \n",
       "\n",
       "                                                   y_4  \\\n",
       "0    [104.5978211529732, 22.04513325984048, 153.672...   \n",
       "1    [114.654700661428, 17.817739838317607, 167.487...   \n",
       "2    [130.2578821609651, 15.584474619733086, 177.42...   \n",
       "3    [137.88253604193972, 15.713794233289649, 183.3...   \n",
       "4    [151.1877135081929, 20.428405998494586, 191.79...   \n",
       "..                                                 ...   \n",
       "745  [8482.545345967035, 138.31156939246756, 8406.1...   \n",
       "746  [8492.787600980304, 137.14967777071794, 8418.2...   \n",
       "747  [8506.449072044425, 145.54815139558673, 8428.5...   \n",
       "748  [8510.666243413116, 139.60396165322837, 8440.1...   \n",
       "749  [8522.268657609671, 140.52361003052246, 8452.9...   \n",
       "\n",
       "                                                   y_5  \\\n",
       "0    [114.654700661428, 17.817739838317607, 167.487...   \n",
       "1    [130.2578821609651, 15.584474619733086, 177.42...   \n",
       "2    [137.88253604193972, 15.713794233289649, 183.3...   \n",
       "3    [151.1877135081929, 20.428405998494586, 191.79...   \n",
       "4    [165.68593072270755, 20.306249618180708, 203.6...   \n",
       "..                                                 ...   \n",
       "745  [8492.787600980304, 137.14967777071794, 8418.2...   \n",
       "746  [8506.449072044425, 145.54815139558673, 8428.5...   \n",
       "747  [8510.666243413116, 139.60396165322837, 8440.1...   \n",
       "748  [8522.268657609671, 140.52361003052246, 8452.9...   \n",
       "749  [8535.609989238463, 139.60087358359183, 8459.2...   \n",
       "\n",
       "                                                   y_6  \\\n",
       "0    [130.2578821609651, 15.584474619733086, 177.42...   \n",
       "1    [137.88253604193972, 15.713794233289649, 183.3...   \n",
       "2    [151.1877135081929, 20.428405998494586, 191.79...   \n",
       "3    [165.68593072270755, 20.306249618180708, 203.6...   \n",
       "4    [175.51762512381282, 22.85107498689041, 214.47...   \n",
       "..                                                 ...   \n",
       "745  [8506.449072044425, 145.54815139558673, 8428.5...   \n",
       "746  [8510.666243413116, 139.60396165322837, 8440.1...   \n",
       "747  [8522.268657609671, 140.52361003052246, 8452.9...   \n",
       "748  [8535.609989238463, 139.60087358359183, 8459.2...   \n",
       "749  [8543.693538597363, 136.3635159741765, 8465.12...   \n",
       "\n",
       "                                                   y_7  \\\n",
       "0    [137.88253604193972, 15.713794233289649, 183.3...   \n",
       "1    [151.1877135081929, 20.428405998494586, 191.79...   \n",
       "2    [165.68593072270755, 20.306249618180708, 203.6...   \n",
       "3    [175.51762512381282, 22.85107498689041, 214.47...   \n",
       "4    [187.1709068399876, 20.589291241101826, 222.27...   \n",
       "..                                                 ...   \n",
       "745  [8510.666243413116, 139.60396165322837, 8440.1...   \n",
       "746  [8522.268657609671, 140.52361003052246, 8452.9...   \n",
       "747  [8535.609989238463, 139.60087358359183, 8459.2...   \n",
       "748  [8543.693538597363, 136.3635159741765, 8465.12...   \n",
       "749  [8555.416489178977, 138.29708004122296, 8474.3...   \n",
       "\n",
       "                                                   y_8  \\\n",
       "0    [151.1877135081929, 20.428405998494586, 191.79...   \n",
       "1    [165.68593072270755, 20.306249618180708, 203.6...   \n",
       "2    [175.51762512381282, 22.85107498689041, 214.47...   \n",
       "3    [187.1709068399876, 20.589291241101826, 222.27...   \n",
       "4    [204.5807988024767, 16.58671837790025, 232.493...   \n",
       "..                                                 ...   \n",
       "745  [8522.268657609671, 140.52361003052246, 8452.9...   \n",
       "746  [8535.609989238463, 139.60087358359183, 8459.2...   \n",
       "747  [8543.693538597363, 136.3635159741765, 8465.12...   \n",
       "748  [8555.416489178977, 138.29708004122296, 8474.3...   \n",
       "749  [8569.926518992972, 139.0579129970643, 8482.14...   \n",
       "\n",
       "                                                   y_9  ...  \\\n",
       "0    [165.68593072270755, 20.306249618180708, 203.6...  ...   \n",
       "1    [175.51762512381282, 22.85107498689041, 214.47...  ...   \n",
       "2    [187.1709068399876, 20.589291241101826, 222.27...  ...   \n",
       "3    [204.5807988024767, 16.58671837790025, 232.493...  ...   \n",
       "4    [209.63801231212918, 22.24546614164495, 243.42...  ...   \n",
       "..                                                 ...  ...   \n",
       "745  [8535.609989238463, 139.60087358359183, 8459.2...  ...   \n",
       "746  [8543.693538597363, 136.3635159741765, 8465.12...  ...   \n",
       "747  [8555.416489178977, 138.29708004122296, 8474.3...  ...   \n",
       "748  [8569.926518992972, 139.0579129970643, 8482.14...  ...   \n",
       "749  [8576.6442358155, 137.6659863548895, 8495.5077...  ...   \n",
       "\n",
       "                                                  y_41  \\\n",
       "0    [548.3095747526077, 55.4499866274405, 542.7064...   \n",
       "1    [559.3591679425583, 54.50747056649596, 551.733...   \n",
       "2    [571.6816533108394, 56.70940531421341, 561.582...   \n",
       "3    [579.4599029964786, 54.63856222177928, 574.765...   \n",
       "4    [595.5333143912487, 52.484301747407294, 584.49...   \n",
       "..                                                 ...   \n",
       "745  [8883.88434368326, 159.93483379880618, 8802.23...   \n",
       "746  [8897.635639854818, 161.3784685634001, 8813.47...   \n",
       "747  [8910.625345877144, 159.51474424855718, 8821.4...   \n",
       "748  [8923.702892001807, 159.2881834613647, 8836.41...   \n",
       "749  [8935.34010033835, 157.95111422237778, 8847.53...   \n",
       "\n",
       "                                                  y_42  \\\n",
       "0    [559.3591679425583, 54.50747056649596, 551.733...   \n",
       "1    [571.6816533108394, 56.70940531421341, 561.582...   \n",
       "2    [579.4599029964786, 54.63856222177928, 574.765...   \n",
       "3    [595.5333143912487, 52.484301747407294, 584.49...   \n",
       "4    [608.6178598298512, 54.07367324497946, 596.594...   \n",
       "..                                                 ...   \n",
       "745  [8897.635639854818, 161.3784685634001, 8813.47...   \n",
       "746  [8910.625345877144, 159.51474424855718, 8821.4...   \n",
       "747  [8923.702892001807, 159.2881834613647, 8836.41...   \n",
       "748  [8935.34010033835, 157.95111422237778, 8847.53...   \n",
       "749  [8940.46656641604, 155.12932330827067, 8861.91...   \n",
       "\n",
       "                                                  y_43  \\\n",
       "0    [571.6816533108394, 56.70940531421341, 561.582...   \n",
       "1    [579.4599029964786, 54.63856222177928, 574.765...   \n",
       "2    [595.5333143912487, 52.484301747407294, 584.49...   \n",
       "3    [608.6178598298512, 54.07367324497946, 596.594...   \n",
       "4    [616.5485332302082, 54.14087619468538, 610.544...   \n",
       "..                                                 ...   \n",
       "745  [8910.625345877144, 159.51474424855718, 8821.4...   \n",
       "746  [8923.702892001807, 159.2881834613647, 8836.41...   \n",
       "747  [8935.34010033835, 157.95111422237778, 8847.53...   \n",
       "748  [8940.46656641604, 155.12932330827067, 8861.91...   \n",
       "749  [8954.623459439574, 159.4531853577779, 8867.34...   \n",
       "\n",
       "                                                  y_44  \\\n",
       "0    [579.4599029964786, 54.63856222177928, 574.765...   \n",
       "1    [595.5333143912487, 52.484301747407294, 584.49...   \n",
       "2    [608.6178598298512, 54.07367324497946, 596.594...   \n",
       "3    [616.5485332302082, 54.14087619468538, 610.544...   \n",
       "4    [630.1560680698611, 48.47305567995223, 624.534...   \n",
       "..                                                 ...   \n",
       "745  [8923.702892001807, 159.2881834613647, 8836.41...   \n",
       "746  [8935.34010033835, 157.95111422237778, 8847.53...   \n",
       "747  [8940.46656641604, 155.12932330827067, 8861.91...   \n",
       "748  [8954.623459439574, 159.4531853577779, 8867.34...   \n",
       "749  [8963.531319216798, 166.59466643040668, 8880.4...   \n",
       "\n",
       "                                                  y_45  \\\n",
       "0    [595.5333143912487, 52.484301747407294, 584.49...   \n",
       "1    [608.6178598298512, 54.07367324497946, 596.594...   \n",
       "2    [616.5485332302082, 54.14087619468538, 610.544...   \n",
       "3    [630.1560680698611, 48.47305567995223, 624.534...   \n",
       "4    [645.4160783268949, 54.60836552550953, 632.508...   \n",
       "..                                                 ...   \n",
       "745  [8935.34010033835, 157.95111422237778, 8847.53...   \n",
       "746  [8940.46656641604, 155.12932330827067, 8861.91...   \n",
       "747  [8954.623459439574, 159.4531853577779, 8867.34...   \n",
       "748  [8963.531319216798, 166.59466643040668, 8880.4...   \n",
       "749  [8971.234802590348, 161.42225471763803, 8889.3...   \n",
       "\n",
       "                                                  y_46  \\\n",
       "0    [608.6178598298512, 54.07367324497946, 596.594...   \n",
       "1    [616.5485332302082, 54.14087619468538, 610.544...   \n",
       "2    [630.1560680698611, 48.47305567995223, 624.534...   \n",
       "3    [645.4160783268949, 54.60836552550953, 632.508...   \n",
       "4    [655.4035507361932, 59.76984000425239, 648.778...   \n",
       "..                                                 ...   \n",
       "745  [8940.46656641604, 155.12932330827067, 8861.91...   \n",
       "746  [8954.623459439574, 159.4531853577779, 8867.34...   \n",
       "747  [8963.531319216798, 166.59466643040668, 8880.4...   \n",
       "748  [8971.234802590348, 161.42225471763803, 8889.3...   \n",
       "749  [8981.526291116494, 162.7594343308071, 8905.46...   \n",
       "\n",
       "                                                  y_47  \\\n",
       "0    [616.5485332302082, 54.14087619468538, 610.544...   \n",
       "1    [630.1560680698611, 48.47305567995223, 624.534...   \n",
       "2    [645.4160783268949, 54.60836552550953, 632.508...   \n",
       "3    [655.4035507361932, 59.76984000425239, 648.778...   \n",
       "4    [664.2836707746479, 64.61612382629107, 654.376...   \n",
       "..                                                 ...   \n",
       "745  [8954.623459439574, 159.4531853577779, 8867.34...   \n",
       "746  [8963.531319216798, 166.59466643040668, 8880.4...   \n",
       "747  [8971.234802590348, 161.42225471763803, 8889.3...   \n",
       "748  [8981.526291116494, 162.7594343308071, 8905.46...   \n",
       "749  [8990.157367074604, 162.0370600843532, 8920.93...   \n",
       "\n",
       "                                                  y_48  \\\n",
       "0    [630.1560680698611, 48.47305567995223, 624.534...   \n",
       "1    [645.4160783268949, 54.60836552550953, 632.508...   \n",
       "2    [655.4035507361932, 59.76984000425239, 648.778...   \n",
       "3    [664.2836707746479, 64.61612382629107, 654.376...   \n",
       "4    [681.5213026017112, 66.98795180722891, 667.341...   \n",
       "..                                                 ...   \n",
       "745  [8963.531319216798, 166.59466643040668, 8880.4...   \n",
       "746  [8971.234802590348, 161.42225471763803, 8889.3...   \n",
       "747  [8981.526291116494, 162.7594343308071, 8905.46...   \n",
       "748  [8990.157367074604, 162.0370600843532, 8920.93...   \n",
       "749  [9007.540353356892, 162.55978798586568, 8926.0...   \n",
       "\n",
       "                                                  y_49  \\\n",
       "0    [645.4160783268949, 54.60836552550953, 632.508...   \n",
       "1    [655.4035507361932, 59.76984000425239, 648.778...   \n",
       "2    [664.2836707746479, 64.61612382629107, 654.376...   \n",
       "3    [681.5213026017112, 66.98795180722891, 667.341...   \n",
       "4    [688.0223759593558, 65.73148848773107, 682.724...   \n",
       "..                                                 ...   \n",
       "745  [8971.234802590348, 161.42225471763803, 8889.3...   \n",
       "746  [8981.526291116494, 162.7594343308071, 8905.46...   \n",
       "747  [8990.157367074604, 162.0370600843532, 8920.93...   \n",
       "748  [9007.540353356892, 162.55978798586568, 8926.0...   \n",
       "749  [9017.586395147311, 165.976863084922, 8941.781...   \n",
       "\n",
       "                                                  y_50  \n",
       "0    [655.4035507361932, 59.76984000425239, 648.778...  \n",
       "1    [664.2836707746479, 64.61612382629107, 654.376...  \n",
       "2    [681.5213026017112, 66.98795180722891, 667.341...  \n",
       "3    [688.0223759593558, 65.73148848773107, 682.724...  \n",
       "4    [700.2392071106094, 66.80523419864559, 692.610...  \n",
       "..                                                 ...  \n",
       "745  [8981.526291116494, 162.7594343308071, 8905.46...  \n",
       "746  [8990.157367074604, 162.0370600843532, 8920.93...  \n",
       "747  [9007.540353356892, 162.55978798586568, 8926.0...  \n",
       "748  [9017.586395147311, 165.976863084922, 8941.781...  \n",
       "749  [9030.630412102242, 161.97515649452268, 8950.1...  \n",
       "\n",
       "[750 rows x 51 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 50\n",
    "\n",
    "# data in columns format (frame, next frame, next next frame, ...)\n",
    "df_n_rounds = pd.DataFrame(columns=['X', 'y_1'])\n",
    "\n",
    "for i in range(1, n):\n",
    "    df_n_rounds['y_' + str(i+1)] = ''\n",
    "\n",
    "for i in range(n+1):\n",
    "    col = []\n",
    "    for j in range(i, len(frames)-n+i):\n",
    "        col.append(frames[j])\n",
    "    if i == 0:\n",
    "        df_n_rounds['X'] = col\n",
    "    else:\n",
    "        df_n_rounds['y_' + str(i)] = col\n",
    "\n",
    "df_n_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_n_rounds['X'].tolist()\n",
    "y = df_n_rounds['y_1'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (600, 30)\n",
      "y_train shape: (600, 30)\n",
      "X_test shape: (150, 30)\n",
      "y_test shape: (150, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"y_train shape: \" + str(y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "\n",
    "optimizer = 'NAdam'\n",
    "loss = 'mae'\n",
    "metrics = ['accuracy']\n",
    "training_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "n_input = 2 * no_skyrmions\n",
    "n_hidden_1 = 64\n",
    "n_hidden_2 = 64\n",
    "n_hidden_3 = 64\n",
    "n_output = 2 * no_skyrmions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "\n",
    "# Neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_1, input_dim=n_input, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_hidden_2, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_hidden_3, activation=activation, kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/1000\n",
      "600/600 [==============================] - 0s 310us/step - loss: 1425.4960 - accuracy: 0.0750 - val_loss: 424.6687 - val_accuracy: 0.3867\n",
      "Epoch 2/1000\n",
      "600/600 [==============================] - 0s 58us/step - loss: 276.5576 - accuracy: 0.1400 - val_loss: 250.7665 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "600/600 [==============================] - 0s 50us/step - loss: 173.3485 - accuracy: 0.2017 - val_loss: 140.5192 - val_accuracy: 0.3867\n",
      "Epoch 4/1000\n",
      "600/600 [==============================] - 0s 52us/step - loss: 144.9627 - accuracy: 0.2700 - val_loss: 232.2406 - val_accuracy: 0.2733\n",
      "Epoch 5/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 147.3595 - accuracy: 0.3000 - val_loss: 163.3986 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "600/600 [==============================] - 0s 48us/step - loss: 139.9153 - accuracy: 0.2483 - val_loss: 178.4885 - val_accuracy: 0.3667\n",
      "Epoch 7/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 138.6770 - accuracy: 0.3733 - val_loss: 56.5343 - val_accuracy: 0.3800\n",
      "Epoch 8/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 126.6999 - accuracy: 0.1717 - val_loss: 91.8734 - val_accuracy: 0.0267\n",
      "Epoch 9/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 123.3079 - accuracy: 0.2367 - val_loss: 165.5238 - val_accuracy: 0.2267\n",
      "Epoch 10/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 119.0036 - accuracy: 0.2933 - val_loss: 191.0642 - val_accuracy: 0.2267\n",
      "Epoch 11/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 121.9701 - accuracy: 0.2500 - val_loss: 93.6668 - val_accuracy: 0.3600\n",
      "Epoch 12/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 116.3375 - accuracy: 0.3233 - val_loss: 115.8054 - val_accuracy: 0.2400\n",
      "Epoch 13/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 109.9817 - accuracy: 0.3133 - val_loss: 150.4611 - val_accuracy: 0.3333\n",
      "Epoch 14/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 116.1512 - accuracy: 0.4317 - val_loss: 88.0337 - val_accuracy: 0.3467\n",
      "Epoch 15/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 113.1873 - accuracy: 0.3217 - val_loss: 49.3271 - val_accuracy: 0.2533\n",
      "Epoch 16/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 106.2712 - accuracy: 0.3533 - val_loss: 118.4370 - val_accuracy: 0.2333\n",
      "Epoch 17/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 105.0720 - accuracy: 0.3350 - val_loss: 90.5960 - val_accuracy: 0.2400\n",
      "Epoch 18/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 106.5893 - accuracy: 0.3300 - val_loss: 121.2614 - val_accuracy: 0.3667\n",
      "Epoch 19/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 108.0853 - accuracy: 0.2983 - val_loss: 98.5639 - val_accuracy: 0.3267\n",
      "Epoch 20/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 102.9469 - accuracy: 0.3917 - val_loss: 71.0424 - val_accuracy: 0.0267\n",
      "Epoch 21/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 98.7625 - accuracy: 0.3883 - val_loss: 89.6658 - val_accuracy: 0.3267\n",
      "Epoch 22/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 100.1978 - accuracy: 0.3883 - val_loss: 164.6046 - val_accuracy: 0.4133\n",
      "Epoch 23/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 101.6977 - accuracy: 0.4683 - val_loss: 41.4087 - val_accuracy: 0.5400\n",
      "Epoch 24/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 101.0616 - accuracy: 0.2550 - val_loss: 112.3721 - val_accuracy: 0.4333\n",
      "Epoch 25/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 106.5849 - accuracy: 0.3900 - val_loss: 75.5171 - val_accuracy: 0.3067\n",
      "Epoch 26/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 98.6538 - accuracy: 0.4367 - val_loss: 161.1149 - val_accuracy: 0.5133\n",
      "Epoch 27/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 97.0169 - accuracy: 0.3383 - val_loss: 106.2847 - val_accuracy: 0.2600\n",
      "Epoch 28/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 91.8513 - accuracy: 0.3750 - val_loss: 131.7185 - val_accuracy: 0.4000\n",
      "Epoch 29/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 100.3325 - accuracy: 0.3517 - val_loss: 108.0930 - val_accuracy: 0.5600\n",
      "Epoch 30/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 95.8439 - accuracy: 0.3950 - val_loss: 44.2891 - val_accuracy: 0.2733\n",
      "Epoch 31/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 91.9897 - accuracy: 0.3150 - val_loss: 126.9560 - val_accuracy: 0.2667\n",
      "Epoch 32/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 94.8476 - accuracy: 0.4717 - val_loss: 94.5653 - val_accuracy: 0.4933\n",
      "Epoch 33/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 95.7968 - accuracy: 0.4850 - val_loss: 156.9513 - val_accuracy: 0.2800\n",
      "Epoch 34/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 92.3258 - accuracy: 0.3833 - val_loss: 41.1524 - val_accuracy: 0.3800\n",
      "Epoch 35/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 90.7629 - accuracy: 0.4800 - val_loss: 109.8562 - val_accuracy: 0.5733\n",
      "Epoch 36/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 96.1490 - accuracy: 0.5333 - val_loss: 125.9050 - val_accuracy: 0.5400\n",
      "Epoch 37/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 96.5609 - accuracy: 0.5200 - val_loss: 111.7930 - val_accuracy: 0.5467\n",
      "Epoch 38/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 94.7815 - accuracy: 0.5967 - val_loss: 26.9160 - val_accuracy: 0.5600\n",
      "Epoch 39/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 86.8904 - accuracy: 0.4100 - val_loss: 54.6918 - val_accuracy: 0.4000\n",
      "Epoch 40/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 90.6445 - accuracy: 0.4450 - val_loss: 85.8632 - val_accuracy: 0.4733\n",
      "Epoch 41/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 90.5213 - accuracy: 0.5100 - val_loss: 53.4162 - val_accuracy: 0.5267\n",
      "Epoch 42/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 86.8697 - accuracy: 0.5250 - val_loss: 49.0020 - val_accuracy: 0.5867\n",
      "Epoch 43/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 82.3180 - accuracy: 0.4717 - val_loss: 60.2020 - val_accuracy: 0.4067\n",
      "Epoch 44/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 85.0281 - accuracy: 0.5150 - val_loss: 112.2845 - val_accuracy: 0.5200\n",
      "Epoch 45/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 86.1139 - accuracy: 0.4450 - val_loss: 92.8504 - val_accuracy: 0.5267\n",
      "Epoch 46/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 86.4868 - accuracy: 0.5617 - val_loss: 155.3526 - val_accuracy: 0.4333\n",
      "Epoch 47/1000\n",
      "600/600 [==============================] - 0s 48us/step - loss: 88.3397 - accuracy: 0.5433 - val_loss: 96.6749 - val_accuracy: 0.4600\n",
      "Epoch 48/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 78.9706 - accuracy: 0.4867 - val_loss: 77.2092 - val_accuracy: 0.5933\n",
      "Epoch 49/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 83.0236 - accuracy: 0.5900 - val_loss: 145.8028 - val_accuracy: 0.5400\n",
      "Epoch 50/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 77.5059 - accuracy: 0.5750 - val_loss: 97.3204 - val_accuracy: 0.5467\n",
      "Epoch 51/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 84.6030 - accuracy: 0.5217 - val_loss: 92.1672 - val_accuracy: 0.6000\n",
      "Epoch 52/1000\n",
      "600/600 [==============================] - 0s 47us/step - loss: 82.6715 - accuracy: 0.5017 - val_loss: 86.0861 - val_accuracy: 0.5133\n",
      "Epoch 53/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 86.0327 - accuracy: 0.5167 - val_loss: 86.4694 - val_accuracy: 0.4200\n",
      "Epoch 54/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 83.2369 - accuracy: 0.5250 - val_loss: 23.4542 - val_accuracy: 0.5600\n",
      "Epoch 55/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 71.0196 - accuracy: 0.5617 - val_loss: 105.3032 - val_accuracy: 0.4867\n",
      "Epoch 56/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 84.0097 - accuracy: 0.4767 - val_loss: 59.0452 - val_accuracy: 0.6067\n",
      "Epoch 57/1000\n",
      "600/600 [==============================] - 0s 47us/step - loss: 78.6180 - accuracy: 0.5467 - val_loss: 115.6108 - val_accuracy: 0.5800\n",
      "Epoch 58/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 81.8107 - accuracy: 0.4933 - val_loss: 94.9169 - val_accuracy: 0.5600\n",
      "Epoch 59/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 81.0843 - accuracy: 0.5717 - val_loss: 81.1765 - val_accuracy: 0.4733\n",
      "Epoch 60/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 73.7392 - accuracy: 0.4850 - val_loss: 91.7063 - val_accuracy: 0.5867\n",
      "Epoch 61/1000\n",
      "600/600 [==============================] - 0s 46us/step - loss: 79.7574 - accuracy: 0.6083 - val_loss: 86.7363 - val_accuracy: 0.5800\n",
      "Epoch 62/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 75.7914 - accuracy: 0.6200 - val_loss: 58.1500 - val_accuracy: 0.5800\n",
      "Epoch 63/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 70.9610 - accuracy: 0.5950 - val_loss: 120.0888 - val_accuracy: 0.5467\n",
      "Epoch 64/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 78.1513 - accuracy: 0.5383 - val_loss: 98.8976 - val_accuracy: 0.5200\n",
      "Epoch 65/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 74.2500 - accuracy: 0.5333 - val_loss: 71.3040 - val_accuracy: 0.5067\n",
      "Epoch 66/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 75.9821 - accuracy: 0.4733 - val_loss: 43.2071 - val_accuracy: 0.5600\n",
      "Epoch 67/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 70.8086 - accuracy: 0.5633 - val_loss: 85.0512 - val_accuracy: 0.5933\n",
      "Epoch 68/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 76.0596 - accuracy: 0.5200 - val_loss: 123.0896 - val_accuracy: 0.2800\n",
      "Epoch 69/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 73.5973 - accuracy: 0.5517 - val_loss: 68.2746 - val_accuracy: 0.5067\n",
      "Epoch 70/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 74.3985 - accuracy: 0.5467 - val_loss: 56.8893 - val_accuracy: 0.1867\n",
      "Epoch 71/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 67.8181 - accuracy: 0.4583 - val_loss: 30.7386 - val_accuracy: 0.6133\n",
      "Epoch 72/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 71.7390 - accuracy: 0.6217 - val_loss: 33.3057 - val_accuracy: 0.5667\n",
      "Epoch 73/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 70.3132 - accuracy: 0.6083 - val_loss: 46.5670 - val_accuracy: 0.5667\n",
      "Epoch 74/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 71.4191 - accuracy: 0.5400 - val_loss: 102.8895 - val_accuracy: 0.4267\n",
      "Epoch 75/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 76.2051 - accuracy: 0.5267 - val_loss: 99.6670 - val_accuracy: 0.5200\n",
      "Epoch 76/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 59.5691 - accuracy: 0.5283 - val_loss: 82.9409 - val_accuracy: 0.6133\n",
      "Epoch 77/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 71.0184 - accuracy: 0.5367 - val_loss: 76.0444 - val_accuracy: 0.5333\n",
      "Epoch 78/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 71.2731 - accuracy: 0.5550 - val_loss: 62.1114 - val_accuracy: 0.6333\n",
      "Epoch 79/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 71.6163 - accuracy: 0.6517 - val_loss: 50.7740 - val_accuracy: 0.5733\n",
      "Epoch 80/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 69.7827 - accuracy: 0.5967 - val_loss: 34.5118 - val_accuracy: 0.5933\n",
      "Epoch 81/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 71.4520 - accuracy: 0.5633 - val_loss: 68.5991 - val_accuracy: 0.4067\n",
      "Epoch 82/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 72.8626 - accuracy: 0.4867 - val_loss: 63.7726 - val_accuracy: 0.4733\n",
      "Epoch 83/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 72.7402 - accuracy: 0.5833 - val_loss: 97.8123 - val_accuracy: 0.5533\n",
      "Epoch 84/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 72.0587 - accuracy: 0.5667 - val_loss: 22.4949 - val_accuracy: 0.4467\n",
      "Epoch 85/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 63.5755 - accuracy: 0.5600 - val_loss: 76.7250 - val_accuracy: 0.5333\n",
      "Epoch 86/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 72.4466 - accuracy: 0.5583 - val_loss: 21.1199 - val_accuracy: 0.6667\n",
      "Epoch 87/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 63.5382 - accuracy: 0.5283 - val_loss: 117.1374 - val_accuracy: 0.5667\n",
      "Epoch 88/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 70.6772 - accuracy: 0.4583 - val_loss: 76.4269 - val_accuracy: 0.5933\n",
      "Epoch 89/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 67.6617 - accuracy: 0.5717 - val_loss: 107.0430 - val_accuracy: 0.5400\n",
      "Epoch 90/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 70.1886 - accuracy: 0.5917 - val_loss: 49.8204 - val_accuracy: 0.5667\n",
      "Epoch 91/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 71.0628 - accuracy: 0.5967 - val_loss: 23.3822 - val_accuracy: 0.6133\n",
      "Epoch 92/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 65.8165 - accuracy: 0.5667 - val_loss: 71.0492 - val_accuracy: 0.5133\n",
      "Epoch 93/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 64.9724 - accuracy: 0.5700 - val_loss: 70.1018 - val_accuracy: 0.2400\n",
      "Epoch 94/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 64.5308 - accuracy: 0.5467 - val_loss: 31.9513 - val_accuracy: 0.5933\n",
      "Epoch 95/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 54.9205 - accuracy: 0.5517 - val_loss: 56.0820 - val_accuracy: 0.6533\n",
      "Epoch 96/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 64.7799 - accuracy: 0.5900 - val_loss: 95.8114 - val_accuracy: 0.5200\n",
      "Epoch 97/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 66.5721 - accuracy: 0.5583 - val_loss: 69.4285 - val_accuracy: 0.5733\n",
      "Epoch 98/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 67.2633 - accuracy: 0.6183 - val_loss: 69.5586 - val_accuracy: 0.3133\n",
      "Epoch 99/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 65.6302 - accuracy: 0.5450 - val_loss: 24.9007 - val_accuracy: 0.6000\n",
      "Epoch 100/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 59.2923 - accuracy: 0.5483 - val_loss: 91.8911 - val_accuracy: 0.2133\n",
      "Epoch 101/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 67.3905 - accuracy: 0.4033 - val_loss: 66.6204 - val_accuracy: 0.4733\n",
      "Epoch 102/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 68.1619 - accuracy: 0.5533 - val_loss: 56.7009 - val_accuracy: 0.4133\n",
      "Epoch 103/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 65.2946 - accuracy: 0.5817 - val_loss: 78.0671 - val_accuracy: 0.6067\n",
      "Epoch 104/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 64.9363 - accuracy: 0.5267 - val_loss: 88.7520 - val_accuracy: 0.2800\n",
      "Epoch 105/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 66.7086 - accuracy: 0.6383 - val_loss: 38.6444 - val_accuracy: 0.5867\n",
      "Epoch 106/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 62.9658 - accuracy: 0.6983 - val_loss: 86.3103 - val_accuracy: 0.5533\n",
      "Epoch 107/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 61.0686 - accuracy: 0.5800 - val_loss: 103.6853 - val_accuracy: 0.6200\n",
      "Epoch 108/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 64.0521 - accuracy: 0.5900 - val_loss: 68.6544 - val_accuracy: 0.4600\n",
      "Epoch 109/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 63.6503 - accuracy: 0.5550 - val_loss: 69.8844 - val_accuracy: 0.5600\n",
      "Epoch 110/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 55.9620 - accuracy: 0.5817 - val_loss: 92.9991 - val_accuracy: 0.4600\n",
      "Epoch 111/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 64.7354 - accuracy: 0.6167 - val_loss: 91.8951 - val_accuracy: 0.5800\n",
      "Epoch 112/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 59.1974 - accuracy: 0.5850 - val_loss: 70.7682 - val_accuracy: 0.5867\n",
      "Epoch 113/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 61.2352 - accuracy: 0.5983 - val_loss: 45.6843 - val_accuracy: 0.5933\n",
      "Epoch 114/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 60.7789 - accuracy: 0.6483 - val_loss: 99.8799 - val_accuracy: 0.6800\n",
      "Epoch 115/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 43.8616 - accuracy: 0.4950 - val_loss: 75.9482 - val_accuracy: 0.2800\n",
      "Epoch 116/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 55.9211 - accuracy: 0.4650 - val_loss: 98.7195 - val_accuracy: 0.5600\n",
      "Epoch 117/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 58.7097 - accuracy: 0.4283 - val_loss: 30.2196 - val_accuracy: 0.5067\n",
      "Epoch 118/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 59.2016 - accuracy: 0.6050 - val_loss: 53.6746 - val_accuracy: 0.5800\n",
      "Epoch 119/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 63.6761 - accuracy: 0.5617 - val_loss: 69.7097 - val_accuracy: 0.5400\n",
      "Epoch 120/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 56.3615 - accuracy: 0.5150 - val_loss: 37.2317 - val_accuracy: 0.5800\n",
      "Epoch 121/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 59.0163 - accuracy: 0.6683 - val_loss: 61.8416 - val_accuracy: 0.5600\n",
      "Epoch 122/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 60.5731 - accuracy: 0.6183 - val_loss: 32.4383 - val_accuracy: 0.5533\n",
      "Epoch 123/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 54.5883 - accuracy: 0.6267 - val_loss: 75.1386 - val_accuracy: 0.5467\n",
      "Epoch 124/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 61.5498 - accuracy: 0.6350 - val_loss: 85.1339 - val_accuracy: 0.5933\n",
      "Epoch 125/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 49.7590 - accuracy: 0.6167 - val_loss: 31.1762 - val_accuracy: 0.5800\n",
      "Epoch 126/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 59.2479 - accuracy: 0.6617 - val_loss: 63.9912 - val_accuracy: 0.5400\n",
      "Epoch 127/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 61.5217 - accuracy: 0.6033 - val_loss: 54.9494 - val_accuracy: 0.5600\n",
      "Epoch 128/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 56.7714 - accuracy: 0.6167 - val_loss: 46.7413 - val_accuracy: 0.6067\n",
      "Epoch 129/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 56.3126 - accuracy: 0.6300 - val_loss: 49.9738 - val_accuracy: 0.5267\n",
      "Epoch 130/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 58.9577 - accuracy: 0.5850 - val_loss: 58.1982 - val_accuracy: 0.5533\n",
      "Epoch 131/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 54.8309 - accuracy: 0.5967 - val_loss: 38.4569 - val_accuracy: 0.5600\n",
      "Epoch 132/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 40.3527 - accuracy: 0.625 - 0s 37us/step - loss: 56.2787 - accuracy: 0.6300 - val_loss: 68.2528 - val_accuracy: 0.6533\n",
      "Epoch 133/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 54.8813 - accuracy: 0.6683 - val_loss: 65.7904 - val_accuracy: 0.6133\n",
      "Epoch 134/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 62.6325 - accuracy: 0.750 - 0s 36us/step - loss: 55.2496 - accuracy: 0.5350 - val_loss: 61.6129 - val_accuracy: 0.6000\n",
      "Epoch 135/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 51.7618 - accuracy: 0.5617 - val_loss: 47.4829 - val_accuracy: 0.4667\n",
      "Epoch 136/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 52.1708 - accuracy: 0.6283 - val_loss: 45.6547 - val_accuracy: 0.5667\n",
      "Epoch 137/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 57.5880 - accuracy: 0.5900 - val_loss: 105.7412 - val_accuracy: 0.5467\n",
      "Epoch 138/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 112.1287 - accuracy: 0.68 - 0s 33us/step - loss: 52.5723 - accuracy: 0.5583 - val_loss: 59.0179 - val_accuracy: 0.6133\n",
      "Epoch 139/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 56.7759 - accuracy: 0.5900 - val_loss: 71.8018 - val_accuracy: 0.7000\n",
      "Epoch 140/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 54.2274 - accuracy: 0.5633 - val_loss: 36.4004 - val_accuracy: 0.6133\n",
      "Epoch 141/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 55.5727 - accuracy: 0.6250 - val_loss: 41.6631 - val_accuracy: 0.5667\n",
      "Epoch 142/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 50.5245 - accuracy: 0.4900 - val_loss: 72.5839 - val_accuracy: 0.5533\n",
      "Epoch 143/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 57.6317 - accuracy: 0.5617 - val_loss: 30.0812 - val_accuracy: 0.5533\n",
      "Epoch 144/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 50.8165 - accuracy: 0.6233 - val_loss: 46.6010 - val_accuracy: 0.6000\n",
      "Epoch 145/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 53.1770 - accuracy: 0.6233 - val_loss: 78.5243 - val_accuracy: 0.5733\n",
      "Epoch 146/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 55.5567 - accuracy: 0.6100 - val_loss: 41.5260 - val_accuracy: 0.6000\n",
      "Epoch 147/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 56.0744 - accuracy: 0.6117 - val_loss: 26.4992 - val_accuracy: 0.6133\n",
      "Epoch 148/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 52.2932 - accuracy: 0.6367 - val_loss: 49.2404 - val_accuracy: 0.5667\n",
      "Epoch 149/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 52.5531 - accuracy: 0.6200 - val_loss: 87.7285 - val_accuracy: 0.5867\n",
      "Epoch 150/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 53.0643 - accuracy: 0.5917 - val_loss: 77.5304 - val_accuracy: 0.2800\n",
      "Epoch 151/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 51.2424 - accuracy: 0.6150 - val_loss: 60.7362 - val_accuracy: 0.5600\n",
      "Epoch 152/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 54.0887 - accuracy: 0.6250 - val_loss: 67.5130 - val_accuracy: 0.5867\n",
      "Epoch 153/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 53.8360 - accuracy: 0.6367 - val_loss: 66.4306 - val_accuracy: 0.5533\n",
      "Epoch 154/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 55.4152 - accuracy: 0.5750 - val_loss: 45.2342 - val_accuracy: 0.5733\n",
      "Epoch 155/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 48.5780 - accuracy: 0.437 - 0s 35us/step - loss: 54.2970 - accuracy: 0.5617 - val_loss: 57.7852 - val_accuracy: 0.4000\n",
      "Epoch 156/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 55.0674 - accuracy: 0.5983 - val_loss: 51.0096 - val_accuracy: 0.6133\n",
      "Epoch 157/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 54.9310 - accuracy: 0.6050 - val_loss: 24.3737 - val_accuracy: 0.5733\n",
      "Epoch 158/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 47.1710 - accuracy: 0.5617 - val_loss: 56.0371 - val_accuracy: 0.5867\n",
      "Epoch 159/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 47.4316 - accuracy: 0.5833 - val_loss: 42.1482 - val_accuracy: 0.5600\n",
      "Epoch 160/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 53.7685 - accuracy: 0.5667 - val_loss: 42.5234 - val_accuracy: 0.4867\n",
      "Epoch 161/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 52.1304 - accuracy: 0.5517 - val_loss: 66.0399 - val_accuracy: 0.5600\n",
      "Epoch 162/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 55.6451 - accuracy: 0.6617 - val_loss: 46.4826 - val_accuracy: 0.6333\n",
      "Epoch 163/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 49.2055 - accuracy: 0.6017 - val_loss: 84.3956 - val_accuracy: 0.5933\n",
      "Epoch 164/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 50.8141 - accuracy: 0.6200 - val_loss: 16.8396 - val_accuracy: 0.5467\n",
      "Epoch 165/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 48.4785 - accuracy: 0.5933 - val_loss: 48.8443 - val_accuracy: 0.4667\n",
      "Epoch 166/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 50.1348 - accuracy: 0.5650 - val_loss: 23.8693 - val_accuracy: 0.6133\n",
      "Epoch 167/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 50.5778 - accuracy: 0.6300 - val_loss: 22.1140 - val_accuracy: 0.6000\n",
      "Epoch 168/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 45.5400 - accuracy: 0.6350 - val_loss: 36.6707 - val_accuracy: 0.5267\n",
      "Epoch 169/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 50.6284 - accuracy: 0.6467 - val_loss: 34.1454 - val_accuracy: 0.6000\n",
      "Epoch 170/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 47.6380 - accuracy: 0.6850 - val_loss: 66.9684 - val_accuracy: 0.6067\n",
      "Epoch 171/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 51.3953 - accuracy: 0.6733 - val_loss: 40.6940 - val_accuracy: 0.5800\n",
      "Epoch 172/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 42.8562 - accuracy: 0.4867 - val_loss: 60.0887 - val_accuracy: 0.5867\n",
      "Epoch 173/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 53.6183 - accuracy: 0.6367 - val_loss: 55.0432 - val_accuracy: 0.5600\n",
      "Epoch 174/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 51.2475 - accuracy: 0.6033 - val_loss: 83.8511 - val_accuracy: 0.5800\n",
      "Epoch 175/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 48.4954 - accuracy: 0.5933 - val_loss: 52.8883 - val_accuracy: 0.6267\n",
      "Epoch 176/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 50.1763 - accuracy: 0.6617 - val_loss: 22.9479 - val_accuracy: 0.5867\n",
      "Epoch 177/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 48.3789 - accuracy: 0.6300 - val_loss: 51.0716 - val_accuracy: 0.5933\n",
      "Epoch 178/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 50.5331 - accuracy: 0.5450 - val_loss: 43.5673 - val_accuracy: 0.5933\n",
      "Epoch 179/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 47.8549 - accuracy: 0.5767 - val_loss: 43.2213 - val_accuracy: 0.6067\n",
      "Epoch 180/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 49.3335 - accuracy: 0.5967 - val_loss: 38.8582 - val_accuracy: 0.5467\n",
      "Epoch 181/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 48.1161 - accuracy: 0.6433 - val_loss: 76.8080 - val_accuracy: 0.6000\n",
      "Epoch 182/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 47.5824 - accuracy: 0.5867 - val_loss: 60.1458 - val_accuracy: 0.5733\n",
      "Epoch 183/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 46.8355 - accuracy: 0.5700 - val_loss: 42.7923 - val_accuracy: 0.5667\n",
      "Epoch 184/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 47.4240 - accuracy: 0.6250 - val_loss: 16.6362 - val_accuracy: 0.6000\n",
      "Epoch 185/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 47.3323 - accuracy: 0.5850 - val_loss: 73.5259 - val_accuracy: 0.5267\n",
      "Epoch 186/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 50.1743 - accuracy: 0.5733 - val_loss: 27.3917 - val_accuracy: 0.5867\n",
      "Epoch 187/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 27.8792 - accuracy: 0.687 - 0s 40us/step - loss: 49.3237 - accuracy: 0.5950 - val_loss: 56.8921 - val_accuracy: 0.5200\n",
      "Epoch 188/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 48.6031 - accuracy: 0.5767 - val_loss: 28.9830 - val_accuracy: 0.5333\n",
      "Epoch 189/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 42.3074 - accuracy: 0.6450 - val_loss: 34.8328 - val_accuracy: 0.5600\n",
      "Epoch 190/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 45.3351 - accuracy: 0.5883 - val_loss: 26.7940 - val_accuracy: 0.5800\n",
      "Epoch 191/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 44.9536 - accuracy: 0.6033 - val_loss: 47.9602 - val_accuracy: 0.6000\n",
      "Epoch 192/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 48.4818 - accuracy: 0.6783 - val_loss: 46.7153 - val_accuracy: 0.6133\n",
      "Epoch 193/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 48.2235 - accuracy: 0.593 - 0s 43us/step - loss: 46.1450 - accuracy: 0.6467 - val_loss: 51.1249 - val_accuracy: 0.5733\n",
      "Epoch 194/1000\n",
      "600/600 [==============================] - 0s 47us/step - loss: 46.6935 - accuracy: 0.5650 - val_loss: 28.3508 - val_accuracy: 0.4867\n",
      "Epoch 195/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 43.3898 - accuracy: 0.6233 - val_loss: 19.0973 - val_accuracy: 0.5600\n",
      "Epoch 196/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 43.5361 - accuracy: 0.5800 - val_loss: 58.3359 - val_accuracy: 0.5667\n",
      "Epoch 197/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 46.8063 - accuracy: 0.6050 - val_loss: 67.3066 - val_accuracy: 0.6067\n",
      "Epoch 198/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 48.6997 - accuracy: 0.6550 - val_loss: 61.4185 - val_accuracy: 0.5867\n",
      "Epoch 199/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 48.9065 - accuracy: 0.6633 - val_loss: 42.0294 - val_accuracy: 0.6200\n",
      "Epoch 200/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 42.3571 - accuracy: 0.6600 - val_loss: 34.0451 - val_accuracy: 0.5333\n",
      "Epoch 201/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 45.3773 - accuracy: 0.5717 - val_loss: 30.1072 - val_accuracy: 0.5400\n",
      "Epoch 202/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 46.1329 - accuracy: 0.6300 - val_loss: 30.7715 - val_accuracy: 0.5667\n",
      "Epoch 203/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 47.1693 - accuracy: 0.6500 - val_loss: 39.5177 - val_accuracy: 0.5533\n",
      "Epoch 204/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 35.4973 - accuracy: 0.593 - 0s 47us/step - loss: 45.4968 - accuracy: 0.5583 - val_loss: 59.9342 - val_accuracy: 0.5800\n",
      "Epoch 205/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 45.5183 - accuracy: 0.6150 - val_loss: 36.5196 - val_accuracy: 0.5667\n",
      "Epoch 206/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 46.5391 - accuracy: 0.5367 - val_loss: 43.8642 - val_accuracy: 0.4600\n",
      "Epoch 207/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 45.5412 - accuracy: 0.5467 - val_loss: 49.6044 - val_accuracy: 0.5600\n",
      "Epoch 208/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 46.8478 - accuracy: 0.6083 - val_loss: 52.5810 - val_accuracy: 0.5933\n",
      "Epoch 209/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 44.6563 - accuracy: 0.5667 - val_loss: 58.7114 - val_accuracy: 0.5067\n",
      "Epoch 210/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 46.8273 - accuracy: 0.5750 - val_loss: 29.9583 - val_accuracy: 0.6133\n",
      "Epoch 211/1000\n",
      "600/600 [==============================] - 0s 46us/step - loss: 45.2056 - accuracy: 0.6550 - val_loss: 54.3976 - val_accuracy: 0.5933\n",
      "Epoch 212/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 42.6674 - accuracy: 0.5867 - val_loss: 42.0193 - val_accuracy: 0.5667\n",
      "Epoch 213/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 41.4228 - accuracy: 0.5483 - val_loss: 30.4019 - val_accuracy: 0.6133\n",
      "Epoch 214/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 42.4499 - accuracy: 0.5500 - val_loss: 46.1960 - val_accuracy: 0.6200\n",
      "Epoch 215/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 42.5641 - accuracy: 0.6467 - val_loss: 81.3085 - val_accuracy: 0.5867\n",
      "Epoch 216/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 44.4779 - accuracy: 0.5300 - val_loss: 36.3295 - val_accuracy: 0.5267\n",
      "Epoch 217/1000\n",
      "600/600 [==============================] - 0s 48us/step - loss: 44.4845 - accuracy: 0.5600 - val_loss: 42.6365 - val_accuracy: 0.5533\n",
      "Epoch 218/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 43.8878 - accuracy: 0.5833 - val_loss: 53.0576 - val_accuracy: 0.6200\n",
      "Epoch 219/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 43.7895 - accuracy: 0.5800 - val_loss: 58.1284 - val_accuracy: 0.5067\n",
      "Epoch 220/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 44.2126 - accuracy: 0.6350 - val_loss: 19.1181 - val_accuracy: 0.5733\n",
      "Epoch 221/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 38.1631 - accuracy: 0.5550 - val_loss: 45.6379 - val_accuracy: 0.5600\n",
      "Epoch 222/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 41.7798 - accuracy: 0.5767 - val_loss: 49.4649 - val_accuracy: 0.6000\n",
      "Epoch 223/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 43.0804 - accuracy: 0.6083 - val_loss: 55.5320 - val_accuracy: 0.5667\n",
      "Epoch 224/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 43.2340 - accuracy: 0.5883 - val_loss: 65.1820 - val_accuracy: 0.5533\n",
      "Epoch 225/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 42.9242 - accuracy: 0.5967 - val_loss: 46.1247 - val_accuracy: 0.5867\n",
      "Epoch 226/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 43.6344 - accuracy: 0.5800 - val_loss: 43.5904 - val_accuracy: 0.5800\n",
      "Epoch 227/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 42.1764 - accuracy: 0.625 - 0s 47us/step - loss: 42.1132 - accuracy: 0.6017 - val_loss: 28.6168 - val_accuracy: 0.5867\n",
      "Epoch 228/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 41.4653 - accuracy: 0.6333 - val_loss: 32.1343 - val_accuracy: 0.5733\n",
      "Epoch 229/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 42.9986 - accuracy: 0.6000 - val_loss: 14.9166 - val_accuracy: 0.6067\n",
      "Epoch 230/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 39.8056 - accuracy: 0.6300 - val_loss: 34.5398 - val_accuracy: 0.5867\n",
      "Epoch 231/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 41.8033 - accuracy: 0.6083 - val_loss: 42.3162 - val_accuracy: 0.5467\n",
      "Epoch 232/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 42.1324 - accuracy: 0.6100 - val_loss: 56.5455 - val_accuracy: 0.5133\n",
      "Epoch 233/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 41.2587 - accuracy: 0.5900 - val_loss: 43.5948 - val_accuracy: 0.5867\n",
      "Epoch 234/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 38.9206 - accuracy: 0.5750 - val_loss: 47.7747 - val_accuracy: 0.5933\n",
      "Epoch 235/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 40.1695 - accuracy: 0.6250 - val_loss: 51.4771 - val_accuracy: 0.5467\n",
      "Epoch 236/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 41.8802 - accuracy: 0.5417 - val_loss: 51.8571 - val_accuracy: 0.6133\n",
      "Epoch 237/1000\n",
      "600/600 [==============================] - 0s 55us/step - loss: 39.7451 - accuracy: 0.5783 - val_loss: 42.8669 - val_accuracy: 0.6067\n",
      "Epoch 238/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 40.7296 - accuracy: 0.5967 - val_loss: 60.3445 - val_accuracy: 0.5733\n",
      "Epoch 239/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 42.6302 - accuracy: 0.5650 - val_loss: 67.1798 - val_accuracy: 0.4733\n",
      "Epoch 240/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 42.7693 - accuracy: 0.5733 - val_loss: 46.9333 - val_accuracy: 0.6200\n",
      "Epoch 241/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 42.2339 - accuracy: 0.5917 - val_loss: 30.4778 - val_accuracy: 0.5333\n",
      "Epoch 242/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 37.7717 - accuracy: 0.6050 - val_loss: 21.5167 - val_accuracy: 0.5667\n",
      "Epoch 243/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 39.2198 - accuracy: 0.5850 - val_loss: 49.7442 - val_accuracy: 0.5867\n",
      "Epoch 244/1000\n",
      "600/600 [==============================] - 0s 52us/step - loss: 39.5376 - accuracy: 0.6017 - val_loss: 56.7536 - val_accuracy: 0.5800\n",
      "Epoch 245/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 41.6528 - accuracy: 0.5983 - val_loss: 27.4287 - val_accuracy: 0.5933\n",
      "Epoch 246/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 36.8594 - accuracy: 0.5950 - val_loss: 46.0764 - val_accuracy: 0.5800\n",
      "Epoch 247/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 39.0942 - accuracy: 0.5883 - val_loss: 29.4063 - val_accuracy: 0.6000\n",
      "Epoch 248/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 38.9680 - accuracy: 0.5983 - val_loss: 49.6080 - val_accuracy: 0.5600\n",
      "Epoch 249/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 37.6851 - accuracy: 0.6167 - val_loss: 45.7935 - val_accuracy: 0.5667\n",
      "Epoch 250/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 41.2593 - accuracy: 0.5717 - val_loss: 51.9646 - val_accuracy: 0.5200\n",
      "Epoch 251/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 41.8069 - accuracy: 0.6517 - val_loss: 37.5974 - val_accuracy: 0.6067\n",
      "Epoch 252/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 38.6023 - accuracy: 0.5600 - val_loss: 50.5680 - val_accuracy: 0.5933\n",
      "Epoch 253/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 39.5494 - accuracy: 0.6000 - val_loss: 38.7755 - val_accuracy: 0.6333\n",
      "Epoch 254/1000\n",
      "600/600 [==============================] - 0s 52us/step - loss: 37.2040 - accuracy: 0.5917 - val_loss: 43.3472 - val_accuracy: 0.6000\n",
      "Epoch 255/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 41.9885 - accuracy: 0.6333 - val_loss: 34.5838 - val_accuracy: 0.5667\n",
      "Epoch 256/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 38.7791 - accuracy: 0.6567 - val_loss: 36.4037 - val_accuracy: 0.6000\n",
      "Epoch 257/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 39.1778 - accuracy: 0.6333 - val_loss: 31.6083 - val_accuracy: 0.5400\n",
      "Epoch 258/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 40.2587 - accuracy: 0.5967 - val_loss: 35.0798 - val_accuracy: 0.5333\n",
      "Epoch 259/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 38.3396 - accuracy: 0.5783 - val_loss: 31.6736 - val_accuracy: 0.5667\n",
      "Epoch 260/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 28.7453 - accuracy: 0.687 - 0s 37us/step - loss: 36.0771 - accuracy: 0.5750 - val_loss: 56.9616 - val_accuracy: 0.7133\n",
      "Epoch 261/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 39.4900 - accuracy: 0.6050 - val_loss: 14.7829 - val_accuracy: 0.4467\n",
      "Epoch 262/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 37.2517 - accuracy: 0.6100 - val_loss: 34.8166 - val_accuracy: 0.6200\n",
      "Epoch 263/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 38.9136 - accuracy: 0.6233 - val_loss: 46.9611 - val_accuracy: 0.4200\n",
      "Epoch 264/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 38.8098 - accuracy: 0.5483 - val_loss: 37.8671 - val_accuracy: 0.6000\n",
      "Epoch 265/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 39.1493 - accuracy: 0.6250 - val_loss: 30.3237 - val_accuracy: 0.4933\n",
      "Epoch 266/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 37.0002 - accuracy: 0.5683 - val_loss: 25.7867 - val_accuracy: 0.5733\n",
      "Epoch 267/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 35.9101 - accuracy: 0.5583 - val_loss: 33.5273 - val_accuracy: 0.5733\n",
      "Epoch 268/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 38.0982 - accuracy: 0.5583 - val_loss: 17.6115 - val_accuracy: 0.5133\n",
      "Epoch 269/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 39.3382 - accuracy: 0.5800 - val_loss: 32.4357 - val_accuracy: 0.5600\n",
      "Epoch 270/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 35.7878 - accuracy: 0.5667 - val_loss: 60.1985 - val_accuracy: 0.5800\n",
      "Epoch 271/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 39.7057 - accuracy: 0.6017 - val_loss: 36.3743 - val_accuracy: 0.5067\n",
      "Epoch 272/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 35.6805 - accuracy: 0.5717 - val_loss: 40.8493 - val_accuracy: 0.6000\n",
      "Epoch 273/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 37.0614 - accuracy: 0.6050 - val_loss: 15.9966 - val_accuracy: 0.5333\n",
      "Epoch 274/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 33.0674 - accuracy: 0.5633 - val_loss: 51.2123 - val_accuracy: 0.6267\n",
      "Epoch 275/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 37.6596 - accuracy: 0.5983 - val_loss: 12.4066 - val_accuracy: 0.4867\n",
      "Epoch 276/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 33.1277 - accuracy: 0.6133 - val_loss: 45.4097 - val_accuracy: 0.4867\n",
      "Epoch 277/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 37.1528 - accuracy: 0.5433 - val_loss: 35.8746 - val_accuracy: 0.5800\n",
      "Epoch 278/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 38.1688 - accuracy: 0.5917 - val_loss: 41.2928 - val_accuracy: 0.5200\n",
      "Epoch 279/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 37.3935 - accuracy: 0.5450 - val_loss: 46.4443 - val_accuracy: 0.5800\n",
      "Epoch 280/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 36.0564 - accuracy: 0.6033 - val_loss: 15.3169 - val_accuracy: 0.6000\n",
      "Epoch 281/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 35.4561 - accuracy: 0.6283 - val_loss: 19.8183 - val_accuracy: 0.5867\n",
      "Epoch 282/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 36.6711 - accuracy: 0.6017 - val_loss: 30.3324 - val_accuracy: 0.5667\n",
      "Epoch 283/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 37.2080 - accuracy: 0.5500 - val_loss: 58.7272 - val_accuracy: 0.6333\n",
      "Epoch 284/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 37.8304 - accuracy: 0.5983 - val_loss: 26.6337 - val_accuracy: 0.5533\n",
      "Epoch 285/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 36.3331 - accuracy: 0.5550 - val_loss: 36.0096 - val_accuracy: 0.5200\n",
      "Epoch 286/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 36.3839 - accuracy: 0.5950 - val_loss: 17.1866 - val_accuracy: 0.6133\n",
      "Epoch 287/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 33.9932 - accuracy: 0.6283 - val_loss: 32.3707 - val_accuracy: 0.5933\n",
      "Epoch 288/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 36.3674 - accuracy: 0.6083 - val_loss: 42.1420 - val_accuracy: 0.6000\n",
      "Epoch 289/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 35.0798 - accuracy: 0.6217 - val_loss: 47.3379 - val_accuracy: 0.5733\n",
      "Epoch 290/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 36.4478 - accuracy: 0.5733 - val_loss: 26.9976 - val_accuracy: 0.5800\n",
      "Epoch 291/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 36.3256 - accuracy: 0.5583 - val_loss: 21.2879 - val_accuracy: 0.5133\n",
      "Epoch 292/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 35.7007 - accuracy: 0.5600 - val_loss: 21.6583 - val_accuracy: 0.5667\n",
      "Epoch 293/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 35.4516 - accuracy: 0.5700 - val_loss: 46.8793 - val_accuracy: 0.6067\n",
      "Epoch 294/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 34.9818 - accuracy: 0.6017 - val_loss: 42.4919 - val_accuracy: 0.5533\n",
      "Epoch 295/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 34.9816 - accuracy: 0.5933 - val_loss: 15.6129 - val_accuracy: 0.6333\n",
      "Epoch 296/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 34.3646 - accuracy: 0.5783 - val_loss: 36.9858 - val_accuracy: 0.6000\n",
      "Epoch 297/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 35.8481 - accuracy: 0.6000 - val_loss: 15.1368 - val_accuracy: 0.5800\n",
      "Epoch 298/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 34.2477 - accuracy: 0.6267 - val_loss: 34.5815 - val_accuracy: 0.5933\n",
      "Epoch 299/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 34.0956 - accuracy: 0.5867 - val_loss: 29.1112 - val_accuracy: 0.5933\n",
      "Epoch 300/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 33.2262 - accuracy: 0.5850 - val_loss: 41.2912 - val_accuracy: 0.5867\n",
      "Epoch 301/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 36.3256 - accuracy: 0.6050 - val_loss: 14.3163 - val_accuracy: 0.6000\n",
      "Epoch 302/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 31.6424 - accuracy: 0.6333 - val_loss: 45.0852 - val_accuracy: 0.7133\n",
      "Epoch 303/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 32.6183 - accuracy: 0.5750 - val_loss: 39.0679 - val_accuracy: 0.5800\n",
      "Epoch 304/1000\n",
      "600/600 [==============================] - 0s 48us/step - loss: 34.3538 - accuracy: 0.5583 - val_loss: 30.6538 - val_accuracy: 0.5067\n",
      "Epoch 305/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 35.3636 - accuracy: 0.5633 - val_loss: 24.1211 - val_accuracy: 0.5333\n",
      "Epoch 306/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 34.7475 - accuracy: 0.5983 - val_loss: 23.8557 - val_accuracy: 0.6000\n",
      "Epoch 307/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 31.1849 - accuracy: 0.6217 - val_loss: 43.8190 - val_accuracy: 0.5667\n",
      "Epoch 308/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 34.9942 - accuracy: 0.5950 - val_loss: 32.5583 - val_accuracy: 0.5067\n",
      "Epoch 309/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 34.3923 - accuracy: 0.5817 - val_loss: 17.6074 - val_accuracy: 0.6000\n",
      "Epoch 310/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 33.7578 - accuracy: 0.6633 - val_loss: 34.0039 - val_accuracy: 0.6533\n",
      "Epoch 311/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 31.4646 - accuracy: 0.6283 - val_loss: 41.4767 - val_accuracy: 0.5867\n",
      "Epoch 312/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 33.2718 - accuracy: 0.5767 - val_loss: 25.2522 - val_accuracy: 0.5800\n",
      "Epoch 313/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 33.2957 - accuracy: 0.6317 - val_loss: 44.8115 - val_accuracy: 0.6067\n",
      "Epoch 314/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 35.8230 - accuracy: 0.6600 - val_loss: 33.8981 - val_accuracy: 0.6267\n",
      "Epoch 315/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 34.7044 - accuracy: 0.6233 - val_loss: 49.6644 - val_accuracy: 0.6400\n",
      "Epoch 316/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 34.8313 - accuracy: 0.6417 - val_loss: 14.3575 - val_accuracy: 0.6200\n",
      "Epoch 317/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 31.8569 - accuracy: 0.5883 - val_loss: 31.8336 - val_accuracy: 0.6067\n",
      "Epoch 318/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 33.7238 - accuracy: 0.6450 - val_loss: 51.3653 - val_accuracy: 0.5800\n",
      "Epoch 319/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 31.5670 - accuracy: 0.6350 - val_loss: 38.6840 - val_accuracy: 0.6200\n",
      "Epoch 320/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 33.7244 - accuracy: 0.6600 - val_loss: 14.3220 - val_accuracy: 0.6133\n",
      "Epoch 321/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 32.2541 - accuracy: 0.6383 - val_loss: 21.0502 - val_accuracy: 0.6533\n",
      "Epoch 322/1000\n",
      "600/600 [==============================] - 0s 48us/step - loss: 33.0951 - accuracy: 0.5967 - val_loss: 26.7041 - val_accuracy: 0.5467\n",
      "Epoch 323/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 34.9202 - accuracy: 0.5667 - val_loss: 38.7576 - val_accuracy: 0.4800\n",
      "Epoch 324/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 34.5139 - accuracy: 0.5700 - val_loss: 48.5131 - val_accuracy: 0.5200\n",
      "Epoch 325/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 33.9877 - accuracy: 0.5000 - val_loss: 45.9075 - val_accuracy: 0.6133\n",
      "Epoch 326/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 34.1503 - accuracy: 0.6633 - val_loss: 50.2733 - val_accuracy: 0.5667\n",
      "Epoch 327/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 32.1180 - accuracy: 0.6217 - val_loss: 32.3821 - val_accuracy: 0.5867\n",
      "Epoch 328/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 33.2900 - accuracy: 0.5667 - val_loss: 50.3637 - val_accuracy: 0.6733\n",
      "Epoch 329/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 33.9576 - accuracy: 0.6433 - val_loss: 27.9223 - val_accuracy: 0.5800\n",
      "Epoch 330/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 30.4393 - accuracy: 0.6100 - val_loss: 30.5286 - val_accuracy: 0.6467\n",
      "Epoch 331/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 32.8484 - accuracy: 0.6433 - val_loss: 16.1991 - val_accuracy: 0.5667\n",
      "Epoch 332/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 32.8186 - accuracy: 0.6533 - val_loss: 26.0292 - val_accuracy: 0.5800\n",
      "Epoch 333/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 31.6960 - accuracy: 0.6983 - val_loss: 18.2085 - val_accuracy: 0.5667\n",
      "Epoch 334/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 31.5018 - accuracy: 0.5583 - val_loss: 30.5599 - val_accuracy: 0.5867\n",
      "Epoch 335/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 32.7416 - accuracy: 0.6183 - val_loss: 29.5654 - val_accuracy: 0.5533\n",
      "Epoch 336/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 30.5482 - accuracy: 0.6283 - val_loss: 19.8105 - val_accuracy: 0.6267\n",
      "Epoch 337/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 31.3837 - accuracy: 0.6067 - val_loss: 16.0833 - val_accuracy: 0.6267\n",
      "Epoch 338/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 31.7398 - accuracy: 0.6267 - val_loss: 28.4070 - val_accuracy: 0.6067\n",
      "Epoch 339/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 33.3546 - accuracy: 0.6700 - val_loss: 51.4359 - val_accuracy: 0.6867\n",
      "Epoch 340/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 30.2382 - accuracy: 0.6833 - val_loss: 14.3497 - val_accuracy: 0.6267\n",
      "Epoch 341/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 32.6894 - accuracy: 0.6133 - val_loss: 44.1675 - val_accuracy: 0.5933\n",
      "Epoch 342/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 32.9735 - accuracy: 0.6083 - val_loss: 40.3672 - val_accuracy: 0.6000\n",
      "Epoch 343/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 32.1460 - accuracy: 0.6750 - val_loss: 48.3293 - val_accuracy: 0.6133\n",
      "Epoch 344/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 30.1779 - accuracy: 0.6083 - val_loss: 31.2465 - val_accuracy: 0.5933\n",
      "Epoch 345/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 31.1252 - accuracy: 0.6433 - val_loss: 14.5382 - val_accuracy: 0.5667\n",
      "Epoch 346/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 29.6681 - accuracy: 0.6900 - val_loss: 47.0029 - val_accuracy: 0.6000\n",
      "Epoch 347/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 32.9191 - accuracy: 0.6533 - val_loss: 25.8776 - val_accuracy: 0.6000\n",
      "Epoch 348/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 30.8937 - accuracy: 0.6067 - val_loss: 41.0811 - val_accuracy: 0.6067\n",
      "Epoch 349/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 29.9005 - accuracy: 0.6267 - val_loss: 43.0640 - val_accuracy: 0.5867\n",
      "Epoch 350/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 46.6451 - accuracy: 0.562 - 0s 37us/step - loss: 33.0529 - accuracy: 0.6733 - val_loss: 21.0576 - val_accuracy: 0.6200\n",
      "Epoch 351/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 29.8255 - accuracy: 0.6683 - val_loss: 46.5999 - val_accuracy: 0.6133\n",
      "Epoch 352/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 31.7942 - accuracy: 0.6267 - val_loss: 31.4935 - val_accuracy: 0.5867\n",
      "Epoch 353/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 32.0165 - accuracy: 0.6050 - val_loss: 12.4225 - val_accuracy: 0.6333\n",
      "Epoch 354/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 26.6115 - accuracy: 0.6250 - val_loss: 21.9695 - val_accuracy: 0.5933\n",
      "Epoch 355/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 30.6182 - accuracy: 0.5883 - val_loss: 32.0200 - val_accuracy: 0.5933\n",
      "Epoch 356/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 30.0788 - accuracy: 0.6533 - val_loss: 33.3181 - val_accuracy: 0.5867\n",
      "Epoch 357/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 30.8309 - accuracy: 0.6450 - val_loss: 18.2234 - val_accuracy: 0.6133\n",
      "Epoch 358/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 28.9243 - accuracy: 0.6583 - val_loss: 26.7840 - val_accuracy: 0.6067\n",
      "Epoch 359/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 29.8892 - accuracy: 0.6367 - val_loss: 55.4147 - val_accuracy: 0.5800\n",
      "Epoch 360/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 31.5908 - accuracy: 0.6433 - val_loss: 17.8407 - val_accuracy: 0.5933\n",
      "Epoch 361/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 31.6078 - accuracy: 0.6700 - val_loss: 49.8747 - val_accuracy: 0.5867\n",
      "Epoch 362/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 30.4054 - accuracy: 0.6433 - val_loss: 32.6471 - val_accuracy: 0.7533\n",
      "Epoch 363/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 32.1632 - accuracy: 0.6800 - val_loss: 16.7897 - val_accuracy: 0.6133\n",
      "Epoch 364/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 16.2606 - accuracy: 0.656 - 0s 43us/step - loss: 31.2930 - accuracy: 0.6600 - val_loss: 42.8564 - val_accuracy: 0.6000\n",
      "Epoch 365/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 31.1843 - accuracy: 0.6450 - val_loss: 42.0699 - val_accuracy: 0.6067\n",
      "Epoch 366/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 27.4553 - accuracy: 0.6467 - val_loss: 54.5972 - val_accuracy: 0.6067\n",
      "Epoch 367/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 30.3285 - accuracy: 0.6383 - val_loss: 40.6300 - val_accuracy: 0.5933\n",
      "Epoch 368/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 29.9469 - accuracy: 0.6150 - val_loss: 31.1367 - val_accuracy: 0.6000\n",
      "Epoch 369/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 30.9036 - accuracy: 0.6350 - val_loss: 37.3259 - val_accuracy: 0.5933\n",
      "Epoch 370/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 30.6357 - accuracy: 0.6350 - val_loss: 25.3859 - val_accuracy: 0.6067\n",
      "Epoch 371/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 29.2288 - accuracy: 0.6850 - val_loss: 31.5175 - val_accuracy: 0.7733\n",
      "Epoch 372/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 29.8674 - accuracy: 0.6633 - val_loss: 42.1497 - val_accuracy: 0.6133\n",
      "Epoch 373/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 30.2002 - accuracy: 0.6817 - val_loss: 29.3215 - val_accuracy: 0.6933\n",
      "Epoch 374/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 28.6727 - accuracy: 0.6700 - val_loss: 38.7939 - val_accuracy: 0.6667\n",
      "Epoch 375/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 30.8888 - accuracy: 0.6533 - val_loss: 26.5203 - val_accuracy: 0.6400\n",
      "Epoch 376/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 30.9177 - accuracy: 0.6900 - val_loss: 26.2984 - val_accuracy: 0.7000\n",
      "Epoch 377/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 30.8829 - accuracy: 0.6400 - val_loss: 23.2155 - val_accuracy: 0.6133\n",
      "Epoch 378/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 29.9060 - accuracy: 0.6533 - val_loss: 49.6093 - val_accuracy: 0.6267\n",
      "Epoch 379/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 30.8359 - accuracy: 0.6233 - val_loss: 31.5862 - val_accuracy: 0.6267\n",
      "Epoch 380/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 28.5929 - accuracy: 0.6550 - val_loss: 38.3045 - val_accuracy: 0.6133\n",
      "Epoch 381/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 28.0166 - accuracy: 0.6650 - val_loss: 28.9973 - val_accuracy: 0.6267\n",
      "Epoch 382/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 29.5540 - accuracy: 0.5933 - val_loss: 27.7594 - val_accuracy: 0.6067\n",
      "Epoch 383/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 29.1713 - accuracy: 0.6333 - val_loss: 20.6096 - val_accuracy: 0.6200\n",
      "Epoch 384/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 29.1193 - accuracy: 0.6067 - val_loss: 26.9435 - val_accuracy: 0.5467\n",
      "Epoch 385/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 29.9996 - accuracy: 0.6200 - val_loss: 18.9137 - val_accuracy: 0.6267\n",
      "Epoch 386/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 29.1841 - accuracy: 0.6583 - val_loss: 35.8641 - val_accuracy: 0.6067\n",
      "Epoch 387/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 29.2921 - accuracy: 0.6417 - val_loss: 44.6566 - val_accuracy: 0.6133\n",
      "Epoch 388/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 30.0551 - accuracy: 0.6817 - val_loss: 30.9970 - val_accuracy: 0.6667\n",
      "Epoch 389/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 30.0509 - accuracy: 0.6467 - val_loss: 36.8440 - val_accuracy: 0.6333\n",
      "Epoch 390/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 30.8627 - accuracy: 0.6683 - val_loss: 32.7263 - val_accuracy: 0.6067\n",
      "Epoch 391/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 28.2011 - accuracy: 0.6517 - val_loss: 27.6705 - val_accuracy: 0.6333\n",
      "Epoch 392/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 29.4483 - accuracy: 0.6683 - val_loss: 30.1255 - val_accuracy: 0.6333\n",
      "Epoch 393/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 28.5875 - accuracy: 0.5867 - val_loss: 39.7019 - val_accuracy: 0.6133\n",
      "Epoch 394/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 29.7760 - accuracy: 0.6733 - val_loss: 26.7292 - val_accuracy: 0.6000\n",
      "Epoch 395/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 29.2415 - accuracy: 0.6667 - val_loss: 20.9784 - val_accuracy: 0.6000\n",
      "Epoch 396/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 25.8078 - accuracy: 0.6517 - val_loss: 36.4086 - val_accuracy: 0.6133\n",
      "Epoch 397/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 38.8707 - accuracy: 0.500 - 0s 37us/step - loss: 29.6577 - accuracy: 0.6133 - val_loss: 21.6791 - val_accuracy: 0.5000\n",
      "Epoch 398/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 28.5423 - accuracy: 0.6667 - val_loss: 30.6938 - val_accuracy: 0.6133\n",
      "Epoch 399/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 27.5701 - accuracy: 0.6950 - val_loss: 43.4659 - val_accuracy: 0.6067\n",
      "Epoch 400/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 27.5185 - accuracy: 0.6150 - val_loss: 12.4800 - val_accuracy: 0.6400\n",
      "Epoch 401/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 27.6816 - accuracy: 0.6633 - val_loss: 34.3499 - val_accuracy: 0.6200\n",
      "Epoch 402/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 28.8840 - accuracy: 0.6083 - val_loss: 27.3877 - val_accuracy: 0.6267\n",
      "Epoch 403/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 27.7682 - accuracy: 0.6250 - val_loss: 36.0232 - val_accuracy: 0.6067\n",
      "Epoch 404/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 29.4593 - accuracy: 0.6767 - val_loss: 16.8500 - val_accuracy: 0.6000\n",
      "Epoch 405/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 27.0460 - accuracy: 0.6600 - val_loss: 30.6289 - val_accuracy: 0.5867\n",
      "Epoch 406/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 29.0300 - accuracy: 0.6683 - val_loss: 22.6001 - val_accuracy: 0.5867\n",
      "Epoch 407/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 28.8434 - accuracy: 0.6400 - val_loss: 44.4039 - val_accuracy: 0.6067\n",
      "Epoch 408/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 27.4015 - accuracy: 0.6833 - val_loss: 39.7046 - val_accuracy: 0.6333\n",
      "Epoch 409/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 37.4884 - accuracy: 0.687 - 0s 35us/step - loss: 27.1266 - accuracy: 0.6683 - val_loss: 22.8825 - val_accuracy: 0.6467\n",
      "Epoch 410/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 28.2940 - accuracy: 0.6617 - val_loss: 31.1278 - val_accuracy: 0.6667\n",
      "Epoch 411/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 27.4464 - accuracy: 0.6550 - val_loss: 19.1852 - val_accuracy: 0.6867\n",
      "Epoch 412/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 27.3550 - accuracy: 0.6917 - val_loss: 11.3494 - val_accuracy: 0.6067\n",
      "Epoch 413/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 23.9110 - accuracy: 0.6433 - val_loss: 40.8117 - val_accuracy: 0.5867\n",
      "Epoch 414/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 27.5309 - accuracy: 0.6683 - val_loss: 38.0864 - val_accuracy: 0.6067\n",
      "Epoch 415/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 26.9911 - accuracy: 0.6583 - val_loss: 24.2042 - val_accuracy: 0.5733\n",
      "Epoch 416/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 27.6215 - accuracy: 0.6350 - val_loss: 28.2701 - val_accuracy: 0.6200\n",
      "Epoch 417/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 27.6693 - accuracy: 0.6583 - val_loss: 22.2897 - val_accuracy: 0.6000\n",
      "Epoch 418/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 26.2747 - accuracy: 0.531 - 0s 40us/step - loss: 28.6294 - accuracy: 0.6483 - val_loss: 23.0283 - val_accuracy: 0.6000\n",
      "Epoch 419/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 27.5315 - accuracy: 0.6150 - val_loss: 31.9715 - val_accuracy: 0.6400\n",
      "Epoch 420/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 28.3394 - accuracy: 0.6300 - val_loss: 27.7008 - val_accuracy: 0.6267\n",
      "Epoch 421/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 28.1192 - accuracy: 0.6467 - val_loss: 28.5073 - val_accuracy: 0.6267\n",
      "Epoch 422/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 26.9010 - accuracy: 0.6300 - val_loss: 41.1479 - val_accuracy: 0.6200\n",
      "Epoch 423/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 28.6669 - accuracy: 0.6550 - val_loss: 20.6300 - val_accuracy: 0.6000\n",
      "Epoch 424/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 25.7984 - accuracy: 0.6417 - val_loss: 22.9445 - val_accuracy: 0.6067\n",
      "Epoch 425/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 27.4234 - accuracy: 0.6350 - val_loss: 19.3781 - val_accuracy: 0.6000\n",
      "Epoch 426/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 24.6061 - accuracy: 0.6050 - val_loss: 26.2344 - val_accuracy: 0.5933\n",
      "Epoch 427/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 26.0166 - accuracy: 0.6333 - val_loss: 24.2929 - val_accuracy: 0.6200\n",
      "Epoch 428/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 26.6230 - accuracy: 0.6600 - val_loss: 24.7442 - val_accuracy: 0.6067\n",
      "Epoch 429/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 24.5133 - accuracy: 0.6200 - val_loss: 27.2701 - val_accuracy: 0.6067\n",
      "Epoch 430/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 27.1583 - accuracy: 0.6550 - val_loss: 33.8812 - val_accuracy: 0.6667\n",
      "Epoch 431/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 28.2228 - accuracy: 0.6717 - val_loss: 37.6653 - val_accuracy: 0.6200\n",
      "Epoch 432/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 27.3123 - accuracy: 0.6200 - val_loss: 21.5561 - val_accuracy: 0.6200\n",
      "Epoch 433/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 26.5183 - accuracy: 0.6550 - val_loss: 37.4920 - val_accuracy: 0.6267\n",
      "Epoch 434/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 26.9321 - accuracy: 0.6733 - val_loss: 22.0455 - val_accuracy: 0.5867\n",
      "Epoch 435/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 26.1302 - accuracy: 0.6517 - val_loss: 27.0505 - val_accuracy: 0.6067\n",
      "Epoch 436/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 26.2751 - accuracy: 0.6633 - val_loss: 19.7166 - val_accuracy: 0.7533\n",
      "Epoch 437/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 25.8671 - accuracy: 0.6667 - val_loss: 22.5931 - val_accuracy: 0.6000\n",
      "Epoch 438/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 26.5622 - accuracy: 0.6883 - val_loss: 34.0428 - val_accuracy: 0.5933\n",
      "Epoch 439/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 26.7142 - accuracy: 0.6583 - val_loss: 13.9439 - val_accuracy: 0.7067\n",
      "Epoch 440/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 13.4597 - accuracy: 0.687 - 0s 35us/step - loss: 24.8506 - accuracy: 0.6017 - val_loss: 17.9154 - val_accuracy: 0.6400\n",
      "Epoch 441/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 25.7546 - accuracy: 0.6633 - val_loss: 29.1528 - val_accuracy: 0.6200\n",
      "Epoch 442/1000\n",
      "600/600 [==============================] - 0s 47us/step - loss: 25.2195 - accuracy: 0.6417 - val_loss: 20.3630 - val_accuracy: 0.6333\n",
      "Epoch 443/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 25.9410 - accuracy: 0.6633 - val_loss: 12.8285 - val_accuracy: 0.6400\n",
      "Epoch 444/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 22.0046 - accuracy: 0.6667 - val_loss: 28.3345 - val_accuracy: 0.6200\n",
      "Epoch 445/1000\n",
      "600/600 [==============================] - 0s 47us/step - loss: 26.5296 - accuracy: 0.6500 - val_loss: 20.0053 - val_accuracy: 0.6000\n",
      "Epoch 446/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 26.8887 - accuracy: 0.6433 - val_loss: 13.7864 - val_accuracy: 0.6200\n",
      "Epoch 447/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 25.1736 - accuracy: 0.6550 - val_loss: 29.9162 - val_accuracy: 0.6000\n",
      "Epoch 448/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 26.6621 - accuracy: 0.6417 - val_loss: 21.2536 - val_accuracy: 0.7400\n",
      "Epoch 449/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 25.9619 - accuracy: 0.6600 - val_loss: 22.2242 - val_accuracy: 0.6000\n",
      "Epoch 450/1000\n",
      "600/600 [==============================] - 0s 36us/step - loss: 25.6626 - accuracy: 0.6617 - val_loss: 20.9280 - val_accuracy: 0.6200\n",
      "Epoch 451/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 25.7486 - accuracy: 0.7217 - val_loss: 18.3583 - val_accuracy: 0.5867\n",
      "Epoch 452/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 24.5194 - accuracy: 0.6700 - val_loss: 23.8468 - val_accuracy: 0.6267\n",
      "Epoch 453/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 24.8819 - accuracy: 0.718 - 0s 43us/step - loss: 25.6735 - accuracy: 0.6867 - val_loss: 31.7954 - val_accuracy: 0.6067\n",
      "Epoch 454/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 26.5536 - accuracy: 0.6600 - val_loss: 15.1090 - val_accuracy: 0.6467\n",
      "Epoch 455/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 26.2616 - accuracy: 0.7217 - val_loss: 11.9916 - val_accuracy: 0.7333\n",
      "Epoch 456/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 23.8929 - accuracy: 0.6850 - val_loss: 16.8931 - val_accuracy: 0.6133\n",
      "Epoch 457/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 24.6678 - accuracy: 0.6517 - val_loss: 23.8324 - val_accuracy: 0.7200\n",
      "Epoch 458/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 24.7450 - accuracy: 0.6433 - val_loss: 20.2576 - val_accuracy: 0.6267\n",
      "Epoch 459/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 25.8626 - accuracy: 0.6067 - val_loss: 21.9686 - val_accuracy: 0.6067\n",
      "Epoch 460/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 26.1205 - accuracy: 0.6750 - val_loss: 13.7478 - val_accuracy: 0.6000\n",
      "Epoch 461/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 24.8439 - accuracy: 0.6683 - val_loss: 18.3822 - val_accuracy: 0.6400\n",
      "Epoch 462/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 24.9139 - accuracy: 0.6867 - val_loss: 33.4460 - val_accuracy: 0.6267\n",
      "Epoch 463/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 26.0366 - accuracy: 0.6767 - val_loss: 11.5847 - val_accuracy: 0.6067\n",
      "Epoch 464/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 24.7968 - accuracy: 0.6717 - val_loss: 39.2299 - val_accuracy: 0.7400\n",
      "Epoch 465/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 25.0745 - accuracy: 0.6733 - val_loss: 29.1562 - val_accuracy: 0.6400\n",
      "Epoch 466/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 23.8172 - accuracy: 0.6300 - val_loss: 26.4436 - val_accuracy: 0.6200\n",
      "Epoch 467/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 24.8468 - accuracy: 0.6817 - val_loss: 21.0806 - val_accuracy: 0.6000\n",
      "Epoch 468/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 24.7320 - accuracy: 0.6650 - val_loss: 31.4535 - val_accuracy: 0.6000\n",
      "Epoch 469/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 26.3404 - accuracy: 0.6883 - val_loss: 12.3524 - val_accuracy: 0.6267\n",
      "Epoch 470/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 23.7716 - accuracy: 0.6783 - val_loss: 26.3230 - val_accuracy: 0.6200\n",
      "Epoch 471/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 25.9426 - accuracy: 0.6850 - val_loss: 11.3680 - val_accuracy: 0.6133\n",
      "Epoch 472/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 24.6194 - accuracy: 0.6467 - val_loss: 19.1496 - val_accuracy: 0.6733\n",
      "Epoch 473/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 24.2393 - accuracy: 0.6917 - val_loss: 15.5963 - val_accuracy: 0.6267\n",
      "Epoch 474/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 23.1763 - accuracy: 0.6517 - val_loss: 16.8017 - val_accuracy: 0.6133\n",
      "Epoch 475/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 24.7310 - accuracy: 0.6733 - val_loss: 24.9070 - val_accuracy: 0.6133\n",
      "Epoch 476/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 24.1195 - accuracy: 0.6717 - val_loss: 40.3588 - val_accuracy: 0.6733\n",
      "Epoch 477/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 24.6338 - accuracy: 0.6700 - val_loss: 30.3890 - val_accuracy: 0.5733\n",
      "Epoch 478/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 24.8443 - accuracy: 0.6433 - val_loss: 16.4140 - val_accuracy: 0.6067\n",
      "Epoch 479/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 22.7164 - accuracy: 0.6750 - val_loss: 14.1308 - val_accuracy: 0.7000\n",
      "Epoch 480/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 25.1190 - accuracy: 0.7067 - val_loss: 27.2696 - val_accuracy: 0.6933\n",
      "Epoch 481/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 24.4672 - accuracy: 0.6967 - val_loss: 29.1271 - val_accuracy: 0.7267\n",
      "Epoch 482/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 24.4197 - accuracy: 0.6867 - val_loss: 23.9121 - val_accuracy: 0.6067\n",
      "Epoch 483/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 25.5749 - accuracy: 0.6533 - val_loss: 17.5945 - val_accuracy: 0.6000\n",
      "Epoch 484/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 23.6846 - accuracy: 0.6683 - val_loss: 19.5241 - val_accuracy: 0.6200\n",
      "Epoch 485/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 24.6620 - accuracy: 0.6900 - val_loss: 31.2268 - val_accuracy: 0.7200\n",
      "Epoch 486/1000\n",
      "600/600 [==============================] - 0s 30us/step - loss: 25.7379 - accuracy: 0.7067 - val_loss: 27.9981 - val_accuracy: 0.7467\n",
      "Epoch 487/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 23.7247 - accuracy: 0.6567 - val_loss: 17.3285 - val_accuracy: 0.6200\n",
      "Epoch 488/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 19.5102 - accuracy: 0.437 - 0s 35us/step - loss: 23.2362 - accuracy: 0.6517 - val_loss: 22.6973 - val_accuracy: 0.6400\n",
      "Epoch 489/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 24.2303 - accuracy: 0.6750 - val_loss: 21.6135 - val_accuracy: 0.6067\n",
      "Epoch 490/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 22.2960 - accuracy: 0.6617 - val_loss: 18.7416 - val_accuracy: 0.6133\n",
      "Epoch 491/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 24.1774 - accuracy: 0.6917 - val_loss: 24.7684 - val_accuracy: 0.7200\n",
      "Epoch 492/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 24.7909 - accuracy: 0.6883 - val_loss: 20.5639 - val_accuracy: 0.7133\n",
      "Epoch 493/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 25.0347 - accuracy: 0.7167 - val_loss: 30.9201 - val_accuracy: 0.6667\n",
      "Epoch 494/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 25.4276 - accuracy: 0.6950 - val_loss: 16.3231 - val_accuracy: 0.6267\n",
      "Epoch 495/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 23.2081 - accuracy: 0.6683 - val_loss: 14.8953 - val_accuracy: 0.6267\n",
      "Epoch 496/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 23.9219 - accuracy: 0.6383 - val_loss: 29.7596 - val_accuracy: 0.6133\n",
      "Epoch 497/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 22.8571 - accuracy: 0.6783 - val_loss: 33.3542 - val_accuracy: 0.6267\n",
      "Epoch 498/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 23.9916 - accuracy: 0.6750 - val_loss: 16.4627 - val_accuracy: 0.6867\n",
      "Epoch 499/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 24.0231 - accuracy: 0.7100 - val_loss: 9.4864 - val_accuracy: 0.7133\n",
      "Epoch 500/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 20.9901 - accuracy: 0.6717 - val_loss: 24.7212 - val_accuracy: 0.7400\n",
      "Epoch 501/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 23.6532 - accuracy: 0.6950 - val_loss: 21.5851 - val_accuracy: 0.6000\n",
      "Epoch 502/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 23.0101 - accuracy: 0.6883 - val_loss: 33.0418 - val_accuracy: 0.6400\n",
      "Epoch 503/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 24.4854 - accuracy: 0.6850 - val_loss: 11.1365 - val_accuracy: 0.6600\n",
      "Epoch 504/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 24.2903 - accuracy: 0.6900 - val_loss: 15.8871 - val_accuracy: 0.6267\n",
      "Epoch 505/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 23.4877 - accuracy: 0.7117 - val_loss: 27.9777 - val_accuracy: 0.7600\n",
      "Epoch 506/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 23.2328 - accuracy: 0.7017 - val_loss: 13.2333 - val_accuracy: 0.6267\n",
      "Epoch 507/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 23.4705 - accuracy: 0.6850 - val_loss: 15.9053 - val_accuracy: 0.7000\n",
      "Epoch 508/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 23.0481 - accuracy: 0.6650 - val_loss: 13.5463 - val_accuracy: 0.7267\n",
      "Epoch 509/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 22.4091 - accuracy: 0.7133 - val_loss: 27.0584 - val_accuracy: 0.6133\n",
      "Epoch 510/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 23.3316 - accuracy: 0.6583 - val_loss: 31.7222 - val_accuracy: 0.6200\n",
      "Epoch 511/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 23.0344 - accuracy: 0.6800 - val_loss: 33.8778 - val_accuracy: 0.6867\n",
      "Epoch 512/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 21.0454 - accuracy: 0.7000 - val_loss: 28.3973 - val_accuracy: 0.7200\n",
      "Epoch 513/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 23.3739 - accuracy: 0.6917 - val_loss: 24.6941 - val_accuracy: 0.6067\n",
      "Epoch 514/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 23.7643 - accuracy: 0.6733 - val_loss: 9.9250 - val_accuracy: 0.6067\n",
      "Epoch 515/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.9867 - accuracy: 0.7283 - val_loss: 26.4095 - val_accuracy: 0.6067\n",
      "Epoch 516/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 24.0801 - accuracy: 0.6783 - val_loss: 34.2171 - val_accuracy: 0.6267\n",
      "Epoch 517/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 23.9952 - accuracy: 0.6683 - val_loss: 20.3848 - val_accuracy: 0.5667\n",
      "Epoch 518/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 22.8920 - accuracy: 0.6517 - val_loss: 23.8262 - val_accuracy: 0.6267\n",
      "Epoch 519/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 23.6136 - accuracy: 0.6900 - val_loss: 44.1798 - val_accuracy: 0.6267\n",
      "Epoch 520/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 22.1685 - accuracy: 0.6717 - val_loss: 19.8124 - val_accuracy: 0.7333\n",
      "Epoch 521/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.3709 - accuracy: 0.6867 - val_loss: 23.1558 - val_accuracy: 0.6133\n",
      "Epoch 522/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 23.5534 - accuracy: 0.7217 - val_loss: 24.0745 - val_accuracy: 0.6067\n",
      "Epoch 523/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 20.2524 - accuracy: 0.6717 - val_loss: 17.6360 - val_accuracy: 0.7133\n",
      "Epoch 524/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 23.1077 - accuracy: 0.7567 - val_loss: 34.2673 - val_accuracy: 0.6067\n",
      "Epoch 525/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 22.0081 - accuracy: 0.6900 - val_loss: 24.3016 - val_accuracy: 0.5933\n",
      "Epoch 526/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 23.8079 - accuracy: 0.6817 - val_loss: 14.5678 - val_accuracy: 0.6667\n",
      "Epoch 527/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 22.2032 - accuracy: 0.6833 - val_loss: 37.9176 - val_accuracy: 0.6000\n",
      "Epoch 528/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 24.5351 - accuracy: 0.7083 - val_loss: 17.8661 - val_accuracy: 0.6067\n",
      "Epoch 529/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 23.6173 - accuracy: 0.7250 - val_loss: 29.9244 - val_accuracy: 0.6333\n",
      "Epoch 530/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 22.1510 - accuracy: 0.6650 - val_loss: 15.0840 - val_accuracy: 0.7467\n",
      "Epoch 531/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.6240 - accuracy: 0.7067 - val_loss: 20.0767 - val_accuracy: 0.6133\n",
      "Epoch 532/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 22.2953 - accuracy: 0.6383 - val_loss: 30.4786 - val_accuracy: 0.6000\n",
      "Epoch 533/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 21.0719 - accuracy: 0.6767 - val_loss: 17.8650 - val_accuracy: 0.6267\n",
      "Epoch 534/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 20.8507 - accuracy: 0.7417 - val_loss: 18.5298 - val_accuracy: 0.6000\n",
      "Epoch 535/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 21.7575 - accuracy: 0.6667 - val_loss: 15.7968 - val_accuracy: 0.6400\n",
      "Epoch 536/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 15.9446 - accuracy: 0.562 - 0s 35us/step - loss: 20.7546 - accuracy: 0.6867 - val_loss: 13.1869 - val_accuracy: 0.6200\n",
      "Epoch 537/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 21.5760 - accuracy: 0.7033 - val_loss: 33.6842 - val_accuracy: 0.7267\n",
      "Epoch 538/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 23.5993 - accuracy: 0.6967 - val_loss: 22.2230 - val_accuracy: 0.6067\n",
      "Epoch 539/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 22.1665 - accuracy: 0.6800 - val_loss: 14.0626 - val_accuracy: 0.6333\n",
      "Epoch 540/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 22.6459 - accuracy: 0.7133 - val_loss: 12.5242 - val_accuracy: 0.8267\n",
      "Epoch 541/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 21.5203 - accuracy: 0.7117 - val_loss: 20.6582 - val_accuracy: 0.6800\n",
      "Epoch 542/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 22.9103 - accuracy: 0.7017 - val_loss: 35.7425 - val_accuracy: 0.6067\n",
      "Epoch 543/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 22.7724 - accuracy: 0.6583 - val_loss: 30.8178 - val_accuracy: 0.7200\n",
      "Epoch 544/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 22.2897 - accuracy: 0.7133 - val_loss: 20.1457 - val_accuracy: 0.6933\n",
      "Epoch 545/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 21.2524 - accuracy: 0.7267 - val_loss: 17.8790 - val_accuracy: 0.6000\n",
      "Epoch 546/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 21.4693 - accuracy: 0.6933 - val_loss: 24.5777 - val_accuracy: 0.6200\n",
      "Epoch 547/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 21.5416 - accuracy: 0.6533 - val_loss: 10.2356 - val_accuracy: 0.6267\n",
      "Epoch 548/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 22.0944 - accuracy: 0.6667 - val_loss: 23.0728 - val_accuracy: 0.7267\n",
      "Epoch 549/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 23.4317 - accuracy: 0.7250 - val_loss: 13.4946 - val_accuracy: 0.6267\n",
      "Epoch 550/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 22.5301 - accuracy: 0.7200 - val_loss: 26.1570 - val_accuracy: 0.6200\n",
      "Epoch 551/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 23.2502 - accuracy: 0.7567 - val_loss: 14.5292 - val_accuracy: 0.7267\n",
      "Epoch 552/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 21.4123 - accuracy: 0.7200 - val_loss: 14.7593 - val_accuracy: 0.7467\n",
      "Epoch 553/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 21.8734 - accuracy: 0.7267 - val_loss: 36.0353 - val_accuracy: 0.6333\n",
      "Epoch 554/1000\n",
      "600/600 [==============================] - 0s 30us/step - loss: 22.6618 - accuracy: 0.6850 - val_loss: 12.7032 - val_accuracy: 0.6000\n",
      "Epoch 555/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 20.4420 - accuracy: 0.6750 - val_loss: 18.2818 - val_accuracy: 0.6400\n",
      "Epoch 556/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 21.6326 - accuracy: 0.6900 - val_loss: 24.4106 - val_accuracy: 0.6267\n",
      "Epoch 557/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 20.6902 - accuracy: 0.7283 - val_loss: 25.9778 - val_accuracy: 0.7933\n",
      "Epoch 558/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 20.9803 - accuracy: 0.7433 - val_loss: 18.1942 - val_accuracy: 0.5067\n",
      "Epoch 559/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 21.5636 - accuracy: 0.6750 - val_loss: 9.5260 - val_accuracy: 0.8267\n",
      "Epoch 560/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 19.7178 - accuracy: 0.7600 - val_loss: 21.5870 - val_accuracy: 0.7800\n",
      "Epoch 561/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.9950 - accuracy: 0.6767 - val_loss: 20.5925 - val_accuracy: 0.6000\n",
      "Epoch 562/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 21.7620 - accuracy: 0.6700 - val_loss: 35.7224 - val_accuracy: 0.7333\n",
      "Epoch 563/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 23.0716 - accuracy: 0.7450 - val_loss: 23.7213 - val_accuracy: 0.6067\n",
      "Epoch 564/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 21.9678 - accuracy: 0.6950 - val_loss: 27.2624 - val_accuracy: 0.6067\n",
      "Epoch 565/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 21.9628 - accuracy: 0.7017 - val_loss: 24.6293 - val_accuracy: 0.5933\n",
      "Epoch 566/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.2138 - accuracy: 0.7017 - val_loss: 23.0462 - val_accuracy: 0.6267\n",
      "Epoch 567/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.9623 - accuracy: 0.7167 - val_loss: 38.0845 - val_accuracy: 0.6133\n",
      "Epoch 568/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 22.7113 - accuracy: 0.7333 - val_loss: 15.7024 - val_accuracy: 0.6133\n",
      "Epoch 569/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 21.2939 - accuracy: 0.7067 - val_loss: 28.7426 - val_accuracy: 0.6000\n",
      "Epoch 570/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 22.0333 - accuracy: 0.7217 - val_loss: 10.1890 - val_accuracy: 0.6333\n",
      "Epoch 571/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 20.7407 - accuracy: 0.6867 - val_loss: 14.1932 - val_accuracy: 0.7933\n",
      "Epoch 572/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.1418 - accuracy: 0.7333 - val_loss: 21.2711 - val_accuracy: 0.6800\n",
      "Epoch 573/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 22.6623 - accuracy: 0.6967 - val_loss: 18.8520 - val_accuracy: 0.5933\n",
      "Epoch 574/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.8144 - accuracy: 0.7600 - val_loss: 13.9550 - val_accuracy: 0.6267\n",
      "Epoch 575/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 20.7811 - accuracy: 0.7200 - val_loss: 29.6274 - val_accuracy: 0.6133\n",
      "Epoch 576/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.6500 - accuracy: 0.7183 - val_loss: 24.1608 - val_accuracy: 0.7000\n",
      "Epoch 577/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.3031 - accuracy: 0.7917 - val_loss: 26.2332 - val_accuracy: 0.6067\n",
      "Epoch 578/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 24.1009 - accuracy: 0.500 - 0s 38us/step - loss: 21.2089 - accuracy: 0.7250 - val_loss: 14.6022 - val_accuracy: 0.6267\n",
      "Epoch 579/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 20.2680 - accuracy: 0.7000 - val_loss: 24.9809 - val_accuracy: 0.6200\n",
      "Epoch 580/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 20.4296 - accuracy: 0.7283 - val_loss: 22.4237 - val_accuracy: 0.6867\n",
      "Epoch 581/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 21.3762 - accuracy: 0.7017 - val_loss: 19.4662 - val_accuracy: 0.6867\n",
      "Epoch 582/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 21.8310 - accuracy: 0.7033 - val_loss: 21.7331 - val_accuracy: 0.6467\n",
      "Epoch 583/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 21.1229 - accuracy: 0.7233 - val_loss: 24.1179 - val_accuracy: 0.7267\n",
      "Epoch 584/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 19.8763 - accuracy: 0.6650 - val_loss: 21.6150 - val_accuracy: 0.6133\n",
      "Epoch 585/1000\n",
      "600/600 [==============================] - 0s 30us/step - loss: 21.9029 - accuracy: 0.7433 - val_loss: 16.6417 - val_accuracy: 0.6933\n",
      "Epoch 586/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.4200 - accuracy: 0.7617 - val_loss: 14.7413 - val_accuracy: 0.7200\n",
      "Epoch 587/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 19.5258 - accuracy: 0.7167 - val_loss: 28.0593 - val_accuracy: 0.6267\n",
      "Epoch 588/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.2352 - accuracy: 0.6850 - val_loss: 9.1744 - val_accuracy: 0.7000\n",
      "Epoch 589/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 18.1007 - accuracy: 0.7233 - val_loss: 27.1856 - val_accuracy: 0.6933\n",
      "Epoch 590/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 19.9560 - accuracy: 0.7583 - val_loss: 19.7428 - val_accuracy: 0.8200\n",
      "Epoch 591/1000\n",
      "600/600 [==============================] - 0s 31us/step - loss: 21.0987 - accuracy: 0.7583 - val_loss: 29.5403 - val_accuracy: 0.7000\n",
      "Epoch 592/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 20.5110 - accuracy: 0.6850 - val_loss: 35.4844 - val_accuracy: 0.7267\n",
      "Epoch 593/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.8257 - accuracy: 0.6933 - val_loss: 22.3744 - val_accuracy: 0.6067\n",
      "Epoch 594/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 20.6630 - accuracy: 0.7317 - val_loss: 20.0006 - val_accuracy: 0.7867\n",
      "Epoch 595/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 20.2228 - accuracy: 0.7100 - val_loss: 16.2802 - val_accuracy: 0.7067\n",
      "Epoch 596/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 20.1534 - accuracy: 0.7617 - val_loss: 22.8519 - val_accuracy: 0.7800\n",
      "Epoch 597/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 21.7018 - accuracy: 0.7833 - val_loss: 28.6558 - val_accuracy: 0.7133\n",
      "Epoch 598/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 20.7081 - accuracy: 0.7450 - val_loss: 26.4865 - val_accuracy: 0.6533\n",
      "Epoch 599/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 19.9436 - accuracy: 0.7467 - val_loss: 37.6502 - val_accuracy: 0.6733\n",
      "Epoch 600/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 19.5092 - accuracy: 0.7267 - val_loss: 24.5995 - val_accuracy: 0.6800\n",
      "Epoch 601/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 21.0754 - accuracy: 0.7533 - val_loss: 24.9597 - val_accuracy: 0.6867\n",
      "Epoch 602/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 20.9925 - accuracy: 0.7233 - val_loss: 12.4970 - val_accuracy: 0.7333\n",
      "Epoch 603/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 19.3740 - accuracy: 0.7483 - val_loss: 24.6071 - val_accuracy: 0.7733\n",
      "Epoch 604/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 19.9329 - accuracy: 0.7333 - val_loss: 18.1477 - val_accuracy: 0.6333\n",
      "Epoch 605/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 20.8376 - accuracy: 0.7583 - val_loss: 17.3337 - val_accuracy: 0.7467\n",
      "Epoch 606/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 19.9507 - accuracy: 0.7483 - val_loss: 27.1930 - val_accuracy: 0.5800\n",
      "Epoch 607/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 20.6670 - accuracy: 0.7233 - val_loss: 17.2789 - val_accuracy: 0.7333\n",
      "Epoch 608/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 21.6811 - accuracy: 0.7517 - val_loss: 12.5547 - val_accuracy: 0.6867\n",
      "Epoch 609/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 20.1507 - accuracy: 0.7467 - val_loss: 33.9174 - val_accuracy: 0.7733\n",
      "Epoch 610/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 20.3242 - accuracy: 0.7067 - val_loss: 30.1622 - val_accuracy: 0.6600\n",
      "Epoch 611/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 20.5770 - accuracy: 0.7433 - val_loss: 15.1933 - val_accuracy: 0.7333\n",
      "Epoch 612/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 19.7488 - accuracy: 0.7800 - val_loss: 19.0880 - val_accuracy: 0.7400\n",
      "Epoch 613/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 20.0601 - accuracy: 0.7050 - val_loss: 35.5370 - val_accuracy: 0.7267\n",
      "Epoch 614/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 21.0086 - accuracy: 0.7183 - val_loss: 15.3585 - val_accuracy: 0.7733\n",
      "Epoch 615/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 19.3375 - accuracy: 0.7750 - val_loss: 31.8915 - val_accuracy: 0.7133\n",
      "Epoch 616/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 20.1852 - accuracy: 0.7867 - val_loss: 25.4524 - val_accuracy: 0.6600\n",
      "Epoch 617/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 20.5388 - accuracy: 0.7050 - val_loss: 17.8696 - val_accuracy: 0.7333\n",
      "Epoch 618/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 20.8876 - accuracy: 0.7617 - val_loss: 8.3869 - val_accuracy: 0.7467\n",
      "Epoch 619/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 19.1570 - accuracy: 0.7567 - val_loss: 13.6617 - val_accuracy: 0.7333\n",
      "Epoch 620/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 19.3514 - accuracy: 0.7667 - val_loss: 24.1983 - val_accuracy: 0.7200\n",
      "Epoch 621/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 19.5047 - accuracy: 0.7517 - val_loss: 22.2391 - val_accuracy: 0.7267\n",
      "Epoch 622/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 20.3405 - accuracy: 0.7383 - val_loss: 15.7645 - val_accuracy: 0.5933\n",
      "Epoch 623/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 18.8328 - accuracy: 0.7300 - val_loss: 19.1498 - val_accuracy: 0.6267\n",
      "Epoch 624/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 18.7110 - accuracy: 0.7317 - val_loss: 21.5561 - val_accuracy: 0.6733\n",
      "Epoch 625/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 18.5957 - accuracy: 0.7683 - val_loss: 29.9628 - val_accuracy: 0.8000\n",
      "Epoch 626/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 20.2554 - accuracy: 0.7750 - val_loss: 11.1910 - val_accuracy: 0.7067\n",
      "Epoch 627/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 19.3972 - accuracy: 0.7917 - val_loss: 12.1943 - val_accuracy: 0.7867\n",
      "Epoch 628/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 19.7962 - accuracy: 0.7983 - val_loss: 12.1230 - val_accuracy: 0.7533\n",
      "Epoch 629/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.9077 - accuracy: 0.7183 - val_loss: 14.1700 - val_accuracy: 0.7133\n",
      "Epoch 630/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 13.9227 - accuracy: 0.906 - 0s 38us/step - loss: 18.7596 - accuracy: 0.6983 - val_loss: 22.5667 - val_accuracy: 0.7733\n",
      "Epoch 631/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 20.4161 - accuracy: 0.7550 - val_loss: 27.8534 - val_accuracy: 0.6867\n",
      "Epoch 632/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 20.9723 - accuracy: 0.7717 - val_loss: 12.7004 - val_accuracy: 0.7533\n",
      "Epoch 633/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 18.1795 - accuracy: 0.7350 - val_loss: 16.6962 - val_accuracy: 0.6667\n",
      "Epoch 634/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 19.5578 - accuracy: 0.7533 - val_loss: 21.6186 - val_accuracy: 0.7067\n",
      "Epoch 635/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 19.5908 - accuracy: 0.7267 - val_loss: 22.1890 - val_accuracy: 0.6600\n",
      "Epoch 636/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 19.7253 - accuracy: 0.7367 - val_loss: 27.0553 - val_accuracy: 0.7267\n",
      "Epoch 637/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 19.9066 - accuracy: 0.7783 - val_loss: 26.5152 - val_accuracy: 0.7067\n",
      "Epoch 638/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 18.9311 - accuracy: 0.7433 - val_loss: 26.1594 - val_accuracy: 0.7867\n",
      "Epoch 639/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 19.3592 - accuracy: 0.7967 - val_loss: 23.3145 - val_accuracy: 0.7733\n",
      "Epoch 640/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 19.7735 - accuracy: 0.7800 - val_loss: 31.5322 - val_accuracy: 0.7467\n",
      "Epoch 641/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 19.6116 - accuracy: 0.7483 - val_loss: 12.2655 - val_accuracy: 0.7267\n",
      "Epoch 642/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.0917 - accuracy: 0.6633 - val_loss: 31.6772 - val_accuracy: 0.7200\n",
      "Epoch 643/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 19.5464 - accuracy: 0.7533 - val_loss: 25.0837 - val_accuracy: 0.7133\n",
      "Epoch 644/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 19.2160 - accuracy: 0.7717 - val_loss: 29.2294 - val_accuracy: 0.7333\n",
      "Epoch 645/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 20.4008 - accuracy: 0.7450 - val_loss: 13.5934 - val_accuracy: 0.8000\n",
      "Epoch 646/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 18.3964 - accuracy: 0.7567 - val_loss: 29.9237 - val_accuracy: 0.7333\n",
      "Epoch 647/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.3661 - accuracy: 0.7350 - val_loss: 16.8126 - val_accuracy: 0.6400\n",
      "Epoch 648/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 18.8260 - accuracy: 0.7067 - val_loss: 19.4257 - val_accuracy: 0.6000\n",
      "Epoch 649/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 19.9655 - accuracy: 0.7100 - val_loss: 9.3018 - val_accuracy: 0.7133\n",
      "Epoch 650/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 16.0694 - accuracy: 0.7050 - val_loss: 17.7824 - val_accuracy: 0.7133\n",
      "Epoch 651/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.9895 - accuracy: 0.7383 - val_loss: 20.2722 - val_accuracy: 0.6133\n",
      "Epoch 652/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.9676 - accuracy: 0.7500 - val_loss: 21.8717 - val_accuracy: 0.7733\n",
      "Epoch 653/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 19.2834 - accuracy: 0.7500 - val_loss: 19.4816 - val_accuracy: 0.6800\n",
      "Epoch 654/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 19.0869 - accuracy: 0.7883 - val_loss: 12.4020 - val_accuracy: 0.7067\n",
      "Epoch 655/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 18.6095 - accuracy: 0.7650 - val_loss: 16.1051 - val_accuracy: 0.6933\n",
      "Epoch 656/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 16.5677 - accuracy: 0.7100 - val_loss: 21.0815 - val_accuracy: 0.7000\n",
      "Epoch 657/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 19.4311 - accuracy: 0.7783 - val_loss: 28.4652 - val_accuracy: 0.6733\n",
      "Epoch 658/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.7204 - accuracy: 0.7333 - val_loss: 23.3191 - val_accuracy: 0.6933\n",
      "Epoch 659/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 19.5361 - accuracy: 0.7633 - val_loss: 28.7918 - val_accuracy: 0.7867\n",
      "Epoch 660/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 19.5260 - accuracy: 0.7933 - val_loss: 26.1441 - val_accuracy: 0.7533\n",
      "Epoch 661/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 18.8173 - accuracy: 0.7550 - val_loss: 27.4143 - val_accuracy: 0.7533\n",
      "Epoch 662/1000\n",
      "600/600 [==============================] - 0s 47us/step - loss: 18.8396 - accuracy: 0.7517 - val_loss: 12.7643 - val_accuracy: 0.7000\n",
      "Epoch 663/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 18.7414 - accuracy: 0.7167 - val_loss: 24.8623 - val_accuracy: 0.6667\n",
      "Epoch 664/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 18.1395 - accuracy: 0.7267 - val_loss: 21.4819 - val_accuracy: 0.7467\n",
      "Epoch 665/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 18.8970 - accuracy: 0.7717 - val_loss: 22.6694 - val_accuracy: 0.5933\n",
      "Epoch 666/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 19.3109 - accuracy: 0.7083 - val_loss: 21.1755 - val_accuracy: 0.7333\n",
      "Epoch 667/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 23.2245 - accuracy: 0.875 - 0s 38us/step - loss: 17.5378 - accuracy: 0.7700 - val_loss: 28.2728 - val_accuracy: 0.7333\n",
      "Epoch 668/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.3782 - accuracy: 0.7667 - val_loss: 19.7888 - val_accuracy: 0.6800\n",
      "Epoch 669/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 19.3951 - accuracy: 0.7483 - val_loss: 21.7198 - val_accuracy: 0.7400\n",
      "Epoch 670/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 20.2700 - accuracy: 0.7633 - val_loss: 21.9199 - val_accuracy: 0.7533\n",
      "Epoch 671/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 19.5647 - accuracy: 0.7783 - val_loss: 16.7580 - val_accuracy: 0.8067\n",
      "Epoch 672/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 19.0550 - accuracy: 0.7333 - val_loss: 21.0344 - val_accuracy: 0.7133\n",
      "Epoch 673/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 19.6477 - accuracy: 0.7567 - val_loss: 15.1141 - val_accuracy: 0.7067\n",
      "Epoch 674/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 19.3341 - accuracy: 0.7517 - val_loss: 11.5657 - val_accuracy: 0.6867\n",
      "Epoch 675/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 17.7468 - accuracy: 0.7583 - val_loss: 24.1527 - val_accuracy: 0.6933\n",
      "Epoch 676/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 18.3342 - accuracy: 0.7383 - val_loss: 15.8826 - val_accuracy: 0.7133\n",
      "Epoch 677/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 14.7997 - accuracy: 0.750 - 0s 38us/step - loss: 18.6863 - accuracy: 0.7450 - val_loss: 16.3945 - val_accuracy: 0.6667\n",
      "Epoch 678/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 17.7497 - accuracy: 0.7483 - val_loss: 13.6296 - val_accuracy: 0.6667\n",
      "Epoch 679/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 17.8334 - accuracy: 0.7317 - val_loss: 17.8369 - val_accuracy: 0.7600\n",
      "Epoch 680/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 19.2436 - accuracy: 0.7567 - val_loss: 21.5828 - val_accuracy: 0.6133\n",
      "Epoch 681/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 18.8742 - accuracy: 0.7150 - val_loss: 24.9129 - val_accuracy: 0.7733\n",
      "Epoch 682/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.2389 - accuracy: 0.7567 - val_loss: 17.6896 - val_accuracy: 0.7200\n",
      "Epoch 683/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 18.8723 - accuracy: 0.7683 - val_loss: 17.5303 - val_accuracy: 0.6933\n",
      "Epoch 684/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 18.2173 - accuracy: 0.7717 - val_loss: 12.3686 - val_accuracy: 0.7733\n",
      "Epoch 685/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 18.9741 - accuracy: 0.7800 - val_loss: 19.1918 - val_accuracy: 0.7133\n",
      "Epoch 686/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.8547 - accuracy: 0.7750 - val_loss: 25.8875 - val_accuracy: 0.7467\n",
      "Epoch 687/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 22.8325 - accuracy: 0.718 - 0s 43us/step - loss: 18.0139 - accuracy: 0.7800 - val_loss: 17.0104 - val_accuracy: 0.8067\n",
      "Epoch 688/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.1551 - accuracy: 0.7467 - val_loss: 20.2688 - val_accuracy: 0.6667\n",
      "Epoch 689/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 18.7013 - accuracy: 0.7633 - val_loss: 16.1410 - val_accuracy: 0.7400\n",
      "Epoch 690/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 19.1880 - accuracy: 0.7950 - val_loss: 29.5796 - val_accuracy: 0.6867\n",
      "Epoch 691/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 17.9057 - accuracy: 0.7550 - val_loss: 18.5965 - val_accuracy: 0.6800\n",
      "Epoch 692/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 18.5483 - accuracy: 0.7550 - val_loss: 22.6005 - val_accuracy: 0.7000\n",
      "Epoch 693/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 18.6632 - accuracy: 0.7633 - val_loss: 18.6290 - val_accuracy: 0.7800\n",
      "Epoch 694/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 18.2303 - accuracy: 0.7700 - val_loss: 29.2070 - val_accuracy: 0.6667\n",
      "Epoch 695/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 18.2526 - accuracy: 0.7650 - val_loss: 12.5109 - val_accuracy: 0.7533\n",
      "Epoch 696/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 17.6028 - accuracy: 0.7700 - val_loss: 21.8267 - val_accuracy: 0.6867\n",
      "Epoch 697/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.4240 - accuracy: 0.7583 - val_loss: 24.1097 - val_accuracy: 0.7800\n",
      "Epoch 698/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.6989 - accuracy: 0.7433 - val_loss: 12.0304 - val_accuracy: 0.7200\n",
      "Epoch 699/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.4994 - accuracy: 0.7433 - val_loss: 21.9296 - val_accuracy: 0.7333\n",
      "Epoch 700/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 18.5750 - accuracy: 0.7600 - val_loss: 16.2159 - val_accuracy: 0.6600\n",
      "Epoch 701/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 18.6024 - accuracy: 0.7417 - val_loss: 15.4026 - val_accuracy: 0.6800\n",
      "Epoch 702/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 18.8229 - accuracy: 0.7783 - val_loss: 16.8470 - val_accuracy: 0.7200\n",
      "Epoch 703/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 18.6334 - accuracy: 0.7900 - val_loss: 9.8088 - val_accuracy: 0.8067\n",
      "Epoch 704/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.1890 - accuracy: 0.7733 - val_loss: 16.5679 - val_accuracy: 0.6733\n",
      "Epoch 705/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.1056 - accuracy: 0.7550 - val_loss: 21.0797 - val_accuracy: 0.7733\n",
      "Epoch 706/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.8040 - accuracy: 0.8033 - val_loss: 18.0158 - val_accuracy: 0.6933\n",
      "Epoch 707/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 18.8043 - accuracy: 0.7850 - val_loss: 21.2061 - val_accuracy: 0.6000\n",
      "Epoch 708/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 18.0396 - accuracy: 0.7167 - val_loss: 10.4125 - val_accuracy: 0.7133\n",
      "Epoch 709/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 17.4004 - accuracy: 0.7767 - val_loss: 13.6357 - val_accuracy: 0.6133\n",
      "Epoch 710/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 18.3813 - accuracy: 0.6767 - val_loss: 17.8560 - val_accuracy: 0.7133\n",
      "Epoch 711/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 17.9065 - accuracy: 0.7583 - val_loss: 11.7874 - val_accuracy: 0.6933\n",
      "Epoch 712/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 17.7222 - accuracy: 0.7667 - val_loss: 22.1253 - val_accuracy: 0.7267\n",
      "Epoch 713/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 21.8953 - accuracy: 0.906 - 0s 35us/step - loss: 18.7807 - accuracy: 0.7733 - val_loss: 17.2224 - val_accuracy: 0.7667\n",
      "Epoch 714/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 17.7805 - accuracy: 0.7750 - val_loss: 13.8931 - val_accuracy: 0.7733\n",
      "Epoch 715/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 16.0507 - accuracy: 0.7767 - val_loss: 9.6553 - val_accuracy: 0.7400\n",
      "Epoch 716/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 17.5619 - accuracy: 0.7533 - val_loss: 25.2195 - val_accuracy: 0.6867\n",
      "Epoch 717/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 17.2277 - accuracy: 0.7500 - val_loss: 13.9626 - val_accuracy: 0.6333\n",
      "Epoch 718/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 18.4062 - accuracy: 0.7600 - val_loss: 17.7920 - val_accuracy: 0.6133\n",
      "Epoch 719/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 18.6408 - accuracy: 0.7717 - val_loss: 8.4874 - val_accuracy: 0.8000\n",
      "Epoch 720/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 16.9078 - accuracy: 0.7450 - val_loss: 16.0169 - val_accuracy: 0.7867\n",
      "Epoch 721/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 16.5870 - accuracy: 0.812 - 0s 43us/step - loss: 18.7972 - accuracy: 0.7667 - val_loss: 14.3602 - val_accuracy: 0.7133\n",
      "Epoch 722/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.5864 - accuracy: 0.7750 - val_loss: 15.2137 - val_accuracy: 0.7467\n",
      "Epoch 723/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 17.6039 - accuracy: 0.7833 - val_loss: 20.0697 - val_accuracy: 0.7267\n",
      "Epoch 724/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 18.0120 - accuracy: 0.7750 - val_loss: 14.8234 - val_accuracy: 0.7667\n",
      "Epoch 725/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 18.0630 - accuracy: 0.7650 - val_loss: 14.6933 - val_accuracy: 0.6600\n",
      "Epoch 726/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 18.2885 - accuracy: 0.7533 - val_loss: 13.1458 - val_accuracy: 0.6933\n",
      "Epoch 727/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.7571 - accuracy: 0.7383 - val_loss: 22.8879 - val_accuracy: 0.6267\n",
      "Epoch 728/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 18.6038 - accuracy: 0.7600 - val_loss: 9.9774 - val_accuracy: 0.8200\n",
      "Epoch 729/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 17.6230 - accuracy: 0.8017 - val_loss: 23.0280 - val_accuracy: 0.7200\n",
      "Epoch 730/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.3510 - accuracy: 0.7400 - val_loss: 11.2896 - val_accuracy: 0.8267\n",
      "Epoch 731/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.3412 - accuracy: 0.7550 - val_loss: 20.4039 - val_accuracy: 0.6333\n",
      "Epoch 732/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 18.5152 - accuracy: 0.7550 - val_loss: 16.0202 - val_accuracy: 0.7933\n",
      "Epoch 733/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 17.1238 - accuracy: 0.7600 - val_loss: 11.9432 - val_accuracy: 0.7733\n",
      "Epoch 734/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.8827 - accuracy: 0.7450 - val_loss: 14.1908 - val_accuracy: 0.7600\n",
      "Epoch 735/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 16.9697 - accuracy: 0.7317 - val_loss: 9.0359 - val_accuracy: 0.8067\n",
      "Epoch 736/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 16.4702 - accuracy: 0.7500 - val_loss: 21.8714 - val_accuracy: 0.6067\n",
      "Epoch 737/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 17.4141 - accuracy: 0.7533 - val_loss: 12.2547 - val_accuracy: 0.7267\n",
      "Epoch 738/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 17.3580 - accuracy: 0.7683 - val_loss: 13.0356 - val_accuracy: 0.7467\n",
      "Epoch 739/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 17.5143 - accuracy: 0.7500 - val_loss: 20.9236 - val_accuracy: 0.7600\n",
      "Epoch 740/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 17.3229 - accuracy: 0.7367 - val_loss: 8.8475 - val_accuracy: 0.7533\n",
      "Epoch 741/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.3709 - accuracy: 0.7600 - val_loss: 20.4962 - val_accuracy: 0.7267\n",
      "Epoch 742/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.8231 - accuracy: 0.7667 - val_loss: 22.3843 - val_accuracy: 0.6467\n",
      "Epoch 743/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 17.6482 - accuracy: 0.7683 - val_loss: 10.3886 - val_accuracy: 0.6733\n",
      "Epoch 744/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.1229 - accuracy: 0.7533 - val_loss: 9.3939 - val_accuracy: 0.7333\n",
      "Epoch 745/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.9432 - accuracy: 0.7900 - val_loss: 9.1271 - val_accuracy: 0.6400\n",
      "Epoch 746/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 16.7389 - accuracy: 0.7883 - val_loss: 29.8532 - val_accuracy: 0.6800\n",
      "Epoch 747/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 17.1004 - accuracy: 0.7683 - val_loss: 18.9316 - val_accuracy: 0.7200\n",
      "Epoch 748/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 17.7242 - accuracy: 0.7517 - val_loss: 18.0286 - val_accuracy: 0.8400\n",
      "Epoch 749/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 17.3269 - accuracy: 0.7883 - val_loss: 15.7967 - val_accuracy: 0.7867\n",
      "Epoch 750/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 15.6720 - accuracy: 0.875 - 0s 40us/step - loss: 17.1932 - accuracy: 0.7750 - val_loss: 18.3720 - val_accuracy: 0.7333\n",
      "Epoch 751/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 17.7905 - accuracy: 0.7883 - val_loss: 25.9599 - val_accuracy: 0.7600\n",
      "Epoch 752/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 17.0985 - accuracy: 0.7400 - val_loss: 20.8653 - val_accuracy: 0.7200\n",
      "Epoch 753/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 16.8310 - accuracy: 0.7467 - val_loss: 14.3003 - val_accuracy: 0.8067\n",
      "Epoch 754/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 17.2126 - accuracy: 0.7800 - val_loss: 17.6392 - val_accuracy: 0.7600\n",
      "Epoch 755/1000\n",
      "600/600 [==============================] - 0s 47us/step - loss: 16.5148 - accuracy: 0.7583 - val_loss: 25.5090 - val_accuracy: 0.7867\n",
      "Epoch 756/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 17.0112 - accuracy: 0.7717 - val_loss: 21.1832 - val_accuracy: 0.7333\n",
      "Epoch 757/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 17.9477 - accuracy: 0.7617 - val_loss: 20.5546 - val_accuracy: 0.6067\n",
      "Epoch 758/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 15.9233 - accuracy: 0.7050 - val_loss: 14.0478 - val_accuracy: 0.7467\n",
      "Epoch 759/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 16.0070 - accuracy: 0.7700 - val_loss: 8.5822 - val_accuracy: 0.8000\n",
      "Epoch 760/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 15.9384 - accuracy: 0.7400 - val_loss: 15.6564 - val_accuracy: 0.7867\n",
      "Epoch 761/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.5392 - accuracy: 0.7733 - val_loss: 24.3637 - val_accuracy: 0.8133\n",
      "Epoch 762/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.9362 - accuracy: 0.7933 - val_loss: 29.5728 - val_accuracy: 0.7800\n",
      "Epoch 763/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 18.0915 - accuracy: 0.7567 - val_loss: 10.1661 - val_accuracy: 0.6400\n",
      "Epoch 764/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 17.2269 - accuracy: 0.7750 - val_loss: 23.2381 - val_accuracy: 0.7267\n",
      "Epoch 765/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.5958 - accuracy: 0.7783 - val_loss: 27.6439 - val_accuracy: 0.7933\n",
      "Epoch 766/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 18.0705 - accuracy: 0.7950 - val_loss: 15.0791 - val_accuracy: 0.7400\n",
      "Epoch 767/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 16.8935 - accuracy: 0.7850 - val_loss: 14.8751 - val_accuracy: 0.7267\n",
      "Epoch 768/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 17.3624 - accuracy: 0.7867 - val_loss: 16.2608 - val_accuracy: 0.7667\n",
      "Epoch 769/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.1696 - accuracy: 0.7550 - val_loss: 9.4823 - val_accuracy: 0.7333\n",
      "Epoch 770/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.2896 - accuracy: 0.7767 - val_loss: 23.1714 - val_accuracy: 0.6867\n",
      "Epoch 771/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.7141 - accuracy: 0.7800 - val_loss: 10.0631 - val_accuracy: 0.7133\n",
      "Epoch 772/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 17.4954 - accuracy: 0.7817 - val_loss: 15.5677 - val_accuracy: 0.7933\n",
      "Epoch 773/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.4150 - accuracy: 0.7700 - val_loss: 18.6716 - val_accuracy: 0.6333\n",
      "Epoch 774/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 17.0309 - accuracy: 0.7200 - val_loss: 13.4382 - val_accuracy: 0.8067\n",
      "Epoch 775/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 16.7312 - accuracy: 0.7617 - val_loss: 17.9097 - val_accuracy: 0.7733\n",
      "Epoch 776/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 16.7852 - accuracy: 0.7717 - val_loss: 14.8885 - val_accuracy: 0.7200\n",
      "Epoch 777/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 17.5063 - accuracy: 0.7783 - val_loss: 10.1172 - val_accuracy: 0.6133\n",
      "Epoch 778/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 15.3430 - accuracy: 0.7367 - val_loss: 28.8977 - val_accuracy: 0.6267\n",
      "Epoch 779/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 15.9117 - accuracy: 0.7800 - val_loss: 9.9265 - val_accuracy: 0.7733\n",
      "Epoch 780/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 15.8900 - accuracy: 0.7533 - val_loss: 18.6841 - val_accuracy: 0.6333\n",
      "Epoch 781/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.6088 - accuracy: 0.6867 - val_loss: 18.1148 - val_accuracy: 0.6400\n",
      "Epoch 782/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 17.8351 - accuracy: 0.7317 - val_loss: 8.7665 - val_accuracy: 0.6000\n",
      "Epoch 783/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.9449 - accuracy: 0.7683 - val_loss: 9.2417 - val_accuracy: 0.7667\n",
      "Epoch 784/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 16.1942 - accuracy: 0.7200 - val_loss: 29.9284 - val_accuracy: 0.7200\n",
      "Epoch 785/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 17.6930 - accuracy: 0.7667 - val_loss: 13.2465 - val_accuracy: 0.7333\n",
      "Epoch 786/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.2853 - accuracy: 0.7850 - val_loss: 28.1621 - val_accuracy: 0.7867\n",
      "Epoch 787/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.4488 - accuracy: 0.7917 - val_loss: 17.0181 - val_accuracy: 0.7533\n",
      "Epoch 788/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 17.7934 - accuracy: 0.7867 - val_loss: 16.9672 - val_accuracy: 0.7600\n",
      "Epoch 789/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.4676 - accuracy: 0.7600 - val_loss: 15.2330 - val_accuracy: 0.6667\n",
      "Epoch 790/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 17.5798 - accuracy: 0.7583 - val_loss: 19.8864 - val_accuracy: 0.6200\n",
      "Epoch 791/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 17.1108 - accuracy: 0.7617 - val_loss: 9.1238 - val_accuracy: 0.7400\n",
      "Epoch 792/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 15.3322 - accuracy: 0.7917 - val_loss: 9.0015 - val_accuracy: 0.8200\n",
      "Epoch 793/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 16.2726 - accuracy: 0.8033 - val_loss: 18.5226 - val_accuracy: 0.7333\n",
      "Epoch 794/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 16.8173 - accuracy: 0.8000 - val_loss: 11.6871 - val_accuracy: 0.7867\n",
      "Epoch 795/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 16.1276 - accuracy: 0.7983 - val_loss: 17.7561 - val_accuracy: 0.7533\n",
      "Epoch 796/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.4264 - accuracy: 0.8033 - val_loss: 20.1633 - val_accuracy: 0.6200\n",
      "Epoch 797/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 16.6505 - accuracy: 0.7717 - val_loss: 25.5610 - val_accuracy: 0.7067\n",
      "Epoch 798/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 16.6323 - accuracy: 0.7550 - val_loss: 8.2269 - val_accuracy: 0.7200\n",
      "Epoch 799/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 16.4697 - accuracy: 0.7800 - val_loss: 12.1896 - val_accuracy: 0.7733\n",
      "Epoch 800/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 17.3819 - accuracy: 0.7983 - val_loss: 9.2182 - val_accuracy: 0.7933\n",
      "Epoch 801/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 16.7304 - accuracy: 0.8000 - val_loss: 21.1754 - val_accuracy: 0.7533\n",
      "Epoch 802/1000\n",
      "600/600 [==============================] - 0s 30us/step - loss: 17.1598 - accuracy: 0.7867 - val_loss: 16.8762 - val_accuracy: 0.8200\n",
      "Epoch 803/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 17.7383 - accuracy: 0.8033 - val_loss: 10.5089 - val_accuracy: 0.7733\n",
      "Epoch 804/1000\n",
      "600/600 [==============================] - 0s 30us/step - loss: 15.1737 - accuracy: 0.7683 - val_loss: 19.1687 - val_accuracy: 0.7067\n",
      "Epoch 805/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 16.2335 - accuracy: 0.7867 - val_loss: 18.5383 - val_accuracy: 0.7600\n",
      "Epoch 806/1000\n",
      "600/600 [==============================] - 0s 30us/step - loss: 16.5775 - accuracy: 0.7867 - val_loss: 20.1208 - val_accuracy: 0.6733\n",
      "Epoch 807/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 15.4148 - accuracy: 0.7483 - val_loss: 15.2429 - val_accuracy: 0.8133\n",
      "Epoch 808/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 17.0428 - accuracy: 0.8000 - val_loss: 12.9202 - val_accuracy: 0.8533\n",
      "Epoch 809/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 12.5655 - accuracy: 0.843 - 0s 33us/step - loss: 15.3155 - accuracy: 0.7967 - val_loss: 14.0742 - val_accuracy: 0.7333\n",
      "Epoch 810/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 16.4811 - accuracy: 0.7650 - val_loss: 10.7578 - val_accuracy: 0.7933\n",
      "Epoch 811/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.5400 - accuracy: 0.7433 - val_loss: 17.5242 - val_accuracy: 0.7267\n",
      "Epoch 812/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.3503 - accuracy: 0.7550 - val_loss: 24.4151 - val_accuracy: 0.8200\n",
      "Epoch 813/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.9856 - accuracy: 0.7400 - val_loss: 18.6193 - val_accuracy: 0.7067\n",
      "Epoch 814/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.3018 - accuracy: 0.7700 - val_loss: 15.9555 - val_accuracy: 0.8467\n",
      "Epoch 815/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 16.5772 - accuracy: 0.875 - 0s 40us/step - loss: 16.7714 - accuracy: 0.8033 - val_loss: 20.0939 - val_accuracy: 0.6267\n",
      "Epoch 816/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 15.8704 - accuracy: 0.7933 - val_loss: 19.7148 - val_accuracy: 0.7533\n",
      "Epoch 817/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.6535 - accuracy: 0.8000 - val_loss: 18.7172 - val_accuracy: 0.6867\n",
      "Epoch 818/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.7217 - accuracy: 0.8033 - val_loss: 22.3510 - val_accuracy: 0.8000\n",
      "Epoch 819/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.4288 - accuracy: 0.7500 - val_loss: 17.7692 - val_accuracy: 0.6267\n",
      "Epoch 820/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 17.0146 - accuracy: 0.7917 - val_loss: 15.9477 - val_accuracy: 0.8133\n",
      "Epoch 821/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 15.7721 - accuracy: 0.7950 - val_loss: 14.7658 - val_accuracy: 0.7933\n",
      "Epoch 822/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.9939 - accuracy: 0.7817 - val_loss: 16.2814 - val_accuracy: 0.7733\n",
      "Epoch 823/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.7995 - accuracy: 0.7983 - val_loss: 10.6804 - val_accuracy: 0.7400\n",
      "Epoch 824/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 15.7090 - accuracy: 0.8250 - val_loss: 17.2205 - val_accuracy: 0.6667\n",
      "Epoch 825/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 16.6109 - accuracy: 0.7683 - val_loss: 19.5508 - val_accuracy: 0.8000\n",
      "Epoch 826/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 14.8383 - accuracy: 0.8017 - val_loss: 11.1220 - val_accuracy: 0.7200\n",
      "Epoch 827/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 16.3568 - accuracy: 0.7967 - val_loss: 21.5716 - val_accuracy: 0.7400\n",
      "Epoch 828/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 16.6255 - accuracy: 0.7933 - val_loss: 18.2719 - val_accuracy: 0.7933\n",
      "Epoch 829/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 17.2317 - accuracy: 0.8117 - val_loss: 22.6344 - val_accuracy: 0.7600\n",
      "Epoch 830/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.0928 - accuracy: 0.7667 - val_loss: 12.6511 - val_accuracy: 0.7067\n",
      "Epoch 831/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 15.0809 - accuracy: 0.7650 - val_loss: 13.1921 - val_accuracy: 0.7200\n",
      "Epoch 832/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 16.1884 - accuracy: 0.7950 - val_loss: 26.1795 - val_accuracy: 0.7867\n",
      "Epoch 833/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.3958 - accuracy: 0.7583 - val_loss: 19.7747 - val_accuracy: 0.7000\n",
      "Epoch 834/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.9380 - accuracy: 0.7850 - val_loss: 13.2413 - val_accuracy: 0.7200\n",
      "Epoch 835/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 15.5589 - accuracy: 0.7383 - val_loss: 19.2358 - val_accuracy: 0.7133\n",
      "Epoch 836/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 16.7711 - accuracy: 0.8150 - val_loss: 21.9175 - val_accuracy: 0.7467\n",
      "Epoch 837/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.3857 - accuracy: 0.7733 - val_loss: 18.2441 - val_accuracy: 0.6000\n",
      "Epoch 838/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.1294 - accuracy: 0.7833 - val_loss: 14.2823 - val_accuracy: 0.7200\n",
      "Epoch 839/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 16.1347 - accuracy: 0.7867 - val_loss: 9.9700 - val_accuracy: 0.6667\n",
      "Epoch 840/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 16.0362 - accuracy: 0.8017 - val_loss: 29.9682 - val_accuracy: 0.7600\n",
      "Epoch 841/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 14.8596 - accuracy: 0.7500 - val_loss: 22.9985 - val_accuracy: 0.8200\n",
      "Epoch 842/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.4612 - accuracy: 0.7883 - val_loss: 13.9394 - val_accuracy: 0.8000\n",
      "Epoch 843/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 17.0761 - accuracy: 0.7667 - val_loss: 11.8453 - val_accuracy: 0.7467\n",
      "Epoch 844/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 16.6874 - accuracy: 0.7950 - val_loss: 10.2609 - val_accuracy: 0.8200\n",
      "Epoch 845/1000\n",
      "600/600 [==============================] - 0s 48us/step - loss: 15.8468 - accuracy: 0.7950 - val_loss: 9.6839 - val_accuracy: 0.6467\n",
      "Epoch 846/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 15.4323 - accuracy: 0.7750 - val_loss: 19.4007 - val_accuracy: 0.8133\n",
      "Epoch 847/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 16.5320 - accuracy: 0.8033 - val_loss: 10.0731 - val_accuracy: 0.7800\n",
      "Epoch 848/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 16.1914 - accuracy: 0.8250 - val_loss: 22.3908 - val_accuracy: 0.6533\n",
      "Epoch 849/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.9115 - accuracy: 0.7767 - val_loss: 16.4683 - val_accuracy: 0.7800\n",
      "Epoch 850/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 15.5650 - accuracy: 0.7933 - val_loss: 19.8691 - val_accuracy: 0.7800\n",
      "Epoch 851/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 17.0339 - accuracy: 0.8083 - val_loss: 10.9340 - val_accuracy: 0.8467\n",
      "Epoch 852/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.2423 - accuracy: 0.7933 - val_loss: 20.7006 - val_accuracy: 0.7533\n",
      "Epoch 853/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.1139 - accuracy: 0.7867 - val_loss: 11.6706 - val_accuracy: 0.8067\n",
      "Epoch 854/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.0026 - accuracy: 0.8100 - val_loss: 8.6577 - val_accuracy: 0.7800\n",
      "Epoch 855/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 14.6485 - accuracy: 0.7717 - val_loss: 18.8685 - val_accuracy: 0.7400\n",
      "Epoch 856/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.1658 - accuracy: 0.7867 - val_loss: 13.0965 - val_accuracy: 0.8133\n",
      "Epoch 857/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 16.0920 - accuracy: 0.8067 - val_loss: 9.0014 - val_accuracy: 0.6467\n",
      "Epoch 858/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.3441 - accuracy: 0.7917 - val_loss: 9.7717 - val_accuracy: 0.7867\n",
      "Epoch 859/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 10.2103 - accuracy: 0.718 - 0s 42us/step - loss: 15.5729 - accuracy: 0.8083 - val_loss: 20.8281 - val_accuracy: 0.7733\n",
      "Epoch 860/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.6706 - accuracy: 0.8050 - val_loss: 10.4038 - val_accuracy: 0.7600\n",
      "Epoch 861/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 16.0766 - accuracy: 0.7983 - val_loss: 16.1174 - val_accuracy: 0.8467\n",
      "Epoch 862/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 16.2133 - accuracy: 0.7733 - val_loss: 22.4544 - val_accuracy: 0.7000\n",
      "Epoch 863/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.5072 - accuracy: 0.7717 - val_loss: 23.4312 - val_accuracy: 0.7867\n",
      "Epoch 864/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 16.0615 - accuracy: 0.8350 - val_loss: 20.1272 - val_accuracy: 0.8133\n",
      "Epoch 865/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 16.3974 - accuracy: 0.7900 - val_loss: 24.0502 - val_accuracy: 0.7333\n",
      "Epoch 866/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 17.2854 - accuracy: 0.8383 - val_loss: 10.9335 - val_accuracy: 0.7867\n",
      "Epoch 867/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 14.7035 - accuracy: 0.7933 - val_loss: 20.5825 - val_accuracy: 0.7000\n",
      "Epoch 868/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.6272 - accuracy: 0.8000 - val_loss: 20.0220 - val_accuracy: 0.8267\n",
      "Epoch 869/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.7043 - accuracy: 0.8250 - val_loss: 21.5213 - val_accuracy: 0.7467\n",
      "Epoch 870/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.8108 - accuracy: 0.8183 - val_loss: 18.9957 - val_accuracy: 0.7800\n",
      "Epoch 871/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.7051 - accuracy: 0.8033 - val_loss: 11.9806 - val_accuracy: 0.8067\n",
      "Epoch 872/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.1289 - accuracy: 0.8083 - val_loss: 13.8736 - val_accuracy: 0.7933\n",
      "Epoch 873/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 14.2913 - accuracy: 0.8017 - val_loss: 19.3951 - val_accuracy: 0.8400\n",
      "Epoch 874/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 16.2299 - accuracy: 0.7767 - val_loss: 15.8758 - val_accuracy: 0.6200\n",
      "Epoch 875/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 15.5846 - accuracy: 0.7600 - val_loss: 14.1009 - val_accuracy: 0.7467\n",
      "Epoch 876/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 14.2202 - accuracy: 0.781 - 0s 30us/step - loss: 15.6457 - accuracy: 0.7850 - val_loss: 16.5236 - val_accuracy: 0.7933\n",
      "Epoch 877/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.0926 - accuracy: 0.8100 - val_loss: 17.0850 - val_accuracy: 0.7467\n",
      "Epoch 878/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.0469 - accuracy: 0.7767 - val_loss: 22.7153 - val_accuracy: 0.8067\n",
      "Epoch 879/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 14.6267 - accuracy: 0.7200 - val_loss: 14.2393 - val_accuracy: 0.6333\n",
      "Epoch 880/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 17.2329 - accuracy: 0.687 - 0s 33us/step - loss: 14.9561 - accuracy: 0.7550 - val_loss: 8.5305 - val_accuracy: 0.7933\n",
      "Epoch 881/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.0325 - accuracy: 0.7483 - val_loss: 15.4769 - val_accuracy: 0.5867\n",
      "Epoch 882/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 15.8686 - accuracy: 0.7683 - val_loss: 10.5679 - val_accuracy: 0.7267\n",
      "Epoch 883/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 10.0367 - accuracy: 0.781 - 0s 32us/step - loss: 15.7073 - accuracy: 0.7933 - val_loss: 18.5522 - val_accuracy: 0.7400\n",
      "Epoch 884/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.9668 - accuracy: 0.7933 - val_loss: 13.6074 - val_accuracy: 0.8200\n",
      "Epoch 885/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.5348 - accuracy: 0.8017 - val_loss: 21.2824 - val_accuracy: 0.7000\n",
      "Epoch 886/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 15.0684 - accuracy: 0.7517 - val_loss: 14.3262 - val_accuracy: 0.6333\n",
      "Epoch 887/1000\n",
      "600/600 [==============================] - 0s 28us/step - loss: 16.1907 - accuracy: 0.8417 - val_loss: 23.6473 - val_accuracy: 0.8333\n",
      "Epoch 888/1000\n",
      "600/600 [==============================] - 0s 30us/step - loss: 15.3285 - accuracy: 0.7900 - val_loss: 14.3894 - val_accuracy: 0.7933\n",
      "Epoch 889/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.8604 - accuracy: 0.8150 - val_loss: 16.5284 - val_accuracy: 0.7200\n",
      "Epoch 890/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 15.9460 - accuracy: 0.7883 - val_loss: 14.1489 - val_accuracy: 0.7867\n",
      "Epoch 891/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.3303 - accuracy: 0.7867 - val_loss: 16.5182 - val_accuracy: 0.8267\n",
      "Epoch 892/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.6671 - accuracy: 0.8000 - val_loss: 17.3199 - val_accuracy: 0.7533\n",
      "Epoch 893/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 16.5628 - accuracy: 0.8067 - val_loss: 15.9365 - val_accuracy: 0.8000\n",
      "Epoch 894/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 14.6916 - accuracy: 0.8300 - val_loss: 17.2163 - val_accuracy: 0.8067\n",
      "Epoch 895/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 16.3476 - accuracy: 0.7900 - val_loss: 18.6276 - val_accuracy: 0.7133\n",
      "Epoch 896/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 15.4489 - accuracy: 0.8167 - val_loss: 20.2952 - val_accuracy: 0.6133\n",
      "Epoch 897/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 24.1427 - accuracy: 0.656 - 0s 40us/step - loss: 15.8430 - accuracy: 0.7717 - val_loss: 15.4616 - val_accuracy: 0.7867\n",
      "Epoch 898/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 13.4621 - accuracy: 0.7700 - val_loss: 21.9214 - val_accuracy: 0.6800\n",
      "Epoch 899/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.3100 - accuracy: 0.7900 - val_loss: 15.2852 - val_accuracy: 0.8200\n",
      "Epoch 900/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.3200 - accuracy: 0.7733 - val_loss: 17.7848 - val_accuracy: 0.6400\n",
      "Epoch 901/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 16.3648 - accuracy: 0.7383 - val_loss: 11.5670 - val_accuracy: 0.7867\n",
      "Epoch 902/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 15.2571 - accuracy: 0.7833 - val_loss: 17.8134 - val_accuracy: 0.7467\n",
      "Epoch 903/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 16.1864 - accuracy: 0.7783 - val_loss: 14.4168 - val_accuracy: 0.7933\n",
      "Epoch 904/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.7266 - accuracy: 0.7867 - val_loss: 10.8933 - val_accuracy: 0.7733\n",
      "Epoch 905/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 13.5025 - accuracy: 0.8033 - val_loss: 21.3513 - val_accuracy: 0.8267\n",
      "Epoch 906/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.9670 - accuracy: 0.8083 - val_loss: 9.4672 - val_accuracy: 0.7800\n",
      "Epoch 907/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 9.9332 - accuracy: 0.81 - 0s 40us/step - loss: 15.5034 - accuracy: 0.7300 - val_loss: 19.9851 - val_accuracy: 0.8200\n",
      "Epoch 908/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 15.4314 - accuracy: 0.8067 - val_loss: 14.8343 - val_accuracy: 0.7667\n",
      "Epoch 909/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 16.0508 - accuracy: 0.8317 - val_loss: 20.4812 - val_accuracy: 0.8267\n",
      "Epoch 910/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 14.4750 - accuracy: 0.8267 - val_loss: 18.9166 - val_accuracy: 0.8200\n",
      "Epoch 911/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.3555 - accuracy: 0.8050 - val_loss: 18.9993 - val_accuracy: 0.6400\n",
      "Epoch 912/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.7133 - accuracy: 0.8183 - val_loss: 23.1634 - val_accuracy: 0.8000\n",
      "Epoch 913/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 22.3638 - accuracy: 0.781 - 0s 43us/step - loss: 15.3310 - accuracy: 0.8383 - val_loss: 17.5909 - val_accuracy: 0.7667\n",
      "Epoch 914/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.6234 - accuracy: 0.8033 - val_loss: 11.6890 - val_accuracy: 0.8400\n",
      "Epoch 915/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.0082 - accuracy: 0.8417 - val_loss: 20.5650 - val_accuracy: 0.8133\n",
      "Epoch 916/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 14.7985 - accuracy: 0.8100 - val_loss: 17.0959 - val_accuracy: 0.6400\n",
      "Epoch 917/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 16.0365 - accuracy: 0.8150 - val_loss: 14.3440 - val_accuracy: 0.8533\n",
      "Epoch 918/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.6943 - accuracy: 0.8367 - val_loss: 9.4722 - val_accuracy: 0.7667\n",
      "Epoch 919/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.2035 - accuracy: 0.7817 - val_loss: 19.6331 - val_accuracy: 0.8400\n",
      "Epoch 920/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.6889 - accuracy: 0.8067 - val_loss: 13.4677 - val_accuracy: 0.7600\n",
      "Epoch 921/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.5893 - accuracy: 0.7800 - val_loss: 13.9919 - val_accuracy: 0.7467\n",
      "Epoch 922/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 15.0616 - accuracy: 0.7900 - val_loss: 17.9469 - val_accuracy: 0.8467\n",
      "Epoch 923/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 15.2673 - accuracy: 0.7767 - val_loss: 8.2070 - val_accuracy: 0.7667\n",
      "Epoch 924/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.2452 - accuracy: 0.8000 - val_loss: 9.8999 - val_accuracy: 0.8533\n",
      "Epoch 925/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 15.9161 - accuracy: 0.8000 - val_loss: 15.5349 - val_accuracy: 0.7267\n",
      "Epoch 926/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.5267 - accuracy: 0.7483 - val_loss: 16.4564 - val_accuracy: 0.8067\n",
      "Epoch 927/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.5820 - accuracy: 0.8183 - val_loss: 21.9263 - val_accuracy: 0.7000\n",
      "Epoch 928/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.5791 - accuracy: 0.8067 - val_loss: 8.4343 - val_accuracy: 0.8600\n",
      "Epoch 929/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 13.6685 - accuracy: 0.7967 - val_loss: 16.9916 - val_accuracy: 0.8067\n",
      "Epoch 930/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.9473 - accuracy: 0.7583 - val_loss: 8.7058 - val_accuracy: 0.8200\n",
      "Epoch 931/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.6812 - accuracy: 0.8000 - val_loss: 12.4042 - val_accuracy: 0.8333\n",
      "Epoch 932/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.8553 - accuracy: 0.7900 - val_loss: 14.8423 - val_accuracy: 0.8067\n",
      "Epoch 933/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 15.4494 - accuracy: 0.8350 - val_loss: 21.0786 - val_accuracy: 0.7467\n",
      "Epoch 934/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 13.0822 - accuracy: 0.7283 - val_loss: 8.8533 - val_accuracy: 0.7333\n",
      "Epoch 935/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 14.8863 - accuracy: 0.7550 - val_loss: 12.1648 - val_accuracy: 0.8267\n",
      "Epoch 936/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.0941 - accuracy: 0.7783 - val_loss: 11.0134 - val_accuracy: 0.8133\n",
      "Epoch 937/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.3966 - accuracy: 0.7917 - val_loss: 15.3714 - val_accuracy: 0.7333\n",
      "Epoch 938/1000\n",
      "600/600 [==============================] - ETA: 0s - loss: 15.1591 - accuracy: 0.750 - 0s 35us/step - loss: 14.6492 - accuracy: 0.7933 - val_loss: 7.8008 - val_accuracy: 0.7467\n",
      "Epoch 939/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 14.6551 - accuracy: 0.8117 - val_loss: 16.3896 - val_accuracy: 0.7800\n",
      "Epoch 940/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 15.2817 - accuracy: 0.7833 - val_loss: 11.3277 - val_accuracy: 0.7133\n",
      "Epoch 941/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.3620 - accuracy: 0.8017 - val_loss: 17.2826 - val_accuracy: 0.8467\n",
      "Epoch 942/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 15.9156 - accuracy: 0.8333 - val_loss: 12.2111 - val_accuracy: 0.8467\n",
      "Epoch 943/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 15.2595 - accuracy: 0.8050 - val_loss: 9.8087 - val_accuracy: 0.7733\n",
      "Epoch 944/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.0327 - accuracy: 0.7983 - val_loss: 12.1104 - val_accuracy: 0.7533\n",
      "Epoch 945/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 14.5342 - accuracy: 0.7467 - val_loss: 13.6755 - val_accuracy: 0.6267\n",
      "Epoch 946/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 14.5386 - accuracy: 0.7800 - val_loss: 13.3454 - val_accuracy: 0.8333\n",
      "Epoch 947/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 14.3078 - accuracy: 0.8083 - val_loss: 8.7757 - val_accuracy: 0.8467\n",
      "Epoch 948/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 14.0600 - accuracy: 0.8100 - val_loss: 9.2023 - val_accuracy: 0.8200\n",
      "Epoch 949/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 14.8380 - accuracy: 0.7867 - val_loss: 11.1268 - val_accuracy: 0.7000\n",
      "Epoch 950/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 14.9677 - accuracy: 0.7800 - val_loss: 19.8909 - val_accuracy: 0.7600\n",
      "Epoch 951/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 15.7715 - accuracy: 0.7783 - val_loss: 15.4487 - val_accuracy: 0.6933\n",
      "Epoch 952/1000\n",
      "600/600 [==============================] - 0s 42us/step - loss: 15.2983 - accuracy: 0.7300 - val_loss: 24.4020 - val_accuracy: 0.6467\n",
      "Epoch 953/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 15.8042 - accuracy: 0.7700 - val_loss: 16.2920 - val_accuracy: 0.7200\n",
      "Epoch 954/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 14.7099 - accuracy: 0.8150 - val_loss: 9.6651 - val_accuracy: 0.7800\n",
      "Epoch 955/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 14.6156 - accuracy: 0.8033 - val_loss: 20.0070 - val_accuracy: 0.8333\n",
      "Epoch 956/1000\n",
      "600/600 [==============================] - 0s 41us/step - loss: 15.1618 - accuracy: 0.7967 - val_loss: 9.7675 - val_accuracy: 0.7533\n",
      "Epoch 957/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 14.8883 - accuracy: 0.7733 - val_loss: 19.8121 - val_accuracy: 0.7533\n",
      "Epoch 958/1000\n",
      "600/600 [==============================] - 0s 45us/step - loss: 14.7770 - accuracy: 0.7800 - val_loss: 21.9675 - val_accuracy: 0.7933\n",
      "Epoch 959/1000\n",
      "600/600 [==============================] - 0s 43us/step - loss: 14.8142 - accuracy: 0.8017 - val_loss: 18.8719 - val_accuracy: 0.7333\n",
      "Epoch 960/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.2409 - accuracy: 0.7833 - val_loss: 18.0113 - val_accuracy: 0.7800\n",
      "Epoch 961/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.2687 - accuracy: 0.8100 - val_loss: 18.6845 - val_accuracy: 0.7867\n",
      "Epoch 962/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 15.2502 - accuracy: 0.7967 - val_loss: 21.9475 - val_accuracy: 0.8467\n",
      "Epoch 963/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 16.3829 - accuracy: 0.8150 - val_loss: 23.8542 - val_accuracy: 0.8067\n",
      "Epoch 964/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 14.6534 - accuracy: 0.7767 - val_loss: 21.9930 - val_accuracy: 0.7667\n",
      "Epoch 965/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 15.1153 - accuracy: 0.7933 - val_loss: 8.0554 - val_accuracy: 0.8200\n",
      "Epoch 966/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.3308 - accuracy: 0.8150 - val_loss: 20.4703 - val_accuracy: 0.8467\n",
      "Epoch 967/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 15.7233 - accuracy: 0.8367 - val_loss: 20.8837 - val_accuracy: 0.7400\n",
      "Epoch 968/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 15.5864 - accuracy: 0.8150 - val_loss: 9.4670 - val_accuracy: 0.7533\n",
      "Epoch 969/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.1757 - accuracy: 0.8017 - val_loss: 12.6466 - val_accuracy: 0.7867\n",
      "Epoch 970/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 14.9168 - accuracy: 0.8317 - val_loss: 24.0926 - val_accuracy: 0.8267\n",
      "Epoch 971/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.1494 - accuracy: 0.7983 - val_loss: 18.1593 - val_accuracy: 0.7400\n",
      "Epoch 972/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 14.5801 - accuracy: 0.8067 - val_loss: 16.9761 - val_accuracy: 0.7867\n",
      "Epoch 973/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 14.2444 - accuracy: 0.7733 - val_loss: 19.9961 - val_accuracy: 0.7667\n",
      "Epoch 974/1000\n",
      "600/600 [==============================] - 0s 40us/step - loss: 14.3358 - accuracy: 0.8017 - val_loss: 19.4699 - val_accuracy: 0.8200\n",
      "Epoch 975/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.2742 - accuracy: 0.8050 - val_loss: 14.3838 - val_accuracy: 0.7667\n",
      "Epoch 976/1000\n",
      "600/600 [==============================] - 0s 38us/step - loss: 15.6828 - accuracy: 0.8100 - val_loss: 11.7157 - val_accuracy: 0.8067\n",
      "Epoch 977/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 14.6601 - accuracy: 0.7967 - val_loss: 19.3902 - val_accuracy: 0.8600\n",
      "Epoch 978/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.2388 - accuracy: 0.8250 - val_loss: 20.5954 - val_accuracy: 0.8333\n",
      "Epoch 979/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.4307 - accuracy: 0.8250 - val_loss: 8.5469 - val_accuracy: 0.8067\n",
      "Epoch 980/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 14.6132 - accuracy: 0.8083 - val_loss: 13.8092 - val_accuracy: 0.8733\n",
      "Epoch 981/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 15.3938 - accuracy: 0.8000 - val_loss: 10.5626 - val_accuracy: 0.8067\n",
      "Epoch 982/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.2659 - accuracy: 0.8167 - val_loss: 19.3609 - val_accuracy: 0.7733\n",
      "Epoch 983/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.5303 - accuracy: 0.8150 - val_loss: 17.9299 - val_accuracy: 0.7467\n",
      "Epoch 984/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.0787 - accuracy: 0.7567 - val_loss: 6.9338 - val_accuracy: 0.7400\n",
      "Epoch 985/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 14.1987 - accuracy: 0.7883 - val_loss: 18.7261 - val_accuracy: 0.8333\n",
      "Epoch 986/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.0274 - accuracy: 0.8350 - val_loss: 24.1478 - val_accuracy: 0.8000\n",
      "Epoch 987/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.3433 - accuracy: 0.8117 - val_loss: 11.0429 - val_accuracy: 0.7933\n",
      "Epoch 988/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 14.3740 - accuracy: 0.7900 - val_loss: 7.6616 - val_accuracy: 0.7533\n",
      "Epoch 989/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.1700 - accuracy: 0.8017 - val_loss: 8.9545 - val_accuracy: 0.8267\n",
      "Epoch 990/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 13.0465 - accuracy: 0.7817 - val_loss: 22.0141 - val_accuracy: 0.8400\n",
      "Epoch 991/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 15.4641 - accuracy: 0.8000 - val_loss: 19.4052 - val_accuracy: 0.8333\n",
      "Epoch 992/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.3912 - accuracy: 0.7950 - val_loss: 14.1124 - val_accuracy: 0.7333\n",
      "Epoch 993/1000\n",
      "600/600 [==============================] - 0s 37us/step - loss: 14.1005 - accuracy: 0.8133 - val_loss: 12.2916 - val_accuracy: 0.8467\n",
      "Epoch 994/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.3596 - accuracy: 0.8000 - val_loss: 11.6854 - val_accuracy: 0.7667\n",
      "Epoch 995/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.2502 - accuracy: 0.7850 - val_loss: 9.5475 - val_accuracy: 0.8267\n",
      "Epoch 996/1000\n",
      "600/600 [==============================] - 0s 33us/step - loss: 14.6528 - accuracy: 0.8067 - val_loss: 15.6453 - val_accuracy: 0.8267\n",
      "Epoch 997/1000\n",
      "600/600 [==============================] - 0s 35us/step - loss: 15.0459 - accuracy: 0.7900 - val_loss: 9.7000 - val_accuracy: 0.6867\n",
      "Epoch 998/1000\n",
      "600/600 [==============================] - 0s 30us/step - loss: 13.6410 - accuracy: 0.8017 - val_loss: 16.8261 - val_accuracy: 0.8733\n",
      "Epoch 999/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 14.1196 - accuracy: 0.8433 - val_loss: 18.6343 - val_accuracy: 0.8800\n",
      "Epoch 1000/1000\n",
      "600/600 [==============================] - 0s 32us/step - loss: 15.3826 - accuracy: 0.7983 - val_loss: 13.7473 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_test,y_test), epochs=training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4883.7788774   104.93559823 4729.4859736    70.60454974 4626.69727016\n",
      "   85.26269219 4775.19620355  115.48231018 4825.51431399   58.43109487\n",
      " 4572.01153158  173.60812512 4675.37869689  114.04954211 4639.08805281\n",
      "  151.4739934  4940.42472434   80.18914334 4673.39708165  169.47738797\n",
      " 4916.42876969   76.88276112 4769.72185061   70.85773566 4719.75091474\n",
      "  156.13372454 4523.41507977  165.60516384 4600.43315635  157.87146367]\n",
      "[4903.479     106.01689  4754.432      81.32141  4648.561      74.254135\n",
      " 4803.2803    116.93732  4860.6724     69.07642  4588.0303    181.0327\n",
      " 4711.677     114.84798  4659.3887    150.21992  4949.1646     78.21464\n",
      " 4693.5596    168.2924   4946.266      76.3683   4811.924      70.30106\n",
      " 4747.4214    154.59781  4535.575     163.13394  4631.5366    166.29292 ]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "print(y_test[0])\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance for y_1 -> y_2\n",
      "--------------------------------------\n",
      "RMSE is 33.99918246713917\n",
      "R2 score is 0.8953547261784897\n",
      "\n",
      "\n",
      "The model performance for y_2 -> y_3\n",
      "--------------------------------------\n",
      "RMSE is 49.75682943774657\n",
      "R2 score is 0.8619662764073125\n",
      "\n",
      "\n",
      "The model performance for y_3 -> y_4\n",
      "--------------------------------------\n",
      "RMSE is 65.65800259648091\n",
      "R2 score is 0.8224062297236358\n",
      "\n",
      "\n",
      "The model performance for y_4 -> y_5\n",
      "--------------------------------------\n",
      "RMSE is 81.61363833275703\n",
      "R2 score is 0.7780433692722732\n",
      "\n",
      "\n",
      "The model performance for y_5 -> y_6\n",
      "--------------------------------------\n",
      "RMSE is 97.60696396253228\n",
      "R2 score is 0.7301832716720615\n",
      "\n",
      "\n",
      "The model performance for y_6 -> y_7\n",
      "--------------------------------------\n",
      "RMSE is 113.63075222129868\n",
      "R2 score is 0.6796749776372865\n",
      "\n",
      "\n",
      "The model performance for y_7 -> y_8\n",
      "--------------------------------------\n",
      "RMSE is 129.6895376803062\n",
      "R2 score is 0.6293744900487804\n",
      "\n",
      "\n",
      "The model performance for y_8 -> y_9\n",
      "--------------------------------------\n",
      "RMSE is 145.790824041124\n",
      "R2 score is 0.5785307096624565\n",
      "\n",
      "\n",
      "The model performance for y_9 -> y_10\n",
      "--------------------------------------\n",
      "RMSE is 161.93293053184897\n",
      "R2 score is 0.5282702761915107\n",
      "\n",
      "\n",
      "The model performance for y_10 -> y_11\n",
      "--------------------------------------\n",
      "RMSE is 178.12373287693543\n",
      "R2 score is 0.4778340652178841\n",
      "\n",
      "\n",
      "The model performance for y_11 -> y_12\n",
      "--------------------------------------\n",
      "RMSE is 194.36576572981534\n",
      "R2 score is 0.42760843058582476\n",
      "\n",
      "\n",
      "The model performance for y_12 -> y_13\n",
      "--------------------------------------\n",
      "RMSE is 210.66649592869575\n",
      "R2 score is 0.3767764407274087\n",
      "\n",
      "\n",
      "The model performance for y_13 -> y_14\n",
      "--------------------------------------\n",
      "RMSE is 227.03833269279266\n",
      "R2 score is 0.3257646713431222\n",
      "\n",
      "\n",
      "The model performance for y_14 -> y_15\n",
      "--------------------------------------\n",
      "RMSE is 243.47941438022005\n",
      "R2 score is 0.27358996086355725\n",
      "\n",
      "\n",
      "The model performance for y_15 -> y_16\n",
      "--------------------------------------\n",
      "RMSE is 259.9879880795576\n",
      "R2 score is 0.22515379603263402\n",
      "\n",
      "\n",
      "The model performance for y_16 -> y_17\n",
      "--------------------------------------\n",
      "RMSE is 276.586217545696\n",
      "R2 score is 0.17623128019673817\n",
      "\n",
      "\n",
      "The model performance for y_17 -> y_18\n",
      "--------------------------------------\n",
      "RMSE is 293.27861268955996\n",
      "R2 score is 0.12658156745850738\n",
      "\n",
      "\n",
      "The model performance for y_18 -> y_19\n",
      "--------------------------------------\n",
      "RMSE is 310.0596807445264\n",
      "R2 score is 0.07689763730462143\n",
      "\n",
      "\n",
      "The model performance for y_19 -> y_20\n",
      "--------------------------------------\n",
      "RMSE is 326.9159180977319\n",
      "R2 score is 0.028439248110950215\n",
      "\n",
      "\n",
      "The model performance for y_20 -> y_21\n",
      "--------------------------------------\n",
      "RMSE is 343.8543030870099\n",
      "R2 score is -0.019120258023369252\n",
      "\n",
      "\n",
      "The model performance for y_21 -> y_22\n",
      "--------------------------------------\n",
      "RMSE is 360.8852519548881\n",
      "R2 score is -0.06473708097786554\n",
      "\n",
      "\n",
      "The model performance for y_22 -> y_23\n",
      "--------------------------------------\n",
      "RMSE is 378.0096623408868\n",
      "R2 score is -0.11077768563336636\n",
      "\n",
      "\n",
      "The model performance for y_23 -> y_24\n",
      "--------------------------------------\n",
      "RMSE is 395.221450903537\n",
      "R2 score is -0.15712185553032673\n",
      "\n",
      "\n",
      "The model performance for y_24 -> y_25\n",
      "--------------------------------------\n",
      "RMSE is 412.5259310194245\n",
      "R2 score is -0.2030026101533992\n",
      "\n",
      "\n",
      "The model performance for y_25 -> y_26\n",
      "--------------------------------------\n",
      "RMSE is 429.9308091264029\n",
      "R2 score is -0.24927413773645993\n",
      "\n",
      "\n",
      "The model performance for y_26 -> y_27\n",
      "--------------------------------------\n",
      "RMSE is 447.43840833586955\n",
      "R2 score is -0.2989261649486611\n",
      "\n",
      "\n",
      "The model performance for y_27 -> y_28\n",
      "--------------------------------------\n",
      "RMSE is 465.05061150484113\n",
      "R2 score is -0.3506766255401974\n",
      "\n",
      "\n",
      "The model performance for y_28 -> y_29\n",
      "--------------------------------------\n",
      "RMSE is 482.7596521617515\n",
      "R2 score is -0.400886382250323\n",
      "\n",
      "\n",
      "The model performance for y_29 -> y_30\n",
      "--------------------------------------\n",
      "RMSE is 500.5693169380106\n",
      "R2 score is -0.4498588357221023\n",
      "\n",
      "\n",
      "The model performance for y_30 -> y_31\n",
      "--------------------------------------\n",
      "RMSE is 518.4891900860271\n",
      "R2 score is -0.5005545187816045\n",
      "\n",
      "\n",
      "The model performance for y_31 -> y_32\n",
      "--------------------------------------\n",
      "RMSE is 536.5231411030541\n",
      "R2 score is -0.5536143843321437\n",
      "\n",
      "\n",
      "The model performance for y_32 -> y_33\n",
      "--------------------------------------\n",
      "RMSE is 554.6768918835938\n",
      "R2 score is -0.6059268099842027\n",
      "\n",
      "\n",
      "The model performance for y_33 -> y_34\n",
      "--------------------------------------\n",
      "RMSE is 572.9440854316462\n",
      "R2 score is -0.6562162265371282\n",
      "\n",
      "\n",
      "The model performance for y_34 -> y_35\n",
      "--------------------------------------\n",
      "RMSE is 591.3205533688401\n",
      "R2 score is -0.7057001579408527\n",
      "\n",
      "\n",
      "The model performance for y_35 -> y_36\n",
      "--------------------------------------\n",
      "RMSE is 609.8163093540294\n",
      "R2 score is -0.7559164259300313\n",
      "\n",
      "\n",
      "The model performance for y_36 -> y_37\n",
      "--------------------------------------\n",
      "RMSE is 628.43098396392\n",
      "R2 score is -0.8059364062104901\n",
      "\n",
      "\n",
      "The model performance for y_37 -> y_38\n",
      "--------------------------------------\n",
      "RMSE is 647.1667445492541\n",
      "R2 score is -0.8556972298023038\n",
      "\n",
      "\n",
      "The model performance for y_38 -> y_39\n",
      "--------------------------------------\n",
      "RMSE is 666.025415729688\n",
      "R2 score is -0.9038192231133987\n",
      "\n",
      "\n",
      "The model performance for y_39 -> y_40\n",
      "--------------------------------------\n",
      "RMSE is 685.0052627763821\n",
      "R2 score is -0.9517054498928511\n",
      "\n",
      "\n",
      "The model performance for y_40 -> y_41\n",
      "--------------------------------------\n",
      "RMSE is 704.1133158967701\n",
      "R2 score is -0.9985253992789992\n",
      "\n",
      "\n",
      "The model performance for y_41 -> y_42\n",
      "--------------------------------------\n",
      "RMSE is 723.3493487196997\n",
      "R2 score is -1.04729539767448\n",
      "\n",
      "\n",
      "The model performance for y_42 -> y_43\n",
      "--------------------------------------\n",
      "RMSE is 742.719852740384\n",
      "R2 score is -1.0959386996835514\n",
      "\n",
      "\n",
      "The model performance for y_43 -> y_44\n",
      "--------------------------------------\n",
      "RMSE is 762.2165266058253\n",
      "R2 score is -1.1446496386802396\n",
      "\n",
      "\n",
      "The model performance for y_44 -> y_45\n",
      "--------------------------------------\n",
      "RMSE is 781.8489444935849\n",
      "R2 score is -1.1933155171883736\n",
      "\n",
      "\n",
      "The model performance for y_45 -> y_46\n",
      "--------------------------------------\n",
      "RMSE is 801.6161828420665\n",
      "R2 score is -1.2416114433676098\n",
      "\n",
      "\n",
      "The model performance for y_46 -> y_47\n",
      "--------------------------------------\n",
      "RMSE is 821.5236078329394\n",
      "R2 score is -1.2917001119435898\n",
      "\n",
      "\n",
      "The model performance for y_47 -> y_48\n",
      "--------------------------------------\n",
      "RMSE is 841.5689592335527\n",
      "R2 score is -1.341208033398123\n",
      "\n",
      "\n",
      "The model performance for y_48 -> y_49\n",
      "--------------------------------------\n",
      "RMSE is 861.7523252629013\n",
      "R2 score is -1.3918064248499937\n",
      "\n",
      "\n",
      "The model performance for y_49 -> y_50\n",
      "--------------------------------------\n",
      "RMSE is 882.0718420214844\n",
      "R2 score is -1.4423718643338348\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_i_predict =  model.predict(np.array(X))\n",
    "for i in range(1, n):\n",
    "    y_i_predict = model.predict(y_i_predict)\n",
    "    rmse = (np.sqrt(mean_squared_error(df_n_rounds['y_' + str(i+1)].tolist(), y_i_predict)))\n",
    "    r2 = r2_score(df_n_rounds['y_' + str(i+1)].tolist(), y_i_predict)\n",
    "\n",
    "    print(\"The model performance for y_{} -> y_{}\".format(i, i+1))\n",
    "    print(\"--------------------------------------\")\n",
    "    print('RMSE is {}'.format(rmse))\n",
    "    print('R2 score is {}'.format(r2))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bitf17e3ae0e0b5412f8624a209625ef4a0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
